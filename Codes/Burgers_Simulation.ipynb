{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit (conda)",
      "metadata": {
        "interpreter": {
          "hash": "51c008122401afe5535fd6c3a84bd5dd8b8bd079df3baf2c4d203d1ed6e4d4a5"
        }
      }
    },
    "colab": {
      "name": "Burgers_Same.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6hx4sG_8ieS"
      },
      "source": [
        "# Burgers Equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub5oHbXH8pkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bdc4589-1bd4-447c-c5d0-54fd12955c39"
      },
      "source": [
        "!pip install pyDOE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/ac/91fe4c039e2744466621343d3b8af4a485193ed0aab53af5b1db03be0989/pyDOE-0.3.8.zip\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.4.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-cp37-none-any.whl size=18178 sha256=899bd3c5e52a425e972c3938edda37ad86584e07dd63040abcda4f8ede388070\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/c8/58/a6493bd415e8ba5735082b5e0c096d7c1f2933077a8ce34544\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeH35EQU8ieb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a98e264-773f-4dc2-c172-11fa7cfbd772"
      },
      "source": [
        "%tensorflow_version 1.x ### to run on Google Colab: \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x ### to run on Google Colab:`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioO_XM0H8iec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62310f1a-853a-48ac-e364-c0c29f832f0f"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5bGi5OH8iee"
      },
      "source": [
        "###############################################################################\n",
        "############################## Helper Functions ###############################\n",
        "###############################################################################\n",
        "\n",
        "def initialize_NN(layers):\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers) \n",
        "    for l in range(0,num_layers-1):\n",
        "        W = xavier_init(size=[layers[l], layers[l+1]])\n",
        "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "        weights.append(W)\n",
        "        biases.append(b)\n",
        "    return weights, biases\n",
        "    \n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    out_dim = size[1]\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev, dtype=tf.float32), dtype=tf.float32)\n",
        "\n",
        "def neural_net(X, weights, biases):\n",
        "    num_layers = len(weights) + 1\n",
        "    H = X\n",
        "    for l in range(0,num_layers-2):\n",
        "        W = weights[l]\n",
        "        b = biases[l]\n",
        "        H = tf.sin(tf.add(tf.matmul(H, W), b))\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    Y = tf.add(tf.matmul(H, W), b)\n",
        "    return Y\n",
        "\n",
        "###############################################################################\n",
        "################################ DeepHPM Class ################################\n",
        "###############################################################################\n",
        "\n",
        "class DeepHPM:\n",
        "    def __init__(self, t, x, u,\n",
        "                       x0, u0, tb, X_f,\n",
        "                       u_layers, pde_layers,\n",
        "                       layers,\n",
        "                       lb_idn, ub_idn,\n",
        "                       lb_sol, ub_sol):\n",
        "        \n",
        "        # Domain Boundary\n",
        "        self.lb_idn = lb_idn\n",
        "        self.ub_idn = ub_idn\n",
        "        \n",
        "        self.lb_sol = lb_sol\n",
        "        self.ub_sol = ub_sol\n",
        "        \n",
        "        # Init for Identification\n",
        "        self.idn_init(t, x, u, u_layers, pde_layers)\n",
        "        \n",
        "        # Init for Solution\n",
        "        self.sol_init(x0, u0, tb, X_f, layers)\n",
        "        \n",
        "        # tf session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "    \n",
        "    ###########################################################################\n",
        "    ############################# Identifier ##################################\n",
        "    ###########################################################################\n",
        "        \n",
        "    def idn_init(self, t, x, u, u_layers, pde_layers):\n",
        "        # Training Data for Identification\n",
        "        self.t = t\n",
        "        self.x = x\n",
        "        self.u = u\n",
        "        \n",
        "        # Layers for Identification\n",
        "        self.u_layers = u_layers\n",
        "        self.pde_layers = pde_layers\n",
        "        \n",
        "        # Initialize NNs for Identification\n",
        "        self.u_weights, self.u_biases = initialize_NN(u_layers)\n",
        "        self.pde_weights, self.pde_biases = initialize_NN(pde_layers)\n",
        "        \n",
        "        # tf placeholders for Identification\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.terms_tf = tf.placeholder(tf.float32, shape=[None, pde_layers[0]])\n",
        "        \n",
        "        # tf graphs for Identification\n",
        "        self.idn_u_pred = self.idn_net_u(self.t_tf, self.x_tf)\n",
        "        self.pde_pred = self.net_pde(self.terms_tf)\n",
        "        self.idn_f_pred = self.idn_net_f(self.t_tf, self.x_tf)\n",
        "        \n",
        "        # loss for Identification\n",
        "        self.idn_u_loss = tf.reduce_sum(tf.square(self.idn_u_pred - self.u_tf))\n",
        "        self.idn_f_loss = tf.reduce_sum(tf.square(self.idn_f_pred))\n",
        "        \n",
        "        # Optimizer for Identification\n",
        "        self.idn_u_optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.idn_u_loss,\n",
        "                               var_list = self.u_weights + self.u_biases,\n",
        "                               method = 'L-BFGS-B',\n",
        "                               options = {'maxiter': 50000,\n",
        "                                          'maxfun': 50000,\n",
        "                                          'maxcor': 50,\n",
        "                                          'maxls': 50,\n",
        "                                          'ftol': 1.0*np.finfo(float).eps})\n",
        "    \n",
        "        self.idn_f_optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.idn_f_loss,\n",
        "                               var_list = self.pde_weights + self.pde_biases,\n",
        "                               method = 'L-BFGS-B',\n",
        "                               options = {'maxiter': 50000,\n",
        "                                          'maxfun': 50000,\n",
        "                                          'maxcor': 50,\n",
        "                                          'maxls': 50,\n",
        "                                          'ftol': 1.0*np.finfo(float).eps})\n",
        "    \n",
        "        self.idn_u_optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.idn_u_train_op_Adam = self.idn_u_optimizer_Adam.minimize(self.idn_u_loss, \n",
        "                                   var_list = self.u_weights + self.u_biases)\n",
        "        \n",
        "        self.idn_f_optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.idn_f_train_op_Adam = self.idn_f_optimizer_Adam.minimize(self.idn_f_loss, \n",
        "                                   var_list = self.pde_weights + self.pde_biases)  \n",
        "    \n",
        "    def idn_net_u(self, t, x):\n",
        "        X = tf.concat([t,x],1)\n",
        "        H = 2.0*(X - self.lb_idn)/(self.ub_idn - self.lb_idn) - 1.0\n",
        "        u = neural_net(H, self.u_weights, self.u_biases)\n",
        "        return u\n",
        "    \n",
        "    def net_pde(self, terms):\n",
        "        pde = neural_net(terms, self.pde_weights, self.pde_biases)\n",
        "        return pde\n",
        "    \n",
        "    def idn_net_f(self, t, x):\n",
        "        u = self.idn_net_u(t, x)\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        \n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        \n",
        "        terms = tf.concat([u,u_x,u_xx],1)\n",
        "\n",
        "        \n",
        "        f = u_t - self.net_pde(terms)\n",
        "        \n",
        "        return f\n",
        "\n",
        "    def idn_u_train(self, N_iter):\n",
        "        tf_dict = {self.t_tf: self.t, self.x_tf: self.x, self.u_tf: self.u}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(N_iter):\n",
        "            \n",
        "            self.sess.run(self.idn_u_train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.idn_u_loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.idn_u_optimizer.minimize(self.sess,\n",
        "                                      feed_dict = tf_dict,\n",
        "                                      fetches = [self.idn_u_loss],\n",
        "                                      loss_callback = self.callback)\n",
        "\n",
        "    def idn_f_train(self, N_iter):\n",
        "        tf_dict = {self.t_tf: self.t, self.x_tf: self.x}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(N_iter):\n",
        "            \n",
        "            self.sess.run(self.idn_f_train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.idn_f_loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.idn_f_optimizer.minimize(self.sess,\n",
        "                                      feed_dict = tf_dict,\n",
        "                                      fetches = [self.idn_f_loss],\n",
        "                                      loss_callback = self.callback)\n",
        "\n",
        "    def idn_predict(self, t_star, x_star):\n",
        "        \n",
        "        tf_dict = {self.t_tf: t_star, self.x_tf: x_star}\n",
        "        \n",
        "        u_star = self.sess.run(self.idn_u_pred, tf_dict)\n",
        "        f_star = self.sess.run(self.idn_f_pred, tf_dict)\n",
        "        \n",
        "        return u_star, f_star\n",
        "    \n",
        "    def predict_pde(self, terms_star):\n",
        "        \n",
        "        tf_dict = {self.terms_tf: terms_star}\n",
        "        \n",
        "        pde_star = self.sess.run(self.pde_pred, tf_dict)\n",
        "        \n",
        "        return pde_star\n",
        "    \n",
        "    ###########################################################################\n",
        "    ############################### Solver ####################################\n",
        "    ###########################################################################\n",
        "    \n",
        "    def sol_init(self, x0, u0, tb, X_f, layers):\n",
        "        # Training Data for Solution\n",
        "        X0 = np.concatenate((0*x0, x0), 1) # (0, x0)\n",
        "        X_lb = np.concatenate((tb, 0*tb + self.lb_sol[1]), 1) # (tb, lb[1])\n",
        "        X_ub = np.concatenate((tb, 0*tb + self.ub_sol[1]), 1) # (tb, ub[1])\n",
        "                \n",
        "        self.X_f = X_f # Collocation Points\n",
        "        self.t0 = X0[:,0:1] # Initial Data (time)\n",
        "        self.x0 = X0[:,1:2] # Initial Data (space)\n",
        "        self.t_lb = X_lb[:,0:1] # Boundary Data (time) -- lower boundary\n",
        "        self.x_lb = X_lb[:,1:2] # Boundary Data (space) -- lower boundary\n",
        "        self.t_ub = X_ub[:,0:1] # Boundary Data (time) -- upper boundary\n",
        "        self.x_ub = X_ub[:,1:2] # Boundary Data (space) -- upper boundary\n",
        "        self.t_f = X_f[:,0:1] # Collocation Points (time)\n",
        "        self.x_f = X_f[:,1:2] # Collocation Points (space)\n",
        "        self.u0 = u0 # Boundary Data\n",
        "        \n",
        "        # Layers for Solution\n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NNs for Solution\n",
        "        self.weights, self.biases = initialize_NN(layers)\n",
        "        \n",
        "        # tf placeholders for Solution\n",
        "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        \n",
        "        # tf graphs for Solution\n",
        "        self.u0_pred, _  = self.sol_net_u(self.t0_tf, self.x0_tf)\n",
        "        self.u_lb_pred, self.u_x_lb_pred = self.sol_net_u(self.t_lb_tf, self.x_lb_tf)\n",
        "        self.u_ub_pred, self.u_x_ub_pred = self.sol_net_u(self.t_ub_tf, self.x_ub_tf)\n",
        "        self.sol_f_pred = self.sol_net_f(self.t_f_tf, self.x_f_tf)\n",
        "        \n",
        "        # loss for Solution\n",
        "        self.sol_loss = tf.reduce_sum(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.sol_f_pred))\n",
        "        \n",
        "        # Optimizer for Solution\n",
        "        self.sol_optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.sol_loss,\n",
        "                             var_list = self.weights + self.biases,\n",
        "                             method = 'L-BFGS-B',\n",
        "                             options = {'maxiter': 50000,\n",
        "                                        'maxfun': 50000,\n",
        "                                        'maxcor': 50,\n",
        "                                        'maxls': 50,\n",
        "                                        'ftol': 1.0*np.finfo(float).eps})\n",
        "    \n",
        "        self.sol_optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.sol_train_op_Adam = self.sol_optimizer_Adam.minimize(self.sol_loss,\n",
        "                                 var_list = self.weights + self.biases)\n",
        "    \n",
        "    def sol_net_u(self, t, x):\n",
        "        X = tf.concat([t,x],1)\n",
        "        H = 2.0*(X - self.lb_sol)/(self.ub_sol - self.lb_sol) - 1.0\n",
        "        u = neural_net(H, self.weights, self.biases)\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        return u, u_x\n",
        "    \n",
        "    def sol_net_f(self, t, x):\n",
        "        u, _ = self.sol_net_u(t,x)\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        \n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        \n",
        "        terms = tf.concat([u,u_x,u_xx],1)\n",
        "        \n",
        "        f = u_t - self.net_pde(terms)\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def callback(self, loss):\n",
        "        print('Loss: %e' % (loss))\n",
        "        \n",
        "    def sol_train(self, N_iter):\n",
        "        tf_dict = {self.t0_tf: self.t0, self.x0_tf: self.x0,\n",
        "                   self.u0_tf: self.u0,\n",
        "                   self.t_lb_tf: self.t_lb, self.x_lb_tf: self.x_lb,\n",
        "                   self.t_ub_tf: self.t_ub, self.x_ub_tf: self.x_ub,\n",
        "                   self.t_f_tf: self.t_f, self.x_f_tf: self.x_f}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(N_iter):\n",
        "            \n",
        "            self.sess.run(self.sol_train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.sol_loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "                \n",
        "        self.sol_optimizer.minimize(self.sess,\n",
        "                                    feed_dict = tf_dict,\n",
        "                                    fetches = [self.sol_loss],\n",
        "                                    loss_callback = self.callback)\n",
        "    \n",
        "    def sol_predict(self, t_star, x_star):\n",
        "        \n",
        "        u_star = self.sess.run(self.u0_pred, {self.t0_tf: t_star, self.x0_tf: x_star})  \n",
        "        f_star = self.sess.run(self.sol_f_pred, {self.t_f_tf: t_star, self.x_f_tf: x_star})\n",
        "               \n",
        "        return u_star, f_star    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dJn1YsD8iem"
      },
      "source": [
        "## Problem Set-Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FFu1pR682CG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17eea04-2b54-4c9d-b30f-c502d93ada35"
      },
      "source": [
        "import os\n",
        "cwd = os.getcwd()\n",
        "print(cwd)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLTVx4zY8iet"
      },
      "source": [
        "   # Doman bounds\n",
        "lb_idn = np.array([0.0, -8.0])\n",
        "ub_idn = np.array([10.0, 8.0])\n",
        "    \n",
        "lb_sol = np.array([0.0, -8.0])\n",
        "ub_sol = np.array([10.0, 8.0])\n",
        "    \n",
        "    ### Load Data ###    \n",
        "data_idn = scipy.io.loadmat('/content/Data/burgers_sine.mat')\n",
        "#data_idn = scipy.io.loadmat('/content/Data/burgers_exp.mat')\n",
        "t_idn = data_idn['t'].flatten()[:,None]\n",
        "x_idn = data_idn['x'].flatten()[:,None]\n",
        "Exact_idn = np.real(data_idn['usol'])    \n",
        "T_idn, X_idn = np.meshgrid(t_idn,x_idn)\n",
        "keep = 2/3\n",
        "index = int(keep*t_idn.shape[0])\n",
        "T_idn = T_idn[:,0:index]\n",
        "X_idn = X_idn[:,0:index]\n",
        "Exact_idn = Exact_idn[:,0:index]\n",
        "    \n",
        "t_idn_star = T_idn.flatten()[:,None]\n",
        "x_idn_star = X_idn.flatten()[:,None]\n",
        "X_idn_star = np.hstack((t_idn_star, x_idn_star))\n",
        "u_idn_star = Exact_idn.flatten()[:,None]\n",
        "    \n",
        "    # Data Solution: \n",
        "data_sol = scipy.io.loadmat('/content/Data/burgers_sine.mat')\n",
        "#data_sol = scipy.io.loadmat('/content/Data/burgers_exp.mat')\n",
        "t_sol = data_sol['t'].flatten()[:,None]\n",
        "x_sol = data_sol['x'].flatten()[:,None]\n",
        "Exact_sol = np.real(data_sol['usol'])\n",
        "T_sol, X_sol = np.meshgrid(t_sol,x_sol)\n",
        "t_sol_star = T_sol.flatten()[:,None]\n",
        "x_sol_star = X_sol.flatten()[:,None]\n",
        "X_sol_star = np.hstack((t_sol_star, x_sol_star))\n",
        "u_sol_star = Exact_sol.flatten()[:,None]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2NDTJCt8iet"
      },
      "source": [
        "## Training Process: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eEyOSPy8iet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243bf94c-b1c1-4caf-aecb-c49e7f27d004"
      },
      "source": [
        "    ### Training Data ###\n",
        "    \n",
        "    # For identification\n",
        "N_train = 10000\n",
        "idx = np.random.choice(t_idn_star.shape[0], N_train, replace=False)    \n",
        "t_train = t_idn_star[idx,:]\n",
        "x_train = x_idn_star[idx,:]\n",
        "u_train = u_idn_star[idx,:]\n",
        "noise = 0.00\n",
        "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "    \n",
        "# For solution\n",
        "N0 = Exact_sol.shape[0]\n",
        "N_b = Exact_sol.shape[1]\n",
        "N_f = 20000\n",
        "idx_x = np.random.choice(x_sol.shape[0], N0, replace=False)\n",
        "x0_train = x_sol[idx_x,:]\n",
        "u0_train = Exact_sol[idx_x,0:1]   \n",
        "idx_t = np.random.choice(t_sol.shape[0], N_b, replace=False)\n",
        "tb_train = t_sol[idx_t,:] \n",
        "X_f_train = lb_sol + (ub_sol-lb_sol)*lhs(2, N_f)\n",
        "        \n",
        "    # Layers\n",
        "u_layers = [2, 50, 50, 50, 50, 1]\n",
        "pde_layers = [3, 100, 100, 1]    \n",
        "layers = [2, 50, 50, 50, 50, 1]\n",
        "    # Model\n",
        "model = DeepHPM(t_train, x_train, u_train,x0_train, u0_train, tb_train, X_f_train,u_layers, pde_layers,\n",
        "                    layers,lb_idn, ub_idn,\n",
        "                    lb_sol,ub_sol)\n",
        "    # Train the identifier\n",
        "model.idn_u_train(N_iter=0)\n",
        "model.idn_f_train(N_iter=0)   \n",
        "u_pred_identifier, f_pred_identifier = model.idn_predict(t_idn_star, x_idn_star)  \n",
        "error_u_identifier = np.linalg.norm(u_idn_star-u_pred_identifier,2)/np.linalg.norm(u_idn_star,2)\n",
        "print('Error u: %e' % (error_u_identifier))\n",
        "\n",
        "    ### Solution ###\n",
        "    \n",
        "    # Train the solver\n",
        "model.sol_train(N_iter=0)\n",
        "u_pred, f_pred = model.sol_predict(t_sol_star, x_sol_star)   \n",
        "u_pred_idn, f_pred_idn = model.sol_predict(t_idn_star, x_idn_star)\n",
        "error_u = np.linalg.norm(u_sol_star-u_pred,2)/np.linalg.norm(u_sol_star,2)\n",
        "error_u_idn = np.linalg.norm(u_idn_star-u_pred_idn,2)/np.linalg.norm(u_idn_star,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error u (idn): %e' % (error_u_idn))\n",
        "U_pred = griddata(X_sol_star, u_pred.flatten(), (T_sol, X_sol), method='cubic')\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss: 8.881470e-02\n",
            "Loss: 8.879566e-02\n",
            "Loss: 8.879194e-02\n",
            "Loss: 8.877636e-02\n",
            "Loss: 8.877116e-02\n",
            "Loss: 8.876041e-02\n",
            "Loss: 8.874983e-02\n",
            "Loss: 8.873417e-02\n",
            "Loss: 8.871918e-02\n",
            "Loss: 8.871566e-02\n",
            "Loss: 8.870115e-02\n",
            "Loss: 8.869723e-02\n",
            "Loss: 8.868873e-02\n",
            "Loss: 8.867864e-02\n",
            "Loss: 8.866641e-02\n",
            "Loss: 8.865873e-02\n",
            "Loss: 8.865038e-02\n",
            "Loss: 8.863974e-02\n",
            "Loss: 8.862647e-02\n",
            "Loss: 8.863664e-02\n",
            "Loss: 8.861775e-02\n",
            "Loss: 8.860488e-02\n",
            "Loss: 8.859733e-02\n",
            "Loss: 8.858790e-02\n",
            "Loss: 8.857537e-02\n",
            "Loss: 8.856323e-02\n",
            "Loss: 8.855325e-02\n",
            "Loss: 8.854327e-02\n",
            "Loss: 8.853164e-02\n",
            "Loss: 8.852173e-02\n",
            "Loss: 8.851641e-02\n",
            "Loss: 8.850612e-02\n",
            "Loss: 8.849727e-02\n",
            "Loss: 8.850095e-02\n",
            "Loss: 8.849091e-02\n",
            "Loss: 8.848225e-02\n",
            "Loss: 8.847141e-02\n",
            "Loss: 8.846508e-02\n",
            "Loss: 8.845083e-02\n",
            "Loss: 8.843649e-02\n",
            "Loss: 8.842576e-02\n",
            "Loss: 8.842004e-02\n",
            "Loss: 8.841043e-02\n",
            "Loss: 8.840363e-02\n",
            "Loss: 8.839759e-02\n",
            "Loss: 8.837975e-02\n",
            "Loss: 8.840881e-02\n",
            "Loss: 8.837269e-02\n",
            "Loss: 8.835822e-02\n",
            "Loss: 8.835192e-02\n",
            "Loss: 8.834880e-02\n",
            "Loss: 8.834564e-02\n",
            "Loss: 8.833859e-02\n",
            "Loss: 8.832765e-02\n",
            "Loss: 8.831780e-02\n",
            "Loss: 8.831146e-02\n",
            "Loss: 8.830213e-02\n",
            "Loss: 8.830339e-02\n",
            "Loss: 8.829968e-02\n",
            "Loss: 8.829527e-02\n",
            "Loss: 8.828516e-02\n",
            "Loss: 8.827779e-02\n",
            "Loss: 8.826394e-02\n",
            "Loss: 8.825188e-02\n",
            "Loss: 8.823855e-02\n",
            "Loss: 8.822857e-02\n",
            "Loss: 8.822175e-02\n",
            "Loss: 8.821139e-02\n",
            "Loss: 8.820347e-02\n",
            "Loss: 8.819008e-02\n",
            "Loss: 8.817660e-02\n",
            "Loss: 8.814967e-02\n",
            "Loss: 8.813450e-02\n",
            "Loss: 8.812071e-02\n",
            "Loss: 8.810513e-02\n",
            "Loss: 8.808938e-02\n",
            "Loss: 8.810042e-02\n",
            "Loss: 8.807988e-02\n",
            "Loss: 8.806158e-02\n",
            "Loss: 8.804736e-02\n",
            "Loss: 8.803307e-02\n",
            "Loss: 8.802007e-02\n",
            "Loss: 8.800921e-02\n",
            "Loss: 8.800000e-02\n",
            "Loss: 8.799119e-02\n",
            "Loss: 8.798499e-02\n",
            "Loss: 8.797944e-02\n",
            "Loss: 8.797428e-02\n",
            "Loss: 8.796980e-02\n",
            "Loss: 8.796581e-02\n",
            "Loss: 8.796022e-02\n",
            "Loss: 8.795534e-02\n",
            "Loss: 8.794413e-02\n",
            "Loss: 8.793581e-02\n",
            "Loss: 8.792157e-02\n",
            "Loss: 8.790969e-02\n",
            "Loss: 8.789437e-02\n",
            "Loss: 8.787999e-02\n",
            "Loss: 8.787187e-02\n",
            "Loss: 8.785585e-02\n",
            "Loss: 8.784066e-02\n",
            "Loss: 8.782861e-02\n",
            "Loss: 8.781536e-02\n",
            "Loss: 8.780742e-02\n",
            "Loss: 8.779716e-02\n",
            "Loss: 8.777979e-02\n",
            "Loss: 8.779836e-02\n",
            "Loss: 8.777094e-02\n",
            "Loss: 8.776307e-02\n",
            "Loss: 8.775620e-02\n",
            "Loss: 8.775019e-02\n",
            "Loss: 8.774316e-02\n",
            "Loss: 8.778809e-02\n",
            "Loss: 8.773778e-02\n",
            "Loss: 8.772628e-02\n",
            "Loss: 8.772071e-02\n",
            "Loss: 8.770061e-02\n",
            "Loss: 8.768469e-02\n",
            "Loss: 8.766045e-02\n",
            "Loss: 8.765142e-02\n",
            "Loss: 8.762657e-02\n",
            "Loss: 8.761767e-02\n",
            "Loss: 8.760403e-02\n",
            "Loss: 8.760555e-02\n",
            "Loss: 8.759434e-02\n",
            "Loss: 8.757417e-02\n",
            "Loss: 8.755416e-02\n",
            "Loss: 8.753219e-02\n",
            "Loss: 8.750815e-02\n",
            "Loss: 8.749571e-02\n",
            "Loss: 8.747437e-02\n",
            "Loss: 8.746493e-02\n",
            "Loss: 8.745624e-02\n",
            "Loss: 8.744715e-02\n",
            "Loss: 8.741943e-02\n",
            "Loss: 8.739326e-02\n",
            "Loss: 8.736517e-02\n",
            "Loss: 8.735377e-02\n",
            "Loss: 8.733992e-02\n",
            "Loss: 8.733103e-02\n",
            "Loss: 8.732149e-02\n",
            "Loss: 8.731220e-02\n",
            "Loss: 8.730552e-02\n",
            "Loss: 8.729868e-02\n",
            "Loss: 8.729029e-02\n",
            "Loss: 8.728621e-02\n",
            "Loss: 8.728023e-02\n",
            "Loss: 8.727483e-02\n",
            "Loss: 8.726528e-02\n",
            "Loss: 8.725360e-02\n",
            "Loss: 8.724327e-02\n",
            "Loss: 8.723435e-02\n",
            "Loss: 8.722781e-02\n",
            "Loss: 8.721400e-02\n",
            "Loss: 8.720143e-02\n",
            "Loss: 8.718893e-02\n",
            "Loss: 8.717757e-02\n",
            "Loss: 8.716846e-02\n",
            "Loss: 8.716244e-02\n",
            "Loss: 8.715485e-02\n",
            "Loss: 8.714737e-02\n",
            "Loss: 8.714066e-02\n",
            "Loss: 8.712071e-02\n",
            "Loss: 8.711999e-02\n",
            "Loss: 8.711011e-02\n",
            "Loss: 8.710197e-02\n",
            "Loss: 8.709396e-02\n",
            "Loss: 8.708984e-02\n",
            "Loss: 8.708411e-02\n",
            "Loss: 8.707998e-02\n",
            "Loss: 8.707490e-02\n",
            "Loss: 8.706566e-02\n",
            "Loss: 8.705453e-02\n",
            "Loss: 8.704203e-02\n",
            "Loss: 8.703323e-02\n",
            "Loss: 8.702453e-02\n",
            "Loss: 8.701923e-02\n",
            "Loss: 8.701953e-02\n",
            "Loss: 8.701548e-02\n",
            "Loss: 8.701063e-02\n",
            "Loss: 8.700658e-02\n",
            "Loss: 8.699973e-02\n",
            "Loss: 8.699206e-02\n",
            "Loss: 8.698009e-02\n",
            "Loss: 8.697180e-02\n",
            "Loss: 8.696178e-02\n",
            "Loss: 8.695003e-02\n",
            "Loss: 8.693428e-02\n",
            "Loss: 8.692072e-02\n",
            "Loss: 8.690789e-02\n",
            "Loss: 8.688748e-02\n",
            "Loss: 8.688118e-02\n",
            "Loss: 8.686480e-02\n",
            "Loss: 8.685237e-02\n",
            "Loss: 8.684218e-02\n",
            "Loss: 8.683441e-02\n",
            "Loss: 8.682558e-02\n",
            "Loss: 8.681226e-02\n",
            "Loss: 8.680210e-02\n",
            "Loss: 8.679141e-02\n",
            "Loss: 8.678180e-02\n",
            "Loss: 8.677719e-02\n",
            "Loss: 8.677053e-02\n",
            "Loss: 8.676282e-02\n",
            "Loss: 8.675156e-02\n",
            "Loss: 8.673883e-02\n",
            "Loss: 8.676371e-02\n",
            "Loss: 8.673386e-02\n",
            "Loss: 8.672386e-02\n",
            "Loss: 8.671737e-02\n",
            "Loss: 8.670691e-02\n",
            "Loss: 8.669759e-02\n",
            "Loss: 8.668949e-02\n",
            "Loss: 8.668494e-02\n",
            "Loss: 8.668132e-02\n",
            "Loss: 8.667590e-02\n",
            "Loss: 8.667216e-02\n",
            "Loss: 8.666504e-02\n",
            "Loss: 8.665811e-02\n",
            "Loss: 8.665182e-02\n",
            "Loss: 8.664228e-02\n",
            "Loss: 8.663689e-02\n",
            "Loss: 8.663018e-02\n",
            "Loss: 8.661752e-02\n",
            "Loss: 8.668140e-02\n",
            "Loss: 8.661202e-02\n",
            "Loss: 8.659817e-02\n",
            "Loss: 8.658563e-02\n",
            "Loss: 8.657425e-02\n",
            "Loss: 8.656443e-02\n",
            "Loss: 8.655205e-02\n",
            "Loss: 8.653870e-02\n",
            "Loss: 8.652812e-02\n",
            "Loss: 8.652104e-02\n",
            "Loss: 8.651292e-02\n",
            "Loss: 8.650829e-02\n",
            "Loss: 8.650208e-02\n",
            "Loss: 8.649050e-02\n",
            "Loss: 8.647442e-02\n",
            "Loss: 8.646368e-02\n",
            "Loss: 8.645147e-02\n",
            "Loss: 8.644225e-02\n",
            "Loss: 8.642933e-02\n",
            "Loss: 8.641269e-02\n",
            "Loss: 8.639956e-02\n",
            "Loss: 8.638813e-02\n",
            "Loss: 8.637661e-02\n",
            "Loss: 8.636373e-02\n",
            "Loss: 8.635354e-02\n",
            "Loss: 8.634771e-02\n",
            "Loss: 8.633801e-02\n",
            "Loss: 8.632874e-02\n",
            "Loss: 8.631606e-02\n",
            "Loss: 8.631335e-02\n",
            "Loss: 8.630592e-02\n",
            "Loss: 8.630306e-02\n",
            "Loss: 8.629906e-02\n",
            "Loss: 8.629524e-02\n",
            "Loss: 8.628897e-02\n",
            "Loss: 8.628131e-02\n",
            "Loss: 8.626983e-02\n",
            "Loss: 8.626528e-02\n",
            "Loss: 8.625840e-02\n",
            "Loss: 8.624938e-02\n",
            "Loss: 8.624005e-02\n",
            "Loss: 8.623210e-02\n",
            "Loss: 8.622403e-02\n",
            "Loss: 8.621681e-02\n",
            "Loss: 8.621573e-02\n",
            "Loss: 8.620977e-02\n",
            "Loss: 8.620250e-02\n",
            "Loss: 8.619240e-02\n",
            "Loss: 8.618099e-02\n",
            "Loss: 8.618025e-02\n",
            "Loss: 8.617385e-02\n",
            "Loss: 8.616537e-02\n",
            "Loss: 8.615892e-02\n",
            "Loss: 8.615102e-02\n",
            "Loss: 8.613577e-02\n",
            "Loss: 8.613342e-02\n",
            "Loss: 8.611400e-02\n",
            "Loss: 8.611042e-02\n",
            "Loss: 8.610249e-02\n",
            "Loss: 8.609571e-02\n",
            "Loss: 8.608089e-02\n",
            "Loss: 8.607396e-02\n",
            "Loss: 8.606891e-02\n",
            "Loss: 8.606298e-02\n",
            "Loss: 8.605606e-02\n",
            "Loss: 8.604593e-02\n",
            "Loss: 8.604097e-02\n",
            "Loss: 8.603366e-02\n",
            "Loss: 8.602700e-02\n",
            "Loss: 8.601540e-02\n",
            "Loss: 8.600502e-02\n",
            "Loss: 8.599816e-02\n",
            "Loss: 8.599159e-02\n",
            "Loss: 8.598471e-02\n",
            "Loss: 8.597266e-02\n",
            "Loss: 8.596399e-02\n",
            "Loss: 8.595792e-02\n",
            "Loss: 8.594485e-02\n",
            "Loss: 8.593463e-02\n",
            "Loss: 8.593911e-02\n",
            "Loss: 8.592742e-02\n",
            "Loss: 8.591530e-02\n",
            "Loss: 8.590995e-02\n",
            "Loss: 8.590273e-02\n",
            "Loss: 8.589601e-02\n",
            "Loss: 8.589312e-02\n",
            "Loss: 8.588383e-02\n",
            "Loss: 8.587830e-02\n",
            "Loss: 8.587356e-02\n",
            "Loss: 8.586784e-02\n",
            "Loss: 8.586618e-02\n",
            "Loss: 8.585683e-02\n",
            "Loss: 8.585353e-02\n",
            "Loss: 8.584799e-02\n",
            "Loss: 8.584289e-02\n",
            "Loss: 8.584299e-02\n",
            "Loss: 8.583695e-02\n",
            "Loss: 8.582836e-02\n",
            "Loss: 8.582242e-02\n",
            "Loss: 8.581752e-02\n",
            "Loss: 8.581170e-02\n",
            "Loss: 8.580165e-02\n",
            "Loss: 8.580701e-02\n",
            "Loss: 8.579486e-02\n",
            "Loss: 8.578732e-02\n",
            "Loss: 8.578315e-02\n",
            "Loss: 8.577812e-02\n",
            "Loss: 8.576913e-02\n",
            "Loss: 8.575606e-02\n",
            "Loss: 8.574336e-02\n",
            "Loss: 8.573699e-02\n",
            "Loss: 8.572838e-02\n",
            "Loss: 8.571864e-02\n",
            "Loss: 8.571059e-02\n",
            "Loss: 8.569580e-02\n",
            "Loss: 8.573399e-02\n",
            "Loss: 8.568872e-02\n",
            "Loss: 8.567554e-02\n",
            "Loss: 8.566807e-02\n",
            "Loss: 8.565841e-02\n",
            "Loss: 8.565075e-02\n",
            "Loss: 8.563334e-02\n",
            "Loss: 8.563169e-02\n",
            "Loss: 8.562365e-02\n",
            "Loss: 8.561157e-02\n",
            "Loss: 8.560422e-02\n",
            "Loss: 8.559830e-02\n",
            "Loss: 8.559134e-02\n",
            "Loss: 8.558498e-02\n",
            "Loss: 8.557537e-02\n",
            "Loss: 8.556105e-02\n",
            "Loss: 8.555517e-02\n",
            "Loss: 8.554526e-02\n",
            "Loss: 8.554208e-02\n",
            "Loss: 8.553314e-02\n",
            "Loss: 8.551951e-02\n",
            "Loss: 8.553501e-02\n",
            "Loss: 8.551273e-02\n",
            "Loss: 8.550144e-02\n",
            "Loss: 8.549163e-02\n",
            "Loss: 8.548220e-02\n",
            "Loss: 8.546878e-02\n",
            "Loss: 8.545359e-02\n",
            "Loss: 8.563084e-02\n",
            "Loss: 8.545050e-02\n",
            "Loss: 8.543853e-02\n",
            "Loss: 8.543094e-02\n",
            "Loss: 8.541948e-02\n",
            "Loss: 8.541226e-02\n",
            "Loss: 8.540286e-02\n",
            "Loss: 8.539938e-02\n",
            "Loss: 8.538956e-02\n",
            "Loss: 8.538438e-02\n",
            "Loss: 8.537836e-02\n",
            "Loss: 8.537170e-02\n",
            "Loss: 8.536712e-02\n",
            "Loss: 8.536107e-02\n",
            "Loss: 8.535141e-02\n",
            "Loss: 8.533913e-02\n",
            "Loss: 8.535494e-02\n",
            "Loss: 8.533642e-02\n",
            "Loss: 8.533031e-02\n",
            "Loss: 8.532535e-02\n",
            "Loss: 8.531865e-02\n",
            "Loss: 8.531190e-02\n",
            "Loss: 8.530210e-02\n",
            "Loss: 8.529402e-02\n",
            "Loss: 8.528503e-02\n",
            "Loss: 8.527456e-02\n",
            "Loss: 8.526210e-02\n",
            "Loss: 8.525286e-02\n",
            "Loss: 8.524230e-02\n",
            "Loss: 8.523177e-02\n",
            "Loss: 8.521978e-02\n",
            "Loss: 8.521013e-02\n",
            "Loss: 8.520557e-02\n",
            "Loss: 8.519647e-02\n",
            "Loss: 8.519036e-02\n",
            "Loss: 8.518460e-02\n",
            "Loss: 8.517764e-02\n",
            "Loss: 8.517325e-02\n",
            "Loss: 8.516952e-02\n",
            "Loss: 8.516439e-02\n",
            "Loss: 8.515915e-02\n",
            "Loss: 8.515300e-02\n",
            "Loss: 8.514677e-02\n",
            "Loss: 8.514046e-02\n",
            "Loss: 8.513471e-02\n",
            "Loss: 8.512834e-02\n",
            "Loss: 8.512383e-02\n",
            "Loss: 8.511722e-02\n",
            "Loss: 8.511382e-02\n",
            "Loss: 8.510843e-02\n",
            "Loss: 8.510453e-02\n",
            "Loss: 8.509699e-02\n",
            "Loss: 8.509140e-02\n",
            "Loss: 8.508570e-02\n",
            "Loss: 8.508574e-02\n",
            "Loss: 8.508200e-02\n",
            "Loss: 8.507704e-02\n",
            "Loss: 8.507217e-02\n",
            "Loss: 8.506409e-02\n",
            "Loss: 8.505468e-02\n",
            "Loss: 8.507688e-02\n",
            "Loss: 8.505075e-02\n",
            "Loss: 8.504376e-02\n",
            "Loss: 8.503904e-02\n",
            "Loss: 8.503391e-02\n",
            "Loss: 8.503510e-02\n",
            "Loss: 8.503196e-02\n",
            "Loss: 8.502873e-02\n",
            "Loss: 8.502502e-02\n",
            "Loss: 8.501625e-02\n",
            "Loss: 8.500925e-02\n",
            "Loss: 8.500450e-02\n",
            "Loss: 8.499472e-02\n",
            "Loss: 8.498691e-02\n",
            "Loss: 8.498304e-02\n",
            "Loss: 8.497687e-02\n",
            "Loss: 8.496888e-02\n",
            "Loss: 8.496330e-02\n",
            "Loss: 8.495899e-02\n",
            "Loss: 8.495276e-02\n",
            "Loss: 8.494800e-02\n",
            "Loss: 8.494125e-02\n",
            "Loss: 8.493548e-02\n",
            "Loss: 8.493106e-02\n",
            "Loss: 8.492464e-02\n",
            "Loss: 8.491150e-02\n",
            "Loss: 8.492243e-02\n",
            "Loss: 8.490667e-02\n",
            "Loss: 8.489563e-02\n",
            "Loss: 8.488598e-02\n",
            "Loss: 8.487675e-02\n",
            "Loss: 8.486895e-02\n",
            "Loss: 8.486291e-02\n",
            "Loss: 8.485787e-02\n",
            "Loss: 8.485474e-02\n",
            "Loss: 8.484964e-02\n",
            "Loss: 8.484603e-02\n",
            "Loss: 8.484203e-02\n",
            "Loss: 8.483252e-02\n",
            "Loss: 8.482885e-02\n",
            "Loss: 8.482413e-02\n",
            "Loss: 8.482068e-02\n",
            "Loss: 8.481781e-02\n",
            "Loss: 8.481225e-02\n",
            "Loss: 8.481432e-02\n",
            "Loss: 8.481166e-02\n",
            "Loss: 8.481053e-02\n",
            "Loss: 8.480655e-02\n",
            "Loss: 8.480243e-02\n",
            "Loss: 8.479613e-02\n",
            "Loss: 8.479080e-02\n",
            "Loss: 8.478803e-02\n",
            "Loss: 8.478334e-02\n",
            "Loss: 8.478178e-02\n",
            "Loss: 8.478001e-02\n",
            "Loss: 8.477689e-02\n",
            "Loss: 8.477185e-02\n",
            "Loss: 8.476938e-02\n",
            "Loss: 8.476183e-02\n",
            "Loss: 8.475877e-02\n",
            "Loss: 8.475456e-02\n",
            "Loss: 8.475207e-02\n",
            "Loss: 8.474851e-02\n",
            "Loss: 8.474500e-02\n",
            "Loss: 8.473781e-02\n",
            "Loss: 8.472995e-02\n",
            "Loss: 8.471467e-02\n",
            "Loss: 8.470131e-02\n",
            "Loss: 8.469557e-02\n",
            "Loss: 8.469035e-02\n",
            "Loss: 8.468691e-02\n",
            "Loss: 8.468354e-02\n",
            "Loss: 8.467761e-02\n",
            "Loss: 8.467462e-02\n",
            "Loss: 8.466771e-02\n",
            "Loss: 8.466429e-02\n",
            "Loss: 8.466023e-02\n",
            "Loss: 8.465508e-02\n",
            "Loss: 8.465046e-02\n",
            "Loss: 8.464613e-02\n",
            "Loss: 8.464215e-02\n",
            "Loss: 8.463874e-02\n",
            "Loss: 8.463252e-02\n",
            "Loss: 8.462509e-02\n",
            "Loss: 8.462003e-02\n",
            "Loss: 8.461465e-02\n",
            "Loss: 8.461258e-02\n",
            "Loss: 8.460654e-02\n",
            "Loss: 8.460324e-02\n",
            "Loss: 8.460084e-02\n",
            "Loss: 8.459681e-02\n",
            "Loss: 8.459385e-02\n",
            "Loss: 8.459198e-02\n",
            "Loss: 8.459049e-02\n",
            "Loss: 8.458786e-02\n",
            "Loss: 8.458424e-02\n",
            "Loss: 8.457907e-02\n",
            "Loss: 8.457898e-02\n",
            "Loss: 8.457623e-02\n",
            "Loss: 8.457276e-02\n",
            "Loss: 8.457030e-02\n",
            "Loss: 8.456829e-02\n",
            "Loss: 8.456362e-02\n",
            "Loss: 8.455529e-02\n",
            "Loss: 8.456060e-02\n",
            "Loss: 8.455148e-02\n",
            "Loss: 8.454568e-02\n",
            "Loss: 8.454237e-02\n",
            "Loss: 8.453975e-02\n",
            "Loss: 8.453391e-02\n",
            "Loss: 8.452580e-02\n",
            "Loss: 8.452371e-02\n",
            "Loss: 8.451787e-02\n",
            "Loss: 8.451620e-02\n",
            "Loss: 8.451290e-02\n",
            "Loss: 8.451168e-02\n",
            "Loss: 8.450780e-02\n",
            "Loss: 8.450592e-02\n",
            "Loss: 8.450260e-02\n",
            "Loss: 8.449635e-02\n",
            "Loss: 8.449453e-02\n",
            "Loss: 8.448669e-02\n",
            "Loss: 8.448424e-02\n",
            "Loss: 8.447967e-02\n",
            "Loss: 8.447472e-02\n",
            "Loss: 8.446957e-02\n",
            "Loss: 8.445995e-02\n",
            "Loss: 8.445693e-02\n",
            "Loss: 8.445248e-02\n",
            "Loss: 8.444860e-02\n",
            "Loss: 8.444199e-02\n",
            "Loss: 8.443339e-02\n",
            "Loss: 8.442094e-02\n",
            "Loss: 8.441382e-02\n",
            "Loss: 8.440299e-02\n",
            "Loss: 8.439765e-02\n",
            "Loss: 8.439192e-02\n",
            "Loss: 8.438442e-02\n",
            "Loss: 8.437819e-02\n",
            "Loss: 8.437406e-02\n",
            "Loss: 8.436979e-02\n",
            "Loss: 8.436587e-02\n",
            "Loss: 8.436064e-02\n",
            "Loss: 8.435297e-02\n",
            "Loss: 8.434790e-02\n",
            "Loss: 8.433975e-02\n",
            "Loss: 8.433586e-02\n",
            "Loss: 8.433093e-02\n",
            "Loss: 8.432755e-02\n",
            "Loss: 8.432268e-02\n",
            "Loss: 8.431876e-02\n",
            "Loss: 8.431440e-02\n",
            "Loss: 8.431705e-02\n",
            "Loss: 8.431302e-02\n",
            "Loss: 8.430959e-02\n",
            "Loss: 8.430576e-02\n",
            "Loss: 8.429834e-02\n",
            "Loss: 8.429379e-02\n",
            "Loss: 8.429071e-02\n",
            "Loss: 8.428848e-02\n",
            "Loss: 8.428305e-02\n",
            "Loss: 8.427735e-02\n",
            "Loss: 8.427320e-02\n",
            "Loss: 8.426741e-02\n",
            "Loss: 8.426511e-02\n",
            "Loss: 8.425796e-02\n",
            "Loss: 8.425260e-02\n",
            "Loss: 8.424631e-02\n",
            "Loss: 8.423683e-02\n",
            "Loss: 8.423352e-02\n",
            "Loss: 8.421651e-02\n",
            "Loss: 8.421283e-02\n",
            "Loss: 8.420889e-02\n",
            "Loss: 8.420436e-02\n",
            "Loss: 8.419660e-02\n",
            "Loss: 8.423159e-02\n",
            "Loss: 8.419313e-02\n",
            "Loss: 8.418638e-02\n",
            "Loss: 8.417760e-02\n",
            "Loss: 8.417003e-02\n",
            "Loss: 8.415656e-02\n",
            "Loss: 8.414967e-02\n",
            "Loss: 8.414659e-02\n",
            "Loss: 8.414426e-02\n",
            "Loss: 8.414133e-02\n",
            "Loss: 8.413613e-02\n",
            "Loss: 8.412735e-02\n",
            "Loss: 8.411851e-02\n",
            "Loss: 8.411717e-02\n",
            "Loss: 8.411217e-02\n",
            "Loss: 8.411059e-02\n",
            "Loss: 8.410843e-02\n",
            "Loss: 8.410512e-02\n",
            "Loss: 8.410073e-02\n",
            "Loss: 8.409490e-02\n",
            "Loss: 8.409192e-02\n",
            "Loss: 8.408811e-02\n",
            "Loss: 8.408482e-02\n",
            "Loss: 8.407980e-02\n",
            "Loss: 8.407250e-02\n",
            "Loss: 8.406794e-02\n",
            "Loss: 8.406612e-02\n",
            "Loss: 8.406296e-02\n",
            "Loss: 8.406086e-02\n",
            "Loss: 8.405774e-02\n",
            "Loss: 8.405384e-02\n",
            "Loss: 8.405861e-02\n",
            "Loss: 8.405186e-02\n",
            "Loss: 8.404797e-02\n",
            "Loss: 8.404563e-02\n",
            "Loss: 8.404293e-02\n",
            "Loss: 8.404017e-02\n",
            "Loss: 8.403770e-02\n",
            "Loss: 8.403480e-02\n",
            "Loss: 8.403111e-02\n",
            "Loss: 8.402499e-02\n",
            "Loss: 8.401871e-02\n",
            "Loss: 8.401388e-02\n",
            "Loss: 8.401039e-02\n",
            "Loss: 8.400740e-02\n",
            "Loss: 8.400462e-02\n",
            "Loss: 8.400203e-02\n",
            "Loss: 8.399829e-02\n",
            "Loss: 8.399603e-02\n",
            "Loss: 8.399127e-02\n",
            "Loss: 8.399232e-02\n",
            "Loss: 8.398972e-02\n",
            "Loss: 8.398607e-02\n",
            "Loss: 8.398230e-02\n",
            "Loss: 8.397859e-02\n",
            "Loss: 8.397502e-02\n",
            "Loss: 8.396987e-02\n",
            "Loss: 8.396517e-02\n",
            "Loss: 8.396124e-02\n",
            "Loss: 8.395839e-02\n",
            "Loss: 8.395614e-02\n",
            "Loss: 8.395232e-02\n",
            "Loss: 8.394878e-02\n",
            "Loss: 8.394405e-02\n",
            "Loss: 8.394167e-02\n",
            "Loss: 8.393784e-02\n",
            "Loss: 8.393053e-02\n",
            "Loss: 8.392559e-02\n",
            "Loss: 8.392116e-02\n",
            "Loss: 8.391632e-02\n",
            "Loss: 8.391181e-02\n",
            "Loss: 8.390492e-02\n",
            "Loss: 8.390080e-02\n",
            "Loss: 8.389883e-02\n",
            "Loss: 8.388875e-02\n",
            "Loss: 8.388451e-02\n",
            "Loss: 8.387720e-02\n",
            "Loss: 8.387146e-02\n",
            "Loss: 8.389150e-02\n",
            "Loss: 8.386910e-02\n",
            "Loss: 8.386485e-02\n",
            "Loss: 8.386132e-02\n",
            "Loss: 8.385488e-02\n",
            "Loss: 8.384992e-02\n",
            "Loss: 8.384813e-02\n",
            "Loss: 8.384143e-02\n",
            "Loss: 8.383864e-02\n",
            "Loss: 8.383512e-02\n",
            "Loss: 8.383176e-02\n",
            "Loss: 8.382908e-02\n",
            "Loss: 8.382680e-02\n",
            "Loss: 8.382574e-02\n",
            "Loss: 8.382079e-02\n",
            "Loss: 8.381573e-02\n",
            "Loss: 8.380427e-02\n",
            "Loss: 8.380399e-02\n",
            "Loss: 8.379856e-02\n",
            "Loss: 8.378793e-02\n",
            "Loss: 8.378100e-02\n",
            "Loss: 8.377539e-02\n",
            "Loss: 8.377416e-02\n",
            "Loss: 8.376707e-02\n",
            "Loss: 8.376476e-02\n",
            "Loss: 8.376314e-02\n",
            "Loss: 8.376060e-02\n",
            "Loss: 8.375642e-02\n",
            "Loss: 8.375165e-02\n",
            "Loss: 8.374679e-02\n",
            "Loss: 8.373954e-02\n",
            "Loss: 8.373565e-02\n",
            "Loss: 8.373022e-02\n",
            "Loss: 8.372335e-02\n",
            "Loss: 8.371923e-02\n",
            "Loss: 8.371409e-02\n",
            "Loss: 8.370640e-02\n",
            "Loss: 8.370083e-02\n",
            "Loss: 8.369175e-02\n",
            "Loss: 8.368854e-02\n",
            "Loss: 8.368085e-02\n",
            "Loss: 8.367347e-02\n",
            "Loss: 8.366781e-02\n",
            "Loss: 8.365905e-02\n",
            "Loss: 8.365277e-02\n",
            "Loss: 8.364415e-02\n",
            "Loss: 8.363780e-02\n",
            "Loss: 8.363241e-02\n",
            "Loss: 8.362121e-02\n",
            "Loss: 8.362325e-02\n",
            "Loss: 8.361572e-02\n",
            "Loss: 8.360782e-02\n",
            "Loss: 8.360274e-02\n",
            "Loss: 8.359748e-02\n",
            "Loss: 8.359122e-02\n",
            "Loss: 8.358530e-02\n",
            "Loss: 8.357916e-02\n",
            "Loss: 8.357298e-02\n",
            "Loss: 8.356467e-02\n",
            "Loss: 8.356359e-02\n",
            "Loss: 8.355972e-02\n",
            "Loss: 8.355756e-02\n",
            "Loss: 8.355431e-02\n",
            "Loss: 8.355031e-02\n",
            "Loss: 8.358780e-02\n",
            "Loss: 8.354884e-02\n",
            "Loss: 8.354453e-02\n",
            "Loss: 8.354060e-02\n",
            "Loss: 8.353544e-02\n",
            "Loss: 8.352982e-02\n",
            "Loss: 8.355246e-02\n",
            "Loss: 8.352663e-02\n",
            "Loss: 8.351838e-02\n",
            "Loss: 8.351580e-02\n",
            "Loss: 8.351317e-02\n",
            "Loss: 8.351094e-02\n",
            "Loss: 8.350755e-02\n",
            "Loss: 8.350141e-02\n",
            "Loss: 8.353712e-02\n",
            "Loss: 8.349907e-02\n",
            "Loss: 8.349239e-02\n",
            "Loss: 8.348829e-02\n",
            "Loss: 8.348425e-02\n",
            "Loss: 8.347712e-02\n",
            "Loss: 8.350259e-02\n",
            "Loss: 8.347450e-02\n",
            "Loss: 8.346587e-02\n",
            "Loss: 8.346209e-02\n",
            "Loss: 8.345737e-02\n",
            "Loss: 8.345255e-02\n",
            "Loss: 8.345240e-02\n",
            "Loss: 8.344837e-02\n",
            "Loss: 8.344203e-02\n",
            "Loss: 8.343604e-02\n",
            "Loss: 8.343036e-02\n",
            "Loss: 8.342355e-02\n",
            "Loss: 8.341958e-02\n",
            "Loss: 8.341478e-02\n",
            "Loss: 8.340862e-02\n",
            "Loss: 8.340504e-02\n",
            "Loss: 8.339678e-02\n",
            "Loss: 8.338556e-02\n",
            "Loss: 8.338759e-02\n",
            "Loss: 8.337943e-02\n",
            "Loss: 8.337413e-02\n",
            "Loss: 8.337041e-02\n",
            "Loss: 8.336624e-02\n",
            "Loss: 8.335856e-02\n",
            "Loss: 8.336135e-02\n",
            "Loss: 8.335607e-02\n",
            "Loss: 8.335284e-02\n",
            "Loss: 8.334848e-02\n",
            "Loss: 8.334459e-02\n",
            "Loss: 8.333858e-02\n",
            "Loss: 8.334041e-02\n",
            "Loss: 8.333545e-02\n",
            "Loss: 8.333018e-02\n",
            "Loss: 8.332576e-02\n",
            "Loss: 8.331953e-02\n",
            "Loss: 8.331243e-02\n",
            "Loss: 8.330734e-02\n",
            "Loss: 8.330068e-02\n",
            "Loss: 8.329768e-02\n",
            "Loss: 8.329529e-02\n",
            "Loss: 8.328846e-02\n",
            "Loss: 8.328629e-02\n",
            "Loss: 8.328073e-02\n",
            "Loss: 8.327545e-02\n",
            "Loss: 8.326617e-02\n",
            "Loss: 8.345332e-02\n",
            "Loss: 8.326244e-02\n",
            "Loss: 8.325525e-02\n",
            "Loss: 8.325195e-02\n",
            "Loss: 8.324814e-02\n",
            "Loss: 8.324479e-02\n",
            "Loss: 8.324098e-02\n",
            "Loss: 8.323500e-02\n",
            "Loss: 8.328234e-02\n",
            "Loss: 8.323291e-02\n",
            "Loss: 8.322728e-02\n",
            "Loss: 8.322201e-02\n",
            "Loss: 8.321287e-02\n",
            "Loss: 8.320668e-02\n",
            "Loss: 8.320383e-02\n",
            "Loss: 8.319984e-02\n",
            "Loss: 8.319798e-02\n",
            "Loss: 8.319500e-02\n",
            "Loss: 8.318861e-02\n",
            "Loss: 8.318184e-02\n",
            "Loss: 8.319401e-02\n",
            "Loss: 8.317908e-02\n",
            "Loss: 8.317426e-02\n",
            "Loss: 8.316930e-02\n",
            "Loss: 8.316372e-02\n",
            "Loss: 8.315559e-02\n",
            "Loss: 8.314770e-02\n",
            "Loss: 8.314200e-02\n",
            "Loss: 8.313575e-02\n",
            "Loss: 8.313067e-02\n",
            "Loss: 8.312273e-02\n",
            "Loss: 8.311729e-02\n",
            "Loss: 8.311263e-02\n",
            "Loss: 8.310867e-02\n",
            "Loss: 8.310674e-02\n",
            "Loss: 8.310064e-02\n",
            "Loss: 8.309315e-02\n",
            "Loss: 8.312029e-02\n",
            "Loss: 8.309105e-02\n",
            "Loss: 8.308382e-02\n",
            "Loss: 8.308009e-02\n",
            "Loss: 8.307631e-02\n",
            "Loss: 8.307232e-02\n",
            "Loss: 8.307727e-02\n",
            "Loss: 8.306925e-02\n",
            "Loss: 8.306177e-02\n",
            "Loss: 8.305346e-02\n",
            "Loss: 8.304851e-02\n",
            "Loss: 8.304477e-02\n",
            "Loss: 8.304231e-02\n",
            "Loss: 8.303672e-02\n",
            "Loss: 8.303292e-02\n",
            "Loss: 8.302915e-02\n",
            "Loss: 8.304583e-02\n",
            "Loss: 8.302723e-02\n",
            "Loss: 8.302487e-02\n",
            "Loss: 8.301938e-02\n",
            "Loss: 8.301537e-02\n",
            "Loss: 8.301218e-02\n",
            "Loss: 8.300672e-02\n",
            "Loss: 8.300808e-02\n",
            "Loss: 8.300330e-02\n",
            "Loss: 8.300020e-02\n",
            "Loss: 8.299809e-02\n",
            "Loss: 8.299500e-02\n",
            "Loss: 8.299105e-02\n",
            "Loss: 8.298542e-02\n",
            "Loss: 8.298336e-02\n",
            "Loss: 8.297978e-02\n",
            "Loss: 8.297519e-02\n",
            "Loss: 8.296725e-02\n",
            "Loss: 8.298708e-02\n",
            "Loss: 8.296524e-02\n",
            "Loss: 8.295954e-02\n",
            "Loss: 8.295318e-02\n",
            "Loss: 8.294444e-02\n",
            "Loss: 8.293341e-02\n",
            "Loss: 8.292231e-02\n",
            "Loss: 8.292561e-02\n",
            "Loss: 8.291787e-02\n",
            "Loss: 8.291229e-02\n",
            "Loss: 8.290912e-02\n",
            "Loss: 8.290390e-02\n",
            "Loss: 8.289764e-02\n",
            "Loss: 8.289186e-02\n",
            "Loss: 8.288731e-02\n",
            "Loss: 8.288220e-02\n",
            "Loss: 8.287723e-02\n",
            "Loss: 8.286775e-02\n",
            "Loss: 8.286056e-02\n",
            "Loss: 8.285496e-02\n",
            "Loss: 8.284785e-02\n",
            "Loss: 8.284267e-02\n",
            "Loss: 8.283670e-02\n",
            "Loss: 8.282822e-02\n",
            "Loss: 8.282622e-02\n",
            "Loss: 8.281755e-02\n",
            "Loss: 8.281134e-02\n",
            "Loss: 8.280554e-02\n",
            "Loss: 8.280007e-02\n",
            "Loss: 8.279976e-02\n",
            "Loss: 8.279593e-02\n",
            "Loss: 8.278782e-02\n",
            "Loss: 8.278241e-02\n",
            "Loss: 8.277711e-02\n",
            "Loss: 8.276785e-02\n",
            "Loss: 8.276071e-02\n",
            "Loss: 8.275384e-02\n",
            "Loss: 8.275096e-02\n",
            "Loss: 8.274508e-02\n",
            "Loss: 8.277464e-02\n",
            "Loss: 8.274292e-02\n",
            "Loss: 8.273706e-02\n",
            "Loss: 8.273295e-02\n",
            "Loss: 8.272853e-02\n",
            "Loss: 8.272316e-02\n",
            "Loss: 8.271433e-02\n",
            "Loss: 8.273687e-02\n",
            "Loss: 8.271153e-02\n",
            "Loss: 8.270545e-02\n",
            "Loss: 8.270094e-02\n",
            "Loss: 8.269496e-02\n",
            "Loss: 8.271673e-02\n",
            "Loss: 8.269399e-02\n",
            "Loss: 8.268994e-02\n",
            "Loss: 8.268454e-02\n",
            "Loss: 8.267689e-02\n",
            "Loss: 8.267049e-02\n",
            "Loss: 8.266337e-02\n",
            "Loss: 8.265625e-02\n",
            "Loss: 8.264655e-02\n",
            "Loss: 8.264226e-02\n",
            "Loss: 8.263323e-02\n",
            "Loss: 8.262070e-02\n",
            "Loss: 8.261943e-02\n",
            "Loss: 8.260562e-02\n",
            "Loss: 8.260161e-02\n",
            "Loss: 8.259454e-02\n",
            "Loss: 8.258522e-02\n",
            "Loss: 8.257417e-02\n",
            "Loss: 8.256278e-02\n",
            "Loss: 8.255899e-02\n",
            "Loss: 8.255103e-02\n",
            "Loss: 8.254415e-02\n",
            "Loss: 8.253445e-02\n",
            "Loss: 8.252303e-02\n",
            "Loss: 8.250571e-02\n",
            "Loss: 8.249341e-02\n",
            "Loss: 8.248378e-02\n",
            "Loss: 8.247790e-02\n",
            "Loss: 8.247121e-02\n",
            "Loss: 8.246040e-02\n",
            "Loss: 8.244725e-02\n",
            "Loss: 8.243920e-02\n",
            "Loss: 8.243388e-02\n",
            "Loss: 8.242652e-02\n",
            "Loss: 8.242105e-02\n",
            "Loss: 8.241822e-02\n",
            "Loss: 8.241223e-02\n",
            "Loss: 8.240084e-02\n",
            "Loss: 8.239339e-02\n",
            "Loss: 8.238317e-02\n",
            "Loss: 8.237991e-02\n",
            "Loss: 8.237633e-02\n",
            "Loss: 8.237129e-02\n",
            "Loss: 8.236374e-02\n",
            "Loss: 8.236013e-02\n",
            "Loss: 8.235465e-02\n",
            "Loss: 8.234804e-02\n",
            "Loss: 8.233945e-02\n",
            "Loss: 8.233038e-02\n",
            "Loss: 8.244811e-02\n",
            "Loss: 8.232737e-02\n",
            "Loss: 8.231732e-02\n",
            "Loss: 8.230818e-02\n",
            "Loss: 8.230545e-02\n",
            "Loss: 8.229466e-02\n",
            "Loss: 8.228970e-02\n",
            "Loss: 8.228329e-02\n",
            "Loss: 8.227783e-02\n",
            "Loss: 8.226563e-02\n",
            "Loss: 8.226147e-02\n",
            "Loss: 8.225578e-02\n",
            "Loss: 8.225181e-02\n",
            "Loss: 8.224479e-02\n",
            "Loss: 8.223729e-02\n",
            "Loss: 8.223113e-02\n",
            "Loss: 8.222859e-02\n",
            "Loss: 8.222375e-02\n",
            "Loss: 8.221804e-02\n",
            "Loss: 8.220938e-02\n",
            "Loss: 8.222625e-02\n",
            "Loss: 8.220595e-02\n",
            "Loss: 8.220058e-02\n",
            "Loss: 8.219471e-02\n",
            "Loss: 8.218820e-02\n",
            "Loss: 8.217879e-02\n",
            "Loss: 8.222611e-02\n",
            "Loss: 8.217539e-02\n",
            "Loss: 8.216641e-02\n",
            "Loss: 8.215959e-02\n",
            "Loss: 8.215246e-02\n",
            "Loss: 8.214165e-02\n",
            "Loss: 8.213288e-02\n",
            "Loss: 8.211631e-02\n",
            "Loss: 8.210843e-02\n",
            "Loss: 8.209999e-02\n",
            "Loss: 8.209464e-02\n",
            "Loss: 8.208464e-02\n",
            "Loss: 8.207385e-02\n",
            "Loss: 8.206575e-02\n",
            "Loss: 8.205897e-02\n",
            "Loss: 8.205600e-02\n",
            "Loss: 8.204956e-02\n",
            "Loss: 8.203913e-02\n",
            "Loss: 8.202653e-02\n",
            "Loss: 8.202272e-02\n",
            "Loss: 8.200579e-02\n",
            "Loss: 8.199378e-02\n",
            "Loss: 8.198909e-02\n",
            "Loss: 8.198185e-02\n",
            "Loss: 8.197901e-02\n",
            "Loss: 8.197363e-02\n",
            "Loss: 8.196831e-02\n",
            "Loss: 8.196034e-02\n",
            "Loss: 8.195109e-02\n",
            "Loss: 8.194411e-02\n",
            "Loss: 8.193795e-02\n",
            "Loss: 8.193192e-02\n",
            "Loss: 8.193419e-02\n",
            "Loss: 8.192916e-02\n",
            "Loss: 8.192406e-02\n",
            "Loss: 8.191720e-02\n",
            "Loss: 8.191260e-02\n",
            "Loss: 8.190502e-02\n",
            "Loss: 8.189361e-02\n",
            "Loss: 8.191968e-02\n",
            "Loss: 8.188708e-02\n",
            "Loss: 8.187658e-02\n",
            "Loss: 8.186611e-02\n",
            "Loss: 8.185432e-02\n",
            "Loss: 8.183954e-02\n",
            "Loss: 8.184918e-02\n",
            "Loss: 8.183122e-02\n",
            "Loss: 8.181512e-02\n",
            "Loss: 8.180742e-02\n",
            "Loss: 8.179750e-02\n",
            "Loss: 8.178556e-02\n",
            "Loss: 8.177260e-02\n",
            "Loss: 8.176738e-02\n",
            "Loss: 8.175659e-02\n",
            "Loss: 8.175142e-02\n",
            "Loss: 8.174445e-02\n",
            "Loss: 8.173271e-02\n",
            "Loss: 8.172264e-02\n",
            "Loss: 8.171149e-02\n",
            "Loss: 8.170538e-02\n",
            "Loss: 8.169909e-02\n",
            "Loss: 8.169494e-02\n",
            "Loss: 8.168629e-02\n",
            "Loss: 8.169745e-02\n",
            "Loss: 8.168270e-02\n",
            "Loss: 8.167633e-02\n",
            "Loss: 8.166809e-02\n",
            "Loss: 8.165906e-02\n",
            "Loss: 8.165367e-02\n",
            "Loss: 8.164731e-02\n",
            "Loss: 8.164408e-02\n",
            "Loss: 8.163653e-02\n",
            "Loss: 8.163012e-02\n",
            "Loss: 8.162279e-02\n",
            "Loss: 8.161605e-02\n",
            "Loss: 8.160339e-02\n",
            "Loss: 8.159471e-02\n",
            "Loss: 8.158728e-02\n",
            "Loss: 8.157909e-02\n",
            "Loss: 8.157247e-02\n",
            "Loss: 8.156957e-02\n",
            "Loss: 8.156601e-02\n",
            "Loss: 8.156237e-02\n",
            "Loss: 8.155587e-02\n",
            "Loss: 8.155274e-02\n",
            "Loss: 8.154507e-02\n",
            "Loss: 8.154029e-02\n",
            "Loss: 8.153536e-02\n",
            "Loss: 8.156449e-02\n",
            "Loss: 8.153355e-02\n",
            "Loss: 8.152921e-02\n",
            "Loss: 8.152303e-02\n",
            "Loss: 8.151438e-02\n",
            "Loss: 8.151126e-02\n",
            "Loss: 8.150396e-02\n",
            "Loss: 8.150093e-02\n",
            "Loss: 8.149619e-02\n",
            "Loss: 8.148845e-02\n",
            "Loss: 8.148293e-02\n",
            "Loss: 8.147173e-02\n",
            "Loss: 8.146569e-02\n",
            "Loss: 8.146217e-02\n",
            "Loss: 8.145930e-02\n",
            "Loss: 8.145507e-02\n",
            "Loss: 8.144893e-02\n",
            "Loss: 8.144201e-02\n",
            "Loss: 8.142946e-02\n",
            "Loss: 8.144322e-02\n",
            "Loss: 8.142372e-02\n",
            "Loss: 8.141242e-02\n",
            "Loss: 8.140436e-02\n",
            "Loss: 8.139809e-02\n",
            "Loss: 8.139009e-02\n",
            "Loss: 8.137940e-02\n",
            "Loss: 8.137669e-02\n",
            "Loss: 8.136762e-02\n",
            "Loss: 8.136522e-02\n",
            "Loss: 8.136109e-02\n",
            "Loss: 8.135402e-02\n",
            "Loss: 8.134610e-02\n",
            "Loss: 8.133787e-02\n",
            "Loss: 8.133367e-02\n",
            "Loss: 8.132687e-02\n",
            "Loss: 8.132197e-02\n",
            "Loss: 8.131544e-02\n",
            "Loss: 8.130909e-02\n",
            "Loss: 8.130436e-02\n",
            "Loss: 8.129510e-02\n",
            "Loss: 8.130040e-02\n",
            "Loss: 8.129183e-02\n",
            "Loss: 8.128694e-02\n",
            "Loss: 8.128190e-02\n",
            "Loss: 8.127730e-02\n",
            "Loss: 8.127744e-02\n",
            "Loss: 8.127464e-02\n",
            "Loss: 8.127010e-02\n",
            "Loss: 8.126290e-02\n",
            "Loss: 8.125701e-02\n",
            "Loss: 8.124673e-02\n",
            "Loss: 8.124597e-02\n",
            "Loss: 8.124290e-02\n",
            "Loss: 8.123699e-02\n",
            "Loss: 8.123220e-02\n",
            "Loss: 8.122735e-02\n",
            "Loss: 8.122213e-02\n",
            "Loss: 8.122773e-02\n",
            "Loss: 8.121885e-02\n",
            "Loss: 8.121369e-02\n",
            "Loss: 8.121228e-02\n",
            "Loss: 8.120765e-02\n",
            "Loss: 8.120223e-02\n",
            "Loss: 8.119560e-02\n",
            "Loss: 8.119051e-02\n",
            "Loss: 8.118585e-02\n",
            "Loss: 8.118236e-02\n",
            "Loss: 8.118014e-02\n",
            "Loss: 8.117247e-02\n",
            "Loss: 8.118977e-02\n",
            "Loss: 8.116905e-02\n",
            "Loss: 8.116238e-02\n",
            "Loss: 8.115935e-02\n",
            "Loss: 8.115501e-02\n",
            "Loss: 8.115394e-02\n",
            "Loss: 8.115045e-02\n",
            "Loss: 8.114523e-02\n",
            "Loss: 8.114488e-02\n",
            "Loss: 8.114201e-02\n",
            "Loss: 8.114003e-02\n",
            "Loss: 8.113547e-02\n",
            "Loss: 8.112950e-02\n",
            "Loss: 8.112320e-02\n",
            "Loss: 8.111505e-02\n",
            "Loss: 8.110721e-02\n",
            "Loss: 8.110650e-02\n",
            "Loss: 8.110037e-02\n",
            "Loss: 8.109747e-02\n",
            "Loss: 8.109324e-02\n",
            "Loss: 8.109016e-02\n",
            "Loss: 8.108393e-02\n",
            "Loss: 8.107886e-02\n",
            "Loss: 8.107549e-02\n",
            "Loss: 8.107086e-02\n",
            "Loss: 8.106805e-02\n",
            "Loss: 8.106508e-02\n",
            "Loss: 8.105993e-02\n",
            "Loss: 8.105713e-02\n",
            "Loss: 8.105261e-02\n",
            "Loss: 8.104780e-02\n",
            "Loss: 8.103923e-02\n",
            "Loss: 8.103871e-02\n",
            "Loss: 8.103608e-02\n",
            "Loss: 8.103131e-02\n",
            "Loss: 8.102597e-02\n",
            "Loss: 8.102224e-02\n",
            "Loss: 8.101510e-02\n",
            "Loss: 8.100986e-02\n",
            "Loss: 8.100342e-02\n",
            "Loss: 8.099737e-02\n",
            "Loss: 8.099242e-02\n",
            "Loss: 8.098952e-02\n",
            "Loss: 8.097918e-02\n",
            "Loss: 8.102128e-02\n",
            "Loss: 8.097737e-02\n",
            "Loss: 8.097287e-02\n",
            "Loss: 8.096603e-02\n",
            "Loss: 8.095697e-02\n",
            "Loss: 8.094780e-02\n",
            "Loss: 8.094081e-02\n",
            "Loss: 8.093531e-02\n",
            "Loss: 8.093134e-02\n",
            "Loss: 8.092770e-02\n",
            "Loss: 8.092082e-02\n",
            "Loss: 8.091284e-02\n",
            "Loss: 8.091735e-02\n",
            "Loss: 8.090881e-02\n",
            "Loss: 8.090248e-02\n",
            "Loss: 8.089402e-02\n",
            "Loss: 8.088993e-02\n",
            "Loss: 8.088148e-02\n",
            "Loss: 8.087604e-02\n",
            "Loss: 8.087157e-02\n",
            "Loss: 8.086583e-02\n",
            "Loss: 8.087073e-02\n",
            "Loss: 8.086245e-02\n",
            "Loss: 8.085668e-02\n",
            "Loss: 8.085354e-02\n",
            "Loss: 8.084637e-02\n",
            "Loss: 8.084016e-02\n",
            "Loss: 8.083208e-02\n",
            "Loss: 8.082539e-02\n",
            "Loss: 8.082119e-02\n",
            "Loss: 8.081710e-02\n",
            "Loss: 8.081323e-02\n",
            "Loss: 8.080877e-02\n",
            "Loss: 8.080024e-02\n",
            "Loss: 8.079485e-02\n",
            "Loss: 8.078121e-02\n",
            "Loss: 8.077746e-02\n",
            "Loss: 8.077182e-02\n",
            "Loss: 8.076518e-02\n",
            "Loss: 8.076132e-02\n",
            "Loss: 8.075830e-02\n",
            "Loss: 8.075029e-02\n",
            "Loss: 8.074766e-02\n",
            "Loss: 8.074125e-02\n",
            "Loss: 8.073390e-02\n",
            "Loss: 8.075057e-02\n",
            "Loss: 8.073053e-02\n",
            "Loss: 8.072243e-02\n",
            "Loss: 8.071650e-02\n",
            "Loss: 8.070862e-02\n",
            "Loss: 8.070111e-02\n",
            "Loss: 8.069547e-02\n",
            "Loss: 8.068462e-02\n",
            "Loss: 8.067474e-02\n",
            "Loss: 8.066780e-02\n",
            "Loss: 8.066081e-02\n",
            "Loss: 8.073256e-02\n",
            "Loss: 8.065934e-02\n",
            "Loss: 8.065517e-02\n",
            "Loss: 8.065221e-02\n",
            "Loss: 8.064672e-02\n",
            "Loss: 8.063966e-02\n",
            "Loss: 8.062381e-02\n",
            "Loss: 8.060913e-02\n",
            "Loss: 8.059640e-02\n",
            "Loss: 8.058763e-02\n",
            "Loss: 8.058801e-02\n",
            "Loss: 8.058238e-02\n",
            "Loss: 8.057589e-02\n",
            "Loss: 8.057012e-02\n",
            "Loss: 8.056480e-02\n",
            "Loss: 8.055530e-02\n",
            "Loss: 8.055189e-02\n",
            "Loss: 8.054300e-02\n",
            "Loss: 8.053844e-02\n",
            "Loss: 8.053467e-02\n",
            "Loss: 8.053091e-02\n",
            "Loss: 8.052301e-02\n",
            "Loss: 8.051839e-02\n",
            "Loss: 8.051100e-02\n",
            "Loss: 8.050098e-02\n",
            "Loss: 8.049275e-02\n",
            "Loss: 8.048493e-02\n",
            "Loss: 8.047977e-02\n",
            "Loss: 8.047572e-02\n",
            "Loss: 8.046889e-02\n",
            "Loss: 8.046512e-02\n",
            "Loss: 8.046175e-02\n",
            "Loss: 8.045486e-02\n",
            "Loss: 8.050634e-02\n",
            "Loss: 8.045346e-02\n",
            "Loss: 8.044579e-02\n",
            "Loss: 8.044092e-02\n",
            "Loss: 8.043033e-02\n",
            "Loss: 8.042268e-02\n",
            "Loss: 8.040773e-02\n",
            "Loss: 8.043419e-02\n",
            "Loss: 8.040122e-02\n",
            "Loss: 8.039234e-02\n",
            "Loss: 8.038704e-02\n",
            "Loss: 8.038107e-02\n",
            "Loss: 8.037160e-02\n",
            "Loss: 8.036088e-02\n",
            "Loss: 8.035652e-02\n",
            "Loss: 8.034446e-02\n",
            "Loss: 8.034083e-02\n",
            "Loss: 8.033690e-02\n",
            "Loss: 8.033133e-02\n",
            "Loss: 8.032323e-02\n",
            "Loss: 8.033569e-02\n",
            "Loss: 8.031739e-02\n",
            "Loss: 8.030751e-02\n",
            "Loss: 8.029962e-02\n",
            "Loss: 8.029032e-02\n",
            "Loss: 8.030343e-02\n",
            "Loss: 8.028700e-02\n",
            "Loss: 8.028204e-02\n",
            "Loss: 8.027920e-02\n",
            "Loss: 8.027447e-02\n",
            "Loss: 8.026467e-02\n",
            "Loss: 8.024403e-02\n",
            "Loss: 8.027061e-02\n",
            "Loss: 8.023342e-02\n",
            "Loss: 8.021437e-02\n",
            "Loss: 8.020727e-02\n",
            "Loss: 8.020279e-02\n",
            "Loss: 8.019803e-02\n",
            "Loss: 8.019087e-02\n",
            "Loss: 8.018222e-02\n",
            "Loss: 8.018688e-02\n",
            "Loss: 8.017701e-02\n",
            "Loss: 8.016903e-02\n",
            "Loss: 8.016238e-02\n",
            "Loss: 8.015712e-02\n",
            "Loss: 8.015304e-02\n",
            "Loss: 8.014705e-02\n",
            "Loss: 8.014192e-02\n",
            "Loss: 8.013686e-02\n",
            "Loss: 8.012907e-02\n",
            "Loss: 8.011456e-02\n",
            "Loss: 8.009853e-02\n",
            "Loss: 8.009010e-02\n",
            "Loss: 8.008200e-02\n",
            "Loss: 8.007655e-02\n",
            "Loss: 8.007304e-02\n",
            "Loss: 8.006756e-02\n",
            "Loss: 8.006090e-02\n",
            "Loss: 8.007619e-02\n",
            "Loss: 8.005752e-02\n",
            "Loss: 8.004677e-02\n",
            "Loss: 8.003907e-02\n",
            "Loss: 8.002871e-02\n",
            "Loss: 8.001852e-02\n",
            "Loss: 8.000831e-02\n",
            "Loss: 8.000009e-02\n",
            "Loss: 7.999509e-02\n",
            "Loss: 7.999133e-02\n",
            "Loss: 7.998456e-02\n",
            "Loss: 7.997830e-02\n",
            "Loss: 7.997084e-02\n",
            "Loss: 7.995914e-02\n",
            "Loss: 7.996136e-02\n",
            "Loss: 7.995444e-02\n",
            "Loss: 7.994703e-02\n",
            "Loss: 7.993097e-02\n",
            "Loss: 7.991722e-02\n",
            "Loss: 7.990612e-02\n",
            "Loss: 7.989952e-02\n",
            "Loss: 7.988928e-02\n",
            "Loss: 7.988408e-02\n",
            "Loss: 7.987914e-02\n",
            "Loss: 7.990038e-02\n",
            "Loss: 7.987697e-02\n",
            "Loss: 7.987107e-02\n",
            "Loss: 7.986692e-02\n",
            "Loss: 7.985937e-02\n",
            "Loss: 7.985338e-02\n",
            "Loss: 7.984385e-02\n",
            "Loss: 7.983608e-02\n",
            "Loss: 7.982795e-02\n",
            "Loss: 7.982305e-02\n",
            "Loss: 7.981977e-02\n",
            "Loss: 7.981481e-02\n",
            "Loss: 7.980549e-02\n",
            "Loss: 7.979409e-02\n",
            "Loss: 7.978271e-02\n",
            "Loss: 7.977538e-02\n",
            "Loss: 7.976750e-02\n",
            "Loss: 7.974998e-02\n",
            "Loss: 7.974564e-02\n",
            "Loss: 7.972603e-02\n",
            "Loss: 7.972050e-02\n",
            "Loss: 7.971673e-02\n",
            "Loss: 7.971193e-02\n",
            "Loss: 7.970257e-02\n",
            "Loss: 7.968931e-02\n",
            "Loss: 7.970069e-02\n",
            "Loss: 7.968293e-02\n",
            "Loss: 7.967515e-02\n",
            "Loss: 7.966823e-02\n",
            "Loss: 7.965870e-02\n",
            "Loss: 7.964966e-02\n",
            "Loss: 7.963998e-02\n",
            "Loss: 7.963368e-02\n",
            "Loss: 7.963233e-02\n",
            "Loss: 7.962535e-02\n",
            "Loss: 7.961312e-02\n",
            "Loss: 7.960720e-02\n",
            "Loss: 7.960033e-02\n",
            "Loss: 7.959355e-02\n",
            "Loss: 7.958418e-02\n",
            "Loss: 7.957629e-02\n",
            "Loss: 7.956934e-02\n",
            "Loss: 7.956236e-02\n",
            "Loss: 7.955889e-02\n",
            "Loss: 7.955293e-02\n",
            "Loss: 7.954414e-02\n",
            "Loss: 7.954887e-02\n",
            "Loss: 7.953850e-02\n",
            "Loss: 7.952747e-02\n",
            "Loss: 7.951974e-02\n",
            "Loss: 7.951362e-02\n",
            "Loss: 7.950894e-02\n",
            "Loss: 7.950030e-02\n",
            "Loss: 7.949121e-02\n",
            "Loss: 7.951305e-02\n",
            "Loss: 7.948551e-02\n",
            "Loss: 7.947675e-02\n",
            "Loss: 7.946937e-02\n",
            "Loss: 7.946497e-02\n",
            "Loss: 7.945975e-02\n",
            "Loss: 7.945463e-02\n",
            "Loss: 7.944847e-02\n",
            "Loss: 7.943569e-02\n",
            "Loss: 7.943598e-02\n",
            "Loss: 7.942865e-02\n",
            "Loss: 7.941583e-02\n",
            "Loss: 7.940113e-02\n",
            "Loss: 7.943700e-02\n",
            "Loss: 7.939312e-02\n",
            "Loss: 7.937761e-02\n",
            "Loss: 7.936206e-02\n",
            "Loss: 7.934974e-02\n",
            "Loss: 7.934410e-02\n",
            "Loss: 7.933832e-02\n",
            "Loss: 7.932980e-02\n",
            "Loss: 7.932460e-02\n",
            "Loss: 7.931504e-02\n",
            "Loss: 7.931877e-02\n",
            "Loss: 7.930908e-02\n",
            "Loss: 7.929878e-02\n",
            "Loss: 7.928653e-02\n",
            "Loss: 7.927319e-02\n",
            "Loss: 7.925826e-02\n",
            "Loss: 7.936082e-02\n",
            "Loss: 7.925421e-02\n",
            "Loss: 7.924455e-02\n",
            "Loss: 7.923640e-02\n",
            "Loss: 7.922815e-02\n",
            "Loss: 7.922104e-02\n",
            "Loss: 7.921492e-02\n",
            "Loss: 7.920684e-02\n",
            "Loss: 7.920235e-02\n",
            "Loss: 7.919285e-02\n",
            "Loss: 7.918501e-02\n",
            "Loss: 7.917470e-02\n",
            "Loss: 7.916016e-02\n",
            "Loss: 7.915059e-02\n",
            "Loss: 7.914469e-02\n",
            "Loss: 7.913965e-02\n",
            "Loss: 7.912838e-02\n",
            "Loss: 7.911959e-02\n",
            "Loss: 7.910954e-02\n",
            "Loss: 7.910267e-02\n",
            "Loss: 7.909569e-02\n",
            "Loss: 7.908609e-02\n",
            "Loss: 7.907122e-02\n",
            "Loss: 7.905988e-02\n",
            "Loss: 7.905236e-02\n",
            "Loss: 7.904632e-02\n",
            "Loss: 7.903624e-02\n",
            "Loss: 7.901478e-02\n",
            "Loss: 7.923587e-02\n",
            "Loss: 7.901231e-02\n",
            "Loss: 7.900076e-02\n",
            "Loss: 7.899327e-02\n",
            "Loss: 7.898673e-02\n",
            "Loss: 7.898490e-02\n",
            "Loss: 7.897718e-02\n",
            "Loss: 7.897194e-02\n",
            "Loss: 7.896767e-02\n",
            "Loss: 7.896279e-02\n",
            "Loss: 7.895708e-02\n",
            "Loss: 7.895210e-02\n",
            "Loss: 7.894471e-02\n",
            "Loss: 7.893993e-02\n",
            "Loss: 7.893046e-02\n",
            "Loss: 7.892514e-02\n",
            "Loss: 7.891998e-02\n",
            "Loss: 7.891659e-02\n",
            "Loss: 7.891019e-02\n",
            "Loss: 7.890441e-02\n",
            "Loss: 7.890011e-02\n",
            "Loss: 7.889304e-02\n",
            "Loss: 7.888101e-02\n",
            "Loss: 7.890140e-02\n",
            "Loss: 7.887654e-02\n",
            "Loss: 7.886749e-02\n",
            "Loss: 7.885771e-02\n",
            "Loss: 7.884876e-02\n",
            "Loss: 7.884176e-02\n",
            "Loss: 7.883507e-02\n",
            "Loss: 7.882953e-02\n",
            "Loss: 7.882468e-02\n",
            "Loss: 7.881937e-02\n",
            "Loss: 7.880985e-02\n",
            "Loss: 7.881012e-02\n",
            "Loss: 7.880422e-02\n",
            "Loss: 7.879535e-02\n",
            "Loss: 7.878540e-02\n",
            "Loss: 7.877883e-02\n",
            "Loss: 7.876725e-02\n",
            "Loss: 7.876041e-02\n",
            "Loss: 7.874347e-02\n",
            "Loss: 7.873902e-02\n",
            "Loss: 7.873192e-02\n",
            "Loss: 7.872301e-02\n",
            "Loss: 7.871184e-02\n",
            "Loss: 7.871810e-02\n",
            "Loss: 7.870431e-02\n",
            "Loss: 7.869071e-02\n",
            "Loss: 7.868067e-02\n",
            "Loss: 7.867312e-02\n",
            "Loss: 7.866567e-02\n",
            "Loss: 7.865706e-02\n",
            "Loss: 7.864996e-02\n",
            "Loss: 7.863748e-02\n",
            "Loss: 7.866327e-02\n",
            "Loss: 7.863106e-02\n",
            "Loss: 7.861898e-02\n",
            "Loss: 7.861135e-02\n",
            "Loss: 7.860535e-02\n",
            "Loss: 7.860105e-02\n",
            "Loss: 7.859544e-02\n",
            "Loss: 7.859319e-02\n",
            "Loss: 7.858244e-02\n",
            "Loss: 7.857689e-02\n",
            "Loss: 7.857084e-02\n",
            "Loss: 7.856253e-02\n",
            "Loss: 7.855318e-02\n",
            "Loss: 7.854282e-02\n",
            "Loss: 7.853548e-02\n",
            "Loss: 7.851952e-02\n",
            "Loss: 7.852068e-02\n",
            "Loss: 7.851261e-02\n",
            "Loss: 7.850707e-02\n",
            "Loss: 7.849886e-02\n",
            "Loss: 7.849371e-02\n",
            "Loss: 7.848413e-02\n",
            "Loss: 7.847448e-02\n",
            "Loss: 7.846800e-02\n",
            "Loss: 7.845997e-02\n",
            "Loss: 7.844011e-02\n",
            "Loss: 7.842680e-02\n",
            "Loss: 7.841241e-02\n",
            "Loss: 7.840551e-02\n",
            "Loss: 7.839975e-02\n",
            "Loss: 7.838831e-02\n",
            "Loss: 7.849373e-02\n",
            "Loss: 7.838438e-02\n",
            "Loss: 7.837644e-02\n",
            "Loss: 7.836542e-02\n",
            "Loss: 7.835313e-02\n",
            "Loss: 7.834104e-02\n",
            "Loss: 7.833119e-02\n",
            "Loss: 7.832285e-02\n",
            "Loss: 7.831666e-02\n",
            "Loss: 7.830969e-02\n",
            "Loss: 7.830338e-02\n",
            "Loss: 7.830078e-02\n",
            "Loss: 7.829375e-02\n",
            "Loss: 7.829615e-02\n",
            "Loss: 7.828785e-02\n",
            "Loss: 7.828159e-02\n",
            "Loss: 7.827285e-02\n",
            "Loss: 7.826835e-02\n",
            "Loss: 7.825688e-02\n",
            "Loss: 7.824521e-02\n",
            "Loss: 7.823806e-02\n",
            "Loss: 7.822145e-02\n",
            "Loss: 7.821153e-02\n",
            "Loss: 7.819977e-02\n",
            "Loss: 7.818492e-02\n",
            "Loss: 7.816951e-02\n",
            "Loss: 7.816327e-02\n",
            "Loss: 7.814934e-02\n",
            "Loss: 7.814354e-02\n",
            "Loss: 7.813866e-02\n",
            "Loss: 7.813295e-02\n",
            "Loss: 7.812160e-02\n",
            "Loss: 7.812233e-02\n",
            "Loss: 7.811510e-02\n",
            "Loss: 7.810286e-02\n",
            "Loss: 7.809434e-02\n",
            "Loss: 7.808591e-02\n",
            "Loss: 7.807982e-02\n",
            "Loss: 7.807116e-02\n",
            "Loss: 7.806332e-02\n",
            "Loss: 7.805467e-02\n",
            "Loss: 7.804514e-02\n",
            "Loss: 7.804004e-02\n",
            "Loss: 7.803418e-02\n",
            "Loss: 7.802061e-02\n",
            "Loss: 7.801512e-02\n",
            "Loss: 7.800558e-02\n",
            "Loss: 7.799586e-02\n",
            "Loss: 7.798869e-02\n",
            "Loss: 7.797765e-02\n",
            "Loss: 7.796648e-02\n",
            "Loss: 7.796399e-02\n",
            "Loss: 7.795036e-02\n",
            "Loss: 7.794618e-02\n",
            "Loss: 7.793975e-02\n",
            "Loss: 7.793166e-02\n",
            "Loss: 7.791836e-02\n",
            "Loss: 7.790504e-02\n",
            "Loss: 7.789166e-02\n",
            "Loss: 7.788087e-02\n",
            "Loss: 7.786407e-02\n",
            "Loss: 7.786129e-02\n",
            "Loss: 7.785245e-02\n",
            "Loss: 7.785019e-02\n",
            "Loss: 7.784732e-02\n",
            "Loss: 7.784175e-02\n",
            "Loss: 7.789721e-02\n",
            "Loss: 7.783890e-02\n",
            "Loss: 7.783213e-02\n",
            "Loss: 7.782239e-02\n",
            "Loss: 7.781222e-02\n",
            "Loss: 7.780120e-02\n",
            "Loss: 7.780033e-02\n",
            "Loss: 7.779485e-02\n",
            "Loss: 7.778560e-02\n",
            "Loss: 7.777926e-02\n",
            "Loss: 7.777169e-02\n",
            "Loss: 7.776435e-02\n",
            "Loss: 7.775006e-02\n",
            "Loss: 7.774425e-02\n",
            "Loss: 7.773142e-02\n",
            "Loss: 7.772493e-02\n",
            "Loss: 7.771813e-02\n",
            "Loss: 7.772920e-02\n",
            "Loss: 7.771452e-02\n",
            "Loss: 7.770699e-02\n",
            "Loss: 7.770032e-02\n",
            "Loss: 7.769367e-02\n",
            "Loss: 7.768879e-02\n",
            "Loss: 7.768514e-02\n",
            "Loss: 7.767809e-02\n",
            "Loss: 7.766959e-02\n",
            "Loss: 7.766080e-02\n",
            "Loss: 7.764810e-02\n",
            "Loss: 7.763972e-02\n",
            "Loss: 7.762747e-02\n",
            "Loss: 7.761737e-02\n",
            "Loss: 7.760783e-02\n",
            "Loss: 7.760558e-02\n",
            "Loss: 7.759441e-02\n",
            "Loss: 7.758871e-02\n",
            "Loss: 7.757999e-02\n",
            "Loss: 7.756793e-02\n",
            "Loss: 7.755230e-02\n",
            "Loss: 7.754476e-02\n",
            "Loss: 7.753898e-02\n",
            "Loss: 7.753349e-02\n",
            "Loss: 7.752860e-02\n",
            "Loss: 7.752393e-02\n",
            "Loss: 7.751953e-02\n",
            "Loss: 7.751653e-02\n",
            "Loss: 7.750685e-02\n",
            "Loss: 7.749631e-02\n",
            "Loss: 7.748614e-02\n",
            "Loss: 7.747732e-02\n",
            "Loss: 7.746558e-02\n",
            "Loss: 7.745524e-02\n",
            "Loss: 7.749063e-02\n",
            "Loss: 7.745028e-02\n",
            "Loss: 7.743631e-02\n",
            "Loss: 7.742531e-02\n",
            "Loss: 7.741681e-02\n",
            "Loss: 7.741159e-02\n",
            "Loss: 7.740435e-02\n",
            "Loss: 7.739627e-02\n",
            "Loss: 7.738762e-02\n",
            "Loss: 7.737985e-02\n",
            "Loss: 7.737078e-02\n",
            "Loss: 7.736081e-02\n",
            "Loss: 7.735607e-02\n",
            "Loss: 7.734965e-02\n",
            "Loss: 7.734267e-02\n",
            "Loss: 7.733654e-02\n",
            "Loss: 7.732973e-02\n",
            "Loss: 7.732214e-02\n",
            "Loss: 7.731418e-02\n",
            "Loss: 7.730464e-02\n",
            "Loss: 7.732455e-02\n",
            "Loss: 7.729711e-02\n",
            "Loss: 7.728235e-02\n",
            "Loss: 7.727345e-02\n",
            "Loss: 7.726314e-02\n",
            "Loss: 7.725460e-02\n",
            "Loss: 7.724330e-02\n",
            "Loss: 7.723571e-02\n",
            "Loss: 7.722242e-02\n",
            "Loss: 7.721177e-02\n",
            "Loss: 7.719687e-02\n",
            "Loss: 7.717765e-02\n",
            "Loss: 7.716966e-02\n",
            "Loss: 7.715990e-02\n",
            "Loss: 7.715487e-02\n",
            "Loss: 7.714965e-02\n",
            "Loss: 7.713838e-02\n",
            "Loss: 7.716282e-02\n",
            "Loss: 7.713261e-02\n",
            "Loss: 7.711882e-02\n",
            "Loss: 7.710867e-02\n",
            "Loss: 7.710440e-02\n",
            "Loss: 7.709607e-02\n",
            "Loss: 7.708891e-02\n",
            "Loss: 7.707726e-02\n",
            "Loss: 7.707454e-02\n",
            "Loss: 7.705942e-02\n",
            "Loss: 7.705481e-02\n",
            "Loss: 7.704370e-02\n",
            "Loss: 7.703465e-02\n",
            "Loss: 7.702880e-02\n",
            "Loss: 7.702406e-02\n",
            "Loss: 7.701731e-02\n",
            "Loss: 7.700660e-02\n",
            "Loss: 7.700034e-02\n",
            "Loss: 7.700438e-02\n",
            "Loss: 7.699323e-02\n",
            "Loss: 7.698465e-02\n",
            "Loss: 7.697646e-02\n",
            "Loss: 7.696834e-02\n",
            "Loss: 7.695957e-02\n",
            "Loss: 7.695256e-02\n",
            "Loss: 7.694461e-02\n",
            "Loss: 7.694051e-02\n",
            "Loss: 7.693355e-02\n",
            "Loss: 7.693105e-02\n",
            "Loss: 7.692818e-02\n",
            "Loss: 7.692386e-02\n",
            "Loss: 7.691870e-02\n",
            "Loss: 7.691269e-02\n",
            "Loss: 7.690457e-02\n",
            "Loss: 7.689553e-02\n",
            "Loss: 7.688781e-02\n",
            "Loss: 7.688005e-02\n",
            "Loss: 7.687394e-02\n",
            "Loss: 7.687639e-02\n",
            "Loss: 7.687148e-02\n",
            "Loss: 7.686773e-02\n",
            "Loss: 7.686200e-02\n",
            "Loss: 7.685287e-02\n",
            "Loss: 7.683828e-02\n",
            "Loss: 7.682826e-02\n",
            "Loss: 7.684115e-02\n",
            "Loss: 7.681859e-02\n",
            "Loss: 7.681017e-02\n",
            "Loss: 7.680558e-02\n",
            "Loss: 7.679990e-02\n",
            "Loss: 7.678538e-02\n",
            "Loss: 7.677118e-02\n",
            "Loss: 7.676023e-02\n",
            "Loss: 7.675210e-02\n",
            "Loss: 7.674333e-02\n",
            "Loss: 7.673538e-02\n",
            "Loss: 7.672562e-02\n",
            "Loss: 7.671750e-02\n",
            "Loss: 7.671167e-02\n",
            "Loss: 7.670180e-02\n",
            "Loss: 7.669538e-02\n",
            "Loss: 7.668605e-02\n",
            "Loss: 7.667352e-02\n",
            "Loss: 7.666409e-02\n",
            "Loss: 7.665931e-02\n",
            "Loss: 7.664794e-02\n",
            "Loss: 7.664354e-02\n",
            "Loss: 7.663830e-02\n",
            "Loss: 7.663088e-02\n",
            "Loss: 7.664204e-02\n",
            "Loss: 7.662649e-02\n",
            "Loss: 7.661500e-02\n",
            "Loss: 7.660903e-02\n",
            "Loss: 7.659879e-02\n",
            "Loss: 7.659477e-02\n",
            "Loss: 7.658906e-02\n",
            "Loss: 7.658269e-02\n",
            "Loss: 7.657798e-02\n",
            "Loss: 7.657473e-02\n",
            "Loss: 7.656740e-02\n",
            "Loss: 7.656492e-02\n",
            "Loss: 7.656147e-02\n",
            "Loss: 7.655605e-02\n",
            "Loss: 7.656854e-02\n",
            "Loss: 7.655182e-02\n",
            "Loss: 7.654522e-02\n",
            "Loss: 7.653783e-02\n",
            "Loss: 7.653422e-02\n",
            "Loss: 7.652897e-02\n",
            "Loss: 7.652581e-02\n",
            "Loss: 7.652090e-02\n",
            "Loss: 7.651369e-02\n",
            "Loss: 7.650091e-02\n",
            "Loss: 7.650103e-02\n",
            "Loss: 7.649330e-02\n",
            "Loss: 7.648683e-02\n",
            "Loss: 7.648233e-02\n",
            "Loss: 7.647865e-02\n",
            "Loss: 7.647507e-02\n",
            "Loss: 7.645879e-02\n",
            "Loss: 7.644963e-02\n",
            "Loss: 7.645846e-02\n",
            "Loss: 7.644641e-02\n",
            "Loss: 7.643926e-02\n",
            "Loss: 7.643653e-02\n",
            "Loss: 7.643080e-02\n",
            "Loss: 7.642582e-02\n",
            "Loss: 7.641886e-02\n",
            "Loss: 7.641418e-02\n",
            "Loss: 7.640931e-02\n",
            "Loss: 7.640722e-02\n",
            "Loss: 7.640406e-02\n",
            "Loss: 7.639664e-02\n",
            "Loss: 7.638490e-02\n",
            "Loss: 7.637441e-02\n",
            "Loss: 7.636616e-02\n",
            "Loss: 7.636118e-02\n",
            "Loss: 7.635649e-02\n",
            "Loss: 7.634345e-02\n",
            "Loss: 7.634136e-02\n",
            "Loss: 7.632957e-02\n",
            "Loss: 7.632602e-02\n",
            "Loss: 7.632351e-02\n",
            "Loss: 7.631845e-02\n",
            "Loss: 7.631475e-02\n",
            "Loss: 7.630959e-02\n",
            "Loss: 7.630303e-02\n",
            "Loss: 7.629347e-02\n",
            "Loss: 7.628472e-02\n",
            "Loss: 7.627831e-02\n",
            "Loss: 7.627080e-02\n",
            "Loss: 7.626417e-02\n",
            "Loss: 7.625901e-02\n",
            "Loss: 7.625158e-02\n",
            "Loss: 7.624463e-02\n",
            "Loss: 7.623820e-02\n",
            "Loss: 7.623131e-02\n",
            "Loss: 7.622565e-02\n",
            "Loss: 7.621514e-02\n",
            "Loss: 7.621086e-02\n",
            "Loss: 7.620056e-02\n",
            "Loss: 7.619447e-02\n",
            "Loss: 7.618835e-02\n",
            "Loss: 7.618945e-02\n",
            "Loss: 7.618339e-02\n",
            "Loss: 7.617525e-02\n",
            "Loss: 7.616490e-02\n",
            "Loss: 7.615791e-02\n",
            "Loss: 7.614920e-02\n",
            "Loss: 7.613640e-02\n",
            "Loss: 7.612802e-02\n",
            "Loss: 7.612016e-02\n",
            "Loss: 7.611134e-02\n",
            "Loss: 7.610427e-02\n",
            "Loss: 7.609978e-02\n",
            "Loss: 7.609740e-02\n",
            "Loss: 7.608847e-02\n",
            "Loss: 7.608606e-02\n",
            "Loss: 7.607939e-02\n",
            "Loss: 7.606889e-02\n",
            "Loss: 7.606048e-02\n",
            "Loss: 7.605387e-02\n",
            "Loss: 7.604580e-02\n",
            "Loss: 7.603942e-02\n",
            "Loss: 7.603446e-02\n",
            "Loss: 7.602568e-02\n",
            "Loss: 7.601321e-02\n",
            "Loss: 7.603992e-02\n",
            "Loss: 7.600962e-02\n",
            "Loss: 7.600476e-02\n",
            "Loss: 7.600129e-02\n",
            "Loss: 7.599396e-02\n",
            "Loss: 7.598393e-02\n",
            "Loss: 7.596718e-02\n",
            "Loss: 7.598044e-02\n",
            "Loss: 7.596150e-02\n",
            "Loss: 7.594922e-02\n",
            "Loss: 7.594251e-02\n",
            "Loss: 7.593578e-02\n",
            "Loss: 7.592690e-02\n",
            "Loss: 7.591064e-02\n",
            "Loss: 7.589939e-02\n",
            "Loss: 7.588866e-02\n",
            "Loss: 7.587714e-02\n",
            "Loss: 7.586840e-02\n",
            "Loss: 7.585679e-02\n",
            "Loss: 7.584430e-02\n",
            "Loss: 7.583664e-02\n",
            "Loss: 7.582359e-02\n",
            "Loss: 7.583125e-02\n",
            "Loss: 7.581988e-02\n",
            "Loss: 7.581186e-02\n",
            "Loss: 7.579745e-02\n",
            "Loss: 7.578307e-02\n",
            "Loss: 7.576795e-02\n",
            "Loss: 7.575225e-02\n",
            "Loss: 7.574968e-02\n",
            "Loss: 7.573357e-02\n",
            "Loss: 7.572857e-02\n",
            "Loss: 7.571844e-02\n",
            "Loss: 7.571930e-02\n",
            "Loss: 7.571172e-02\n",
            "Loss: 7.570095e-02\n",
            "Loss: 7.569070e-02\n",
            "Loss: 7.568067e-02\n",
            "Loss: 7.566586e-02\n",
            "Loss: 7.569336e-02\n",
            "Loss: 7.565943e-02\n",
            "Loss: 7.564694e-02\n",
            "Loss: 7.563903e-02\n",
            "Loss: 7.563592e-02\n",
            "Loss: 7.563280e-02\n",
            "Loss: 7.562925e-02\n",
            "Loss: 7.562602e-02\n",
            "Loss: 7.562070e-02\n",
            "Loss: 7.561669e-02\n",
            "Loss: 7.560746e-02\n",
            "Loss: 7.560068e-02\n",
            "Loss: 7.558650e-02\n",
            "Loss: 7.557656e-02\n",
            "Loss: 7.557080e-02\n",
            "Loss: 7.555749e-02\n",
            "Loss: 7.555413e-02\n",
            "Loss: 7.554764e-02\n",
            "Loss: 7.554040e-02\n",
            "Loss: 7.553054e-02\n",
            "Loss: 7.552049e-02\n",
            "Loss: 7.551124e-02\n",
            "Loss: 7.550449e-02\n",
            "Loss: 7.549738e-02\n",
            "Loss: 7.548379e-02\n",
            "Loss: 7.548510e-02\n",
            "Loss: 7.547683e-02\n",
            "Loss: 7.547164e-02\n",
            "Loss: 7.546918e-02\n",
            "Loss: 7.546431e-02\n",
            "Loss: 7.546163e-02\n",
            "Loss: 7.545692e-02\n",
            "Loss: 7.545254e-02\n",
            "Loss: 7.543938e-02\n",
            "Loss: 7.543287e-02\n",
            "Loss: 7.542476e-02\n",
            "Loss: 7.541979e-02\n",
            "Loss: 7.541408e-02\n",
            "Loss: 7.540604e-02\n",
            "Loss: 7.541056e-02\n",
            "Loss: 7.540005e-02\n",
            "Loss: 7.539380e-02\n",
            "Loss: 7.538689e-02\n",
            "Loss: 7.538270e-02\n",
            "Loss: 7.537210e-02\n",
            "Loss: 7.537168e-02\n",
            "Loss: 7.536648e-02\n",
            "Loss: 7.535956e-02\n",
            "Loss: 7.535334e-02\n",
            "Loss: 7.534897e-02\n",
            "Loss: 7.534230e-02\n",
            "Loss: 7.533811e-02\n",
            "Loss: 7.533266e-02\n",
            "Loss: 7.532962e-02\n",
            "Loss: 7.532551e-02\n",
            "Loss: 7.531691e-02\n",
            "Loss: 7.530710e-02\n",
            "Loss: 7.530589e-02\n",
            "Loss: 7.530019e-02\n",
            "Loss: 7.529417e-02\n",
            "Loss: 7.529233e-02\n",
            "Loss: 7.528602e-02\n",
            "Loss: 7.528385e-02\n",
            "Loss: 7.527816e-02\n",
            "Loss: 7.527360e-02\n",
            "Loss: 7.526522e-02\n",
            "Loss: 7.526006e-02\n",
            "Loss: 7.525184e-02\n",
            "Loss: 7.524871e-02\n",
            "Loss: 7.524576e-02\n",
            "Loss: 7.524030e-02\n",
            "Loss: 7.522902e-02\n",
            "Loss: 7.526015e-02\n",
            "Loss: 7.522403e-02\n",
            "Loss: 7.521307e-02\n",
            "Loss: 7.520609e-02\n",
            "Loss: 7.520232e-02\n",
            "Loss: 7.519671e-02\n",
            "Loss: 7.519217e-02\n",
            "Loss: 7.518861e-02\n",
            "Loss: 7.518378e-02\n",
            "Loss: 7.517703e-02\n",
            "Loss: 7.516766e-02\n",
            "Loss: 7.516883e-02\n",
            "Loss: 7.516187e-02\n",
            "Loss: 7.515251e-02\n",
            "Loss: 7.514419e-02\n",
            "Loss: 7.515577e-02\n",
            "Loss: 7.513921e-02\n",
            "Loss: 7.512975e-02\n",
            "Loss: 7.511812e-02\n",
            "Loss: 7.510645e-02\n",
            "Loss: 7.509908e-02\n",
            "Loss: 7.509304e-02\n",
            "Loss: 7.508698e-02\n",
            "Loss: 7.508130e-02\n",
            "Loss: 7.507639e-02\n",
            "Loss: 7.506721e-02\n",
            "Loss: 7.516393e-02\n",
            "Loss: 7.506359e-02\n",
            "Loss: 7.505784e-02\n",
            "Loss: 7.504991e-02\n",
            "Loss: 7.503848e-02\n",
            "Loss: 7.504949e-02\n",
            "Loss: 7.503417e-02\n",
            "Loss: 7.502791e-02\n",
            "Loss: 7.502131e-02\n",
            "Loss: 7.501625e-02\n",
            "Loss: 7.500885e-02\n",
            "Loss: 7.500232e-02\n",
            "Loss: 7.499756e-02\n",
            "Loss: 7.499055e-02\n",
            "Loss: 7.498023e-02\n",
            "Loss: 7.497342e-02\n",
            "Loss: 7.496853e-02\n",
            "Loss: 7.496308e-02\n",
            "Loss: 7.495949e-02\n",
            "Loss: 7.495447e-02\n",
            "Loss: 7.494707e-02\n",
            "Loss: 7.495586e-02\n",
            "Loss: 7.494517e-02\n",
            "Loss: 7.494143e-02\n",
            "Loss: 7.493656e-02\n",
            "Loss: 7.493199e-02\n",
            "Loss: 7.492615e-02\n",
            "Loss: 7.492691e-02\n",
            "Loss: 7.492238e-02\n",
            "Loss: 7.491488e-02\n",
            "Loss: 7.490989e-02\n",
            "Loss: 7.490290e-02\n",
            "Loss: 7.489470e-02\n",
            "Loss: 7.489027e-02\n",
            "Loss: 7.487915e-02\n",
            "Loss: 7.487373e-02\n",
            "Loss: 7.486574e-02\n",
            "Loss: 7.489436e-02\n",
            "Loss: 7.486322e-02\n",
            "Loss: 7.485535e-02\n",
            "Loss: 7.484851e-02\n",
            "Loss: 7.484160e-02\n",
            "Loss: 7.483484e-02\n",
            "Loss: 7.482683e-02\n",
            "Loss: 7.481674e-02\n",
            "Loss: 7.481423e-02\n",
            "Loss: 7.480353e-02\n",
            "Loss: 7.479822e-02\n",
            "Loss: 7.479175e-02\n",
            "Loss: 7.478390e-02\n",
            "Loss: 7.477669e-02\n",
            "Loss: 7.477327e-02\n",
            "Loss: 7.476740e-02\n",
            "Loss: 7.477874e-02\n",
            "Loss: 7.476463e-02\n",
            "Loss: 7.475748e-02\n",
            "Loss: 7.474765e-02\n",
            "Loss: 7.473745e-02\n",
            "Loss: 7.476798e-02\n",
            "Loss: 7.473423e-02\n",
            "Loss: 7.472626e-02\n",
            "Loss: 7.471941e-02\n",
            "Loss: 7.471206e-02\n",
            "Loss: 7.470556e-02\n",
            "Loss: 7.469447e-02\n",
            "Loss: 7.468586e-02\n",
            "Loss: 7.467676e-02\n",
            "Loss: 7.466978e-02\n",
            "Loss: 7.466545e-02\n",
            "Loss: 7.466036e-02\n",
            "Loss: 7.465441e-02\n",
            "Loss: 7.464231e-02\n",
            "Loss: 7.464585e-02\n",
            "Loss: 7.463847e-02\n",
            "Loss: 7.463242e-02\n",
            "Loss: 7.462712e-02\n",
            "Loss: 7.461793e-02\n",
            "Loss: 7.462034e-02\n",
            "Loss: 7.461329e-02\n",
            "Loss: 7.460634e-02\n",
            "Loss: 7.460034e-02\n",
            "Loss: 7.460772e-02\n",
            "Loss: 7.459930e-02\n",
            "Loss: 7.459649e-02\n",
            "Loss: 7.459043e-02\n",
            "Loss: 7.458156e-02\n",
            "Loss: 7.459220e-02\n",
            "Loss: 7.457975e-02\n",
            "Loss: 7.457585e-02\n",
            "Loss: 7.457150e-02\n",
            "Loss: 7.456603e-02\n",
            "Loss: 7.456034e-02\n",
            "Loss: 7.454926e-02\n",
            "Loss: 7.455099e-02\n",
            "Loss: 7.454259e-02\n",
            "Loss: 7.452875e-02\n",
            "Loss: 7.452063e-02\n",
            "Loss: 7.451466e-02\n",
            "Loss: 7.452781e-02\n",
            "Loss: 7.451187e-02\n",
            "Loss: 7.450762e-02\n",
            "Loss: 7.449715e-02\n",
            "Loss: 7.449104e-02\n",
            "Loss: 7.447691e-02\n",
            "Loss: 7.447241e-02\n",
            "Loss: 7.444717e-02\n",
            "Loss: 7.444084e-02\n",
            "Loss: 7.443629e-02\n",
            "Loss: 7.443197e-02\n",
            "Loss: 7.442781e-02\n",
            "Loss: 7.441700e-02\n",
            "Loss: 7.441266e-02\n",
            "Loss: 7.440723e-02\n",
            "Loss: 7.440406e-02\n",
            "Loss: 7.439902e-02\n",
            "Loss: 7.439504e-02\n",
            "Loss: 7.439188e-02\n",
            "Loss: 7.438771e-02\n",
            "Loss: 7.438413e-02\n",
            "Loss: 7.437937e-02\n",
            "Loss: 7.437371e-02\n",
            "Loss: 7.436965e-02\n",
            "Loss: 7.436284e-02\n",
            "Loss: 7.435243e-02\n",
            "Loss: 7.434309e-02\n",
            "Loss: 7.432445e-02\n",
            "Loss: 7.431653e-02\n",
            "Loss: 7.431318e-02\n",
            "Loss: 7.430544e-02\n",
            "Loss: 7.430136e-02\n",
            "Loss: 7.429817e-02\n",
            "Loss: 7.429194e-02\n",
            "Loss: 7.428388e-02\n",
            "Loss: 7.427684e-02\n",
            "Loss: 7.427461e-02\n",
            "Loss: 7.426858e-02\n",
            "Loss: 7.426642e-02\n",
            "Loss: 7.426250e-02\n",
            "Loss: 7.425791e-02\n",
            "Loss: 7.425281e-02\n",
            "Loss: 7.424497e-02\n",
            "Loss: 7.424310e-02\n",
            "Loss: 7.423714e-02\n",
            "Loss: 7.423271e-02\n",
            "Loss: 7.422783e-02\n",
            "Loss: 7.422087e-02\n",
            "Loss: 7.420956e-02\n",
            "Loss: 7.424569e-02\n",
            "Loss: 7.420639e-02\n",
            "Loss: 7.420073e-02\n",
            "Loss: 7.419115e-02\n",
            "Loss: 7.418365e-02\n",
            "Loss: 7.419350e-02\n",
            "Loss: 7.418010e-02\n",
            "Loss: 7.417604e-02\n",
            "Loss: 7.417124e-02\n",
            "Loss: 7.416364e-02\n",
            "Loss: 7.415525e-02\n",
            "Loss: 7.415656e-02\n",
            "Loss: 7.415032e-02\n",
            "Loss: 7.414280e-02\n",
            "Loss: 7.413768e-02\n",
            "Loss: 7.413261e-02\n",
            "Loss: 7.412489e-02\n",
            "Loss: 7.411405e-02\n",
            "Loss: 7.410648e-02\n",
            "Loss: 7.409875e-02\n",
            "Loss: 7.409316e-02\n",
            "Loss: 7.408296e-02\n",
            "Loss: 7.408161e-02\n",
            "Loss: 7.407239e-02\n",
            "Loss: 7.406948e-02\n",
            "Loss: 7.406427e-02\n",
            "Loss: 7.407144e-02\n",
            "Loss: 7.406240e-02\n",
            "Loss: 7.405725e-02\n",
            "Loss: 7.405138e-02\n",
            "Loss: 7.404502e-02\n",
            "Loss: 7.403899e-02\n",
            "Loss: 7.403041e-02\n",
            "Loss: 7.402496e-02\n",
            "Loss: 7.401612e-02\n",
            "Loss: 7.401114e-02\n",
            "Loss: 7.400420e-02\n",
            "Loss: 7.399803e-02\n",
            "Loss: 7.399225e-02\n",
            "Loss: 7.398424e-02\n",
            "Loss: 7.398094e-02\n",
            "Loss: 7.397187e-02\n",
            "Loss: 7.396622e-02\n",
            "Loss: 7.396170e-02\n",
            "Loss: 7.395786e-02\n",
            "Loss: 7.395528e-02\n",
            "Loss: 7.395183e-02\n",
            "Loss: 7.394820e-02\n",
            "Loss: 7.394410e-02\n",
            "Loss: 7.393938e-02\n",
            "Loss: 7.393436e-02\n",
            "Loss: 7.392682e-02\n",
            "Loss: 7.391781e-02\n",
            "Loss: 7.391328e-02\n",
            "Loss: 7.390530e-02\n",
            "Loss: 7.389913e-02\n",
            "Loss: 7.388953e-02\n",
            "Loss: 7.388332e-02\n",
            "Loss: 7.387508e-02\n",
            "Loss: 7.386960e-02\n",
            "Loss: 7.386415e-02\n",
            "Loss: 7.385852e-02\n",
            "Loss: 7.385135e-02\n",
            "Loss: 7.384038e-02\n",
            "Loss: 7.382533e-02\n",
            "Loss: 7.381525e-02\n",
            "Loss: 7.380774e-02\n",
            "Loss: 7.380188e-02\n",
            "Loss: 7.379594e-02\n",
            "Loss: 7.379200e-02\n",
            "Loss: 7.378226e-02\n",
            "Loss: 7.377687e-02\n",
            "Loss: 7.376938e-02\n",
            "Loss: 7.376512e-02\n",
            "Loss: 7.376248e-02\n",
            "Loss: 7.375739e-02\n",
            "Loss: 7.375062e-02\n",
            "Loss: 7.374437e-02\n",
            "Loss: 7.373893e-02\n",
            "Loss: 7.373430e-02\n",
            "Loss: 7.373063e-02\n",
            "Loss: 7.372524e-02\n",
            "Loss: 7.372137e-02\n",
            "Loss: 7.371808e-02\n",
            "Loss: 7.370920e-02\n",
            "Loss: 7.370153e-02\n",
            "Loss: 7.369559e-02\n",
            "Loss: 7.369041e-02\n",
            "Loss: 7.368252e-02\n",
            "Loss: 7.368119e-02\n",
            "Loss: 7.367358e-02\n",
            "Loss: 7.366958e-02\n",
            "Loss: 7.366481e-02\n",
            "Loss: 7.365932e-02\n",
            "Loss: 7.366126e-02\n",
            "Loss: 7.365601e-02\n",
            "Loss: 7.364844e-02\n",
            "Loss: 7.364426e-02\n",
            "Loss: 7.363513e-02\n",
            "Loss: 7.363978e-02\n",
            "Loss: 7.363197e-02\n",
            "Loss: 7.362835e-02\n",
            "Loss: 7.362404e-02\n",
            "Loss: 7.362144e-02\n",
            "Loss: 7.361527e-02\n",
            "Loss: 7.364405e-02\n",
            "Loss: 7.361338e-02\n",
            "Loss: 7.361084e-02\n",
            "Loss: 7.360651e-02\n",
            "Loss: 7.360151e-02\n",
            "Loss: 7.359358e-02\n",
            "Loss: 7.358640e-02\n",
            "Loss: 7.357854e-02\n",
            "Loss: 7.357306e-02\n",
            "Loss: 7.356863e-02\n",
            "Loss: 7.356598e-02\n",
            "Loss: 7.356344e-02\n",
            "Loss: 7.355806e-02\n",
            "Loss: 7.355469e-02\n",
            "Loss: 7.354616e-02\n",
            "Loss: 7.353790e-02\n",
            "Loss: 7.354243e-02\n",
            "Loss: 7.353331e-02\n",
            "Loss: 7.352696e-02\n",
            "Loss: 7.352372e-02\n",
            "Loss: 7.351949e-02\n",
            "Loss: 7.351353e-02\n",
            "Loss: 7.350574e-02\n",
            "Loss: 7.349885e-02\n",
            "Loss: 7.349338e-02\n",
            "Loss: 7.348786e-02\n",
            "Loss: 7.348163e-02\n",
            "Loss: 7.347531e-02\n",
            "Loss: 7.347193e-02\n",
            "Loss: 7.346636e-02\n",
            "Loss: 7.345945e-02\n",
            "Loss: 7.344764e-02\n",
            "Loss: 7.345770e-02\n",
            "Loss: 7.344270e-02\n",
            "Loss: 7.343780e-02\n",
            "Loss: 7.343388e-02\n",
            "Loss: 7.343078e-02\n",
            "Loss: 7.342213e-02\n",
            "Loss: 7.340808e-02\n",
            "Loss: 7.339394e-02\n",
            "Loss: 7.338411e-02\n",
            "Loss: 7.337765e-02\n",
            "Loss: 7.337693e-02\n",
            "Loss: 7.337213e-02\n",
            "Loss: 7.336930e-02\n",
            "Loss: 7.336539e-02\n",
            "Loss: 7.336070e-02\n",
            "Loss: 7.335424e-02\n",
            "Loss: 7.334972e-02\n",
            "Loss: 7.334594e-02\n",
            "Loss: 7.334274e-02\n",
            "Loss: 7.334097e-02\n",
            "Loss: 7.333523e-02\n",
            "Loss: 7.333585e-02\n",
            "Loss: 7.333206e-02\n",
            "Loss: 7.332698e-02\n",
            "Loss: 7.332776e-02\n",
            "Loss: 7.332373e-02\n",
            "Loss: 7.332022e-02\n",
            "Loss: 7.331719e-02\n",
            "Loss: 7.331224e-02\n",
            "Loss: 7.330451e-02\n",
            "Loss: 7.329312e-02\n",
            "Loss: 7.328909e-02\n",
            "Loss: 7.327656e-02\n",
            "Loss: 7.327181e-02\n",
            "Loss: 7.326595e-02\n",
            "Loss: 7.325928e-02\n",
            "Loss: 7.325441e-02\n",
            "Loss: 7.324255e-02\n",
            "Loss: 7.323429e-02\n",
            "Loss: 7.322243e-02\n",
            "Loss: 7.321254e-02\n",
            "Loss: 7.320748e-02\n",
            "Loss: 7.320396e-02\n",
            "Loss: 7.320106e-02\n",
            "Loss: 7.319798e-02\n",
            "Loss: 7.319301e-02\n",
            "Loss: 7.318569e-02\n",
            "Loss: 7.319371e-02\n",
            "Loss: 7.318179e-02\n",
            "Loss: 7.317691e-02\n",
            "Loss: 7.317320e-02\n",
            "Loss: 7.316743e-02\n",
            "Loss: 7.316423e-02\n",
            "Loss: 7.315876e-02\n",
            "Loss: 7.315298e-02\n",
            "Loss: 7.314921e-02\n",
            "Loss: 7.314133e-02\n",
            "Loss: 7.313025e-02\n",
            "Loss: 7.313016e-02\n",
            "Loss: 7.312208e-02\n",
            "Loss: 7.311434e-02\n",
            "Loss: 7.310878e-02\n",
            "Loss: 7.310342e-02\n",
            "Loss: 7.308903e-02\n",
            "Loss: 7.310513e-02\n",
            "Loss: 7.308146e-02\n",
            "Loss: 7.306802e-02\n",
            "Loss: 7.305880e-02\n",
            "Loss: 7.305359e-02\n",
            "Loss: 7.304476e-02\n",
            "Loss: 7.303868e-02\n",
            "Loss: 7.303295e-02\n",
            "Loss: 7.302707e-02\n",
            "Loss: 7.301601e-02\n",
            "Loss: 7.306268e-02\n",
            "Loss: 7.301232e-02\n",
            "Loss: 7.300185e-02\n",
            "Loss: 7.299216e-02\n",
            "Loss: 7.298091e-02\n",
            "Loss: 7.297544e-02\n",
            "Loss: 7.296857e-02\n",
            "Loss: 7.296493e-02\n",
            "Loss: 7.296183e-02\n",
            "Loss: 7.295886e-02\n",
            "Loss: 7.295617e-02\n",
            "Loss: 7.295265e-02\n",
            "Loss: 7.295059e-02\n",
            "Loss: 7.294459e-02\n",
            "Loss: 7.295169e-02\n",
            "Loss: 7.294055e-02\n",
            "Loss: 7.293505e-02\n",
            "Loss: 7.293982e-02\n",
            "Loss: 7.293292e-02\n",
            "Loss: 7.292894e-02\n",
            "Loss: 7.292508e-02\n",
            "Loss: 7.291821e-02\n",
            "Loss: 7.291239e-02\n",
            "Loss: 7.290536e-02\n",
            "Loss: 7.290036e-02\n",
            "Loss: 7.289242e-02\n",
            "Loss: 7.288773e-02\n",
            "Loss: 7.287960e-02\n",
            "Loss: 7.287288e-02\n",
            "Loss: 7.288340e-02\n",
            "Loss: 7.286684e-02\n",
            "Loss: 7.285770e-02\n",
            "Loss: 7.285219e-02\n",
            "Loss: 7.284759e-02\n",
            "Loss: 7.284084e-02\n",
            "Loss: 7.283145e-02\n",
            "Loss: 7.281980e-02\n",
            "Loss: 7.281040e-02\n",
            "Loss: 7.280110e-02\n",
            "Loss: 7.279229e-02\n",
            "Loss: 7.278699e-02\n",
            "Loss: 7.277924e-02\n",
            "Loss: 7.277772e-02\n",
            "Loss: 7.277334e-02\n",
            "Loss: 7.277071e-02\n",
            "Loss: 7.276651e-02\n",
            "Loss: 7.276072e-02\n",
            "Loss: 7.275414e-02\n",
            "Loss: 7.274475e-02\n",
            "Loss: 7.274094e-02\n",
            "Loss: 7.273407e-02\n",
            "Loss: 7.272867e-02\n",
            "Loss: 7.275204e-02\n",
            "Loss: 7.272591e-02\n",
            "Loss: 7.271858e-02\n",
            "Loss: 7.271098e-02\n",
            "Loss: 7.270335e-02\n",
            "Loss: 7.270770e-02\n",
            "Loss: 7.270077e-02\n",
            "Loss: 7.269733e-02\n",
            "Loss: 7.269290e-02\n",
            "Loss: 7.268711e-02\n",
            "Loss: 7.268211e-02\n",
            "Loss: 7.267933e-02\n",
            "Loss: 7.267630e-02\n",
            "Loss: 7.267302e-02\n",
            "Loss: 7.266741e-02\n",
            "Loss: 7.266217e-02\n",
            "Loss: 7.265664e-02\n",
            "Loss: 7.265356e-02\n",
            "Loss: 7.264910e-02\n",
            "Loss: 7.264392e-02\n",
            "Loss: 7.263356e-02\n",
            "Loss: 7.268071e-02\n",
            "Loss: 7.263085e-02\n",
            "Loss: 7.262518e-02\n",
            "Loss: 7.262114e-02\n",
            "Loss: 7.261945e-02\n",
            "Loss: 7.261482e-02\n",
            "Loss: 7.260729e-02\n",
            "Loss: 7.262333e-02\n",
            "Loss: 7.260338e-02\n",
            "Loss: 7.259483e-02\n",
            "Loss: 7.258961e-02\n",
            "Loss: 7.258600e-02\n",
            "Loss: 7.258309e-02\n",
            "Loss: 7.257788e-02\n",
            "Loss: 7.257244e-02\n",
            "Loss: 7.256544e-02\n",
            "Loss: 7.256833e-02\n",
            "Loss: 7.256186e-02\n",
            "Loss: 7.255633e-02\n",
            "Loss: 7.254952e-02\n",
            "Loss: 7.254575e-02\n",
            "Loss: 7.253852e-02\n",
            "Loss: 7.252861e-02\n",
            "Loss: 7.253537e-02\n",
            "Loss: 7.252297e-02\n",
            "Loss: 7.251559e-02\n",
            "Loss: 7.251094e-02\n",
            "Loss: 7.250600e-02\n",
            "Loss: 7.250591e-02\n",
            "Loss: 7.250231e-02\n",
            "Loss: 7.249757e-02\n",
            "Loss: 7.249520e-02\n",
            "Loss: 7.249068e-02\n",
            "Loss: 7.248546e-02\n",
            "Loss: 7.248679e-02\n",
            "Loss: 7.248162e-02\n",
            "Loss: 7.247487e-02\n",
            "Loss: 7.246923e-02\n",
            "Loss: 7.246377e-02\n",
            "Loss: 7.245739e-02\n",
            "Loss: 7.245141e-02\n",
            "Loss: 7.244147e-02\n",
            "Loss: 7.243777e-02\n",
            "Loss: 7.243178e-02\n",
            "Loss: 7.242755e-02\n",
            "Loss: 7.242119e-02\n",
            "Loss: 7.241508e-02\n",
            "Loss: 7.241091e-02\n",
            "Loss: 7.240804e-02\n",
            "Loss: 7.240044e-02\n",
            "Loss: 7.239558e-02\n",
            "Loss: 7.238592e-02\n",
            "Loss: 7.238123e-02\n",
            "Loss: 7.237627e-02\n",
            "Loss: 7.237123e-02\n",
            "Loss: 7.235898e-02\n",
            "Loss: 7.235429e-02\n",
            "Loss: 7.234886e-02\n",
            "Loss: 7.234522e-02\n",
            "Loss: 7.234199e-02\n",
            "Loss: 7.233729e-02\n",
            "Loss: 7.232915e-02\n",
            "Loss: 7.232216e-02\n",
            "Loss: 7.231317e-02\n",
            "Loss: 7.230651e-02\n",
            "Loss: 7.231882e-02\n",
            "Loss: 7.230291e-02\n",
            "Loss: 7.229794e-02\n",
            "Loss: 7.229345e-02\n",
            "Loss: 7.228607e-02\n",
            "Loss: 7.227736e-02\n",
            "Loss: 7.226980e-02\n",
            "Loss: 7.225969e-02\n",
            "Loss: 7.226010e-02\n",
            "Loss: 7.225813e-02\n",
            "Loss: 7.225491e-02\n",
            "Loss: 7.225148e-02\n",
            "Loss: 7.224619e-02\n",
            "Loss: 7.223901e-02\n",
            "Loss: 7.223514e-02\n",
            "Loss: 7.223078e-02\n",
            "Loss: 7.222901e-02\n",
            "Loss: 7.222574e-02\n",
            "Loss: 7.222168e-02\n",
            "Loss: 7.222989e-02\n",
            "Loss: 7.221957e-02\n",
            "Loss: 7.221732e-02\n",
            "Loss: 7.221480e-02\n",
            "Loss: 7.221176e-02\n",
            "Loss: 7.220637e-02\n",
            "Loss: 7.222231e-02\n",
            "Loss: 7.220505e-02\n",
            "Loss: 7.220113e-02\n",
            "Loss: 7.219804e-02\n",
            "Loss: 7.219484e-02\n",
            "Loss: 7.219049e-02\n",
            "Loss: 7.218377e-02\n",
            "Loss: 7.218482e-02\n",
            "Loss: 7.217993e-02\n",
            "Loss: 7.217086e-02\n",
            "Loss: 7.216604e-02\n",
            "Loss: 7.216183e-02\n",
            "Loss: 7.215820e-02\n",
            "Loss: 7.215395e-02\n",
            "Loss: 7.215090e-02\n",
            "Loss: 7.214817e-02\n",
            "Loss: 7.214431e-02\n",
            "Loss: 7.214111e-02\n",
            "Loss: 7.213230e-02\n",
            "Loss: 7.218198e-02\n",
            "Loss: 7.213012e-02\n",
            "Loss: 7.212138e-02\n",
            "Loss: 7.211453e-02\n",
            "Loss: 7.211004e-02\n",
            "Loss: 7.210618e-02\n",
            "Loss: 7.210170e-02\n",
            "Loss: 7.209696e-02\n",
            "Loss: 7.209457e-02\n",
            "Loss: 7.208706e-02\n",
            "Loss: 7.208218e-02\n",
            "Loss: 7.207858e-02\n",
            "Loss: 7.207017e-02\n",
            "Loss: 7.206301e-02\n",
            "Loss: 7.205453e-02\n",
            "Loss: 7.204556e-02\n",
            "Loss: 7.203885e-02\n",
            "Loss: 7.203087e-02\n",
            "Loss: 7.202719e-02\n",
            "Loss: 7.202145e-02\n",
            "Loss: 7.201710e-02\n",
            "Loss: 7.201418e-02\n",
            "Loss: 7.201092e-02\n",
            "Loss: 7.200676e-02\n",
            "Loss: 7.200222e-02\n",
            "Loss: 7.199720e-02\n",
            "Loss: 7.199369e-02\n",
            "Loss: 7.198611e-02\n",
            "Loss: 7.198300e-02\n",
            "Loss: 7.197757e-02\n",
            "Loss: 7.196968e-02\n",
            "Loss: 7.197102e-02\n",
            "Loss: 7.196410e-02\n",
            "Loss: 7.195760e-02\n",
            "Loss: 7.195274e-02\n",
            "Loss: 7.194646e-02\n",
            "Loss: 7.194002e-02\n",
            "Loss: 7.193479e-02\n",
            "Loss: 7.193185e-02\n",
            "Loss: 7.192735e-02\n",
            "Loss: 7.192369e-02\n",
            "Loss: 7.191972e-02\n",
            "Loss: 7.191367e-02\n",
            "Loss: 7.190651e-02\n",
            "Loss: 7.190277e-02\n",
            "Loss: 7.190008e-02\n",
            "Loss: 7.189678e-02\n",
            "Loss: 7.189308e-02\n",
            "Loss: 7.188606e-02\n",
            "Loss: 7.189153e-02\n",
            "Loss: 7.188359e-02\n",
            "Loss: 7.187843e-02\n",
            "Loss: 7.187614e-02\n",
            "Loss: 7.187252e-02\n",
            "Loss: 7.186538e-02\n",
            "Loss: 7.187413e-02\n",
            "Loss: 7.186243e-02\n",
            "Loss: 7.185575e-02\n",
            "Loss: 7.185200e-02\n",
            "Loss: 7.184734e-02\n",
            "Loss: 7.184435e-02\n",
            "Loss: 7.184355e-02\n",
            "Loss: 7.183998e-02\n",
            "Loss: 7.183813e-02\n",
            "Loss: 7.183708e-02\n",
            "Loss: 7.183523e-02\n",
            "Loss: 7.182983e-02\n",
            "Loss: 7.182461e-02\n",
            "Loss: 7.181963e-02\n",
            "Loss: 7.181586e-02\n",
            "Loss: 7.181369e-02\n",
            "Loss: 7.180814e-02\n",
            "Loss: 7.180242e-02\n",
            "Loss: 7.179777e-02\n",
            "Loss: 7.179404e-02\n",
            "Loss: 7.179160e-02\n",
            "Loss: 7.178850e-02\n",
            "Loss: 7.178450e-02\n",
            "Loss: 7.177877e-02\n",
            "Loss: 7.177673e-02\n",
            "Loss: 7.177300e-02\n",
            "Loss: 7.178222e-02\n",
            "Loss: 7.177235e-02\n",
            "Loss: 7.177035e-02\n",
            "Loss: 7.176671e-02\n",
            "Loss: 7.175995e-02\n",
            "Loss: 7.175358e-02\n",
            "Loss: 7.175217e-02\n",
            "Loss: 7.174522e-02\n",
            "Loss: 7.174310e-02\n",
            "Loss: 7.174055e-02\n",
            "Loss: 7.173582e-02\n",
            "Loss: 7.172593e-02\n",
            "Loss: 7.179598e-02\n",
            "Loss: 7.172430e-02\n",
            "Loss: 7.171726e-02\n",
            "Loss: 7.171050e-02\n",
            "Loss: 7.170425e-02\n",
            "Loss: 7.170068e-02\n",
            "Loss: 7.169480e-02\n",
            "Loss: 7.168476e-02\n",
            "Loss: 7.167684e-02\n",
            "Loss: 7.166491e-02\n",
            "Loss: 7.167961e-02\n",
            "Loss: 7.165927e-02\n",
            "Loss: 7.164967e-02\n",
            "Loss: 7.164460e-02\n",
            "Loss: 7.166661e-02\n",
            "Loss: 7.164209e-02\n",
            "Loss: 7.163798e-02\n",
            "Loss: 7.163276e-02\n",
            "Loss: 7.162704e-02\n",
            "Loss: 7.162307e-02\n",
            "Loss: 7.162366e-02\n",
            "Loss: 7.161946e-02\n",
            "Loss: 7.161532e-02\n",
            "Loss: 7.161137e-02\n",
            "Loss: 7.160817e-02\n",
            "Loss: 7.160178e-02\n",
            "Loss: 7.163940e-02\n",
            "Loss: 7.160050e-02\n",
            "Loss: 7.159606e-02\n",
            "Loss: 7.159293e-02\n",
            "Loss: 7.158828e-02\n",
            "Loss: 7.158351e-02\n",
            "Loss: 7.157763e-02\n",
            "Loss: 7.157647e-02\n",
            "Loss: 7.157122e-02\n",
            "Loss: 7.156993e-02\n",
            "Loss: 7.156634e-02\n",
            "Loss: 7.156169e-02\n",
            "Loss: 7.157028e-02\n",
            "Loss: 7.155917e-02\n",
            "Loss: 7.155365e-02\n",
            "Loss: 7.154857e-02\n",
            "Loss: 7.154486e-02\n",
            "Loss: 7.154174e-02\n",
            "Loss: 7.153869e-02\n",
            "Loss: 7.153534e-02\n",
            "Loss: 7.152980e-02\n",
            "Loss: 7.152466e-02\n",
            "Loss: 7.151846e-02\n",
            "Loss: 7.151406e-02\n",
            "Loss: 7.150766e-02\n",
            "Loss: 7.150254e-02\n",
            "Loss: 7.149564e-02\n",
            "Loss: 7.148995e-02\n",
            "Loss: 7.148384e-02\n",
            "Loss: 7.148163e-02\n",
            "Loss: 7.147863e-02\n",
            "Loss: 7.147503e-02\n",
            "Loss: 7.147831e-02\n",
            "Loss: 7.147283e-02\n",
            "Loss: 7.146864e-02\n",
            "Loss: 7.146458e-02\n",
            "Loss: 7.145989e-02\n",
            "Loss: 7.145687e-02\n",
            "Loss: 7.145373e-02\n",
            "Loss: 7.145219e-02\n",
            "Loss: 7.144979e-02\n",
            "Loss: 7.144508e-02\n",
            "Loss: 7.144083e-02\n",
            "Loss: 7.143597e-02\n",
            "Loss: 7.143193e-02\n",
            "Loss: 7.142908e-02\n",
            "Loss: 7.142613e-02\n",
            "Loss: 7.142417e-02\n",
            "Loss: 7.141884e-02\n",
            "Loss: 7.141307e-02\n",
            "Loss: 7.141737e-02\n",
            "Loss: 7.141036e-02\n",
            "Loss: 7.140568e-02\n",
            "Loss: 7.140242e-02\n",
            "Loss: 7.139358e-02\n",
            "Loss: 7.138696e-02\n",
            "Loss: 7.140679e-02\n",
            "Loss: 7.138474e-02\n",
            "Loss: 7.138050e-02\n",
            "Loss: 7.137791e-02\n",
            "Loss: 7.137284e-02\n",
            "Loss: 7.136493e-02\n",
            "Loss: 7.135691e-02\n",
            "Loss: 7.135209e-02\n",
            "Loss: 7.134561e-02\n",
            "Loss: 7.134341e-02\n",
            "Loss: 7.133928e-02\n",
            "Loss: 7.133267e-02\n",
            "Loss: 7.132597e-02\n",
            "Loss: 7.132070e-02\n",
            "Loss: 7.131846e-02\n",
            "Loss: 7.131641e-02\n",
            "Loss: 7.131421e-02\n",
            "Loss: 7.131133e-02\n",
            "Loss: 7.130561e-02\n",
            "Loss: 7.129938e-02\n",
            "Loss: 7.131888e-02\n",
            "Loss: 7.129762e-02\n",
            "Loss: 7.129291e-02\n",
            "Loss: 7.128950e-02\n",
            "Loss: 7.128340e-02\n",
            "Loss: 7.127447e-02\n",
            "Loss: 7.126713e-02\n",
            "Loss: 7.127018e-02\n",
            "Loss: 7.126453e-02\n",
            "Loss: 7.125966e-02\n",
            "Loss: 7.125591e-02\n",
            "Loss: 7.125176e-02\n",
            "Loss: 7.124545e-02\n",
            "Loss: 7.128899e-02\n",
            "Loss: 7.124346e-02\n",
            "Loss: 7.123826e-02\n",
            "Loss: 7.123407e-02\n",
            "Loss: 7.122857e-02\n",
            "Loss: 7.122213e-02\n",
            "Loss: 7.123236e-02\n",
            "Loss: 7.121900e-02\n",
            "Loss: 7.121308e-02\n",
            "Loss: 7.120957e-02\n",
            "Loss: 7.120638e-02\n",
            "Loss: 7.120102e-02\n",
            "Loss: 7.119744e-02\n",
            "Loss: 7.119077e-02\n",
            "Loss: 7.118700e-02\n",
            "Loss: 7.118212e-02\n",
            "Loss: 7.117595e-02\n",
            "Loss: 7.116879e-02\n",
            "Loss: 7.116474e-02\n",
            "Loss: 7.116002e-02\n",
            "Loss: 7.115533e-02\n",
            "Loss: 7.114968e-02\n",
            "Loss: 7.114597e-02\n",
            "Loss: 7.113959e-02\n",
            "Loss: 7.113843e-02\n",
            "Loss: 7.113469e-02\n",
            "Loss: 7.112903e-02\n",
            "Loss: 7.111745e-02\n",
            "Loss: 7.110863e-02\n",
            "Loss: 7.110020e-02\n",
            "Loss: 7.109395e-02\n",
            "Loss: 7.109020e-02\n",
            "Loss: 7.108217e-02\n",
            "Loss: 7.107741e-02\n",
            "Loss: 7.107301e-02\n",
            "Loss: 7.106966e-02\n",
            "Loss: 7.106560e-02\n",
            "Loss: 7.105934e-02\n",
            "Loss: 7.105473e-02\n",
            "Loss: 7.104968e-02\n",
            "Loss: 7.104418e-02\n",
            "Loss: 7.104237e-02\n",
            "Loss: 7.104038e-02\n",
            "Loss: 7.103720e-02\n",
            "Loss: 7.103208e-02\n",
            "Loss: 7.103097e-02\n",
            "Loss: 7.102656e-02\n",
            "Loss: 7.102454e-02\n",
            "Loss: 7.102098e-02\n",
            "Loss: 7.101738e-02\n",
            "Loss: 7.101595e-02\n",
            "Loss: 7.101081e-02\n",
            "Loss: 7.100848e-02\n",
            "Loss: 7.100706e-02\n",
            "Loss: 7.100353e-02\n",
            "Loss: 7.100037e-02\n",
            "Loss: 7.099859e-02\n",
            "Loss: 7.099559e-02\n",
            "Loss: 7.099188e-02\n",
            "Loss: 7.098638e-02\n",
            "Loss: 7.098302e-02\n",
            "Loss: 7.097889e-02\n",
            "Loss: 7.097574e-02\n",
            "Loss: 7.097302e-02\n",
            "Loss: 7.096817e-02\n",
            "Loss: 7.096447e-02\n",
            "Loss: 7.096350e-02\n",
            "Loss: 7.095999e-02\n",
            "Loss: 7.095741e-02\n",
            "Loss: 7.095352e-02\n",
            "Loss: 7.094859e-02\n",
            "Loss: 7.095225e-02\n",
            "Loss: 7.094648e-02\n",
            "Loss: 7.094320e-02\n",
            "Loss: 7.093831e-02\n",
            "Loss: 7.093515e-02\n",
            "Loss: 7.093141e-02\n",
            "Loss: 7.092768e-02\n",
            "Loss: 7.094099e-02\n",
            "Loss: 7.092659e-02\n",
            "Loss: 7.092424e-02\n",
            "Loss: 7.092163e-02\n",
            "Loss: 7.091822e-02\n",
            "Loss: 7.091419e-02\n",
            "Loss: 7.091031e-02\n",
            "Loss: 7.090563e-02\n",
            "Loss: 7.090290e-02\n",
            "Loss: 7.090005e-02\n",
            "Loss: 7.091095e-02\n",
            "Loss: 7.089859e-02\n",
            "Loss: 7.089601e-02\n",
            "Loss: 7.089478e-02\n",
            "Loss: 7.089276e-02\n",
            "Loss: 7.089064e-02\n",
            "Loss: 7.088809e-02\n",
            "Loss: 7.088535e-02\n",
            "Loss: 7.088351e-02\n",
            "Loss: 7.088102e-02\n",
            "Loss: 7.087610e-02\n",
            "Loss: 7.086981e-02\n",
            "Loss: 7.086392e-02\n",
            "Loss: 7.085909e-02\n",
            "Loss: 7.085489e-02\n",
            "Loss: 7.085192e-02\n",
            "Loss: 7.084367e-02\n",
            "Loss: 7.085729e-02\n",
            "Loss: 7.084122e-02\n",
            "Loss: 7.083683e-02\n",
            "Loss: 7.083459e-02\n",
            "Loss: 7.083206e-02\n",
            "Loss: 7.082818e-02\n",
            "Loss: 7.082643e-02\n",
            "Loss: 7.082414e-02\n",
            "Loss: 7.082248e-02\n",
            "Loss: 7.082110e-02\n",
            "Loss: 7.081742e-02\n",
            "Loss: 7.081801e-02\n",
            "Loss: 7.081496e-02\n",
            "Loss: 7.081329e-02\n",
            "Loss: 7.081194e-02\n",
            "Loss: 7.080990e-02\n",
            "Loss: 7.080607e-02\n",
            "Loss: 7.080276e-02\n",
            "Loss: 7.079855e-02\n",
            "Loss: 7.079658e-02\n",
            "Loss: 7.079451e-02\n",
            "Loss: 7.079204e-02\n",
            "Loss: 7.078768e-02\n",
            "Loss: 7.079525e-02\n",
            "Loss: 7.078613e-02\n",
            "Loss: 7.078332e-02\n",
            "Loss: 7.077930e-02\n",
            "Loss: 7.077586e-02\n",
            "Loss: 7.077204e-02\n",
            "Loss: 7.076855e-02\n",
            "Loss: 7.076334e-02\n",
            "Loss: 7.075828e-02\n",
            "Loss: 7.075199e-02\n",
            "Loss: 7.074767e-02\n",
            "Loss: 7.073888e-02\n",
            "Loss: 7.073207e-02\n",
            "Loss: 7.072545e-02\n",
            "Loss: 7.072148e-02\n",
            "Loss: 7.071923e-02\n",
            "Loss: 7.071427e-02\n",
            "Loss: 7.070658e-02\n",
            "Loss: 7.070371e-02\n",
            "Loss: 7.070020e-02\n",
            "Loss: 7.069898e-02\n",
            "Loss: 7.069604e-02\n",
            "Loss: 7.069308e-02\n",
            "Loss: 7.068959e-02\n",
            "Loss: 7.068644e-02\n",
            "Loss: 7.068168e-02\n",
            "Loss: 7.067811e-02\n",
            "Loss: 7.067698e-02\n",
            "Loss: 7.067428e-02\n",
            "Loss: 7.066838e-02\n",
            "Loss: 7.066562e-02\n",
            "Loss: 7.066020e-02\n",
            "Loss: 7.065649e-02\n",
            "Loss: 7.066837e-02\n",
            "Loss: 7.065503e-02\n",
            "Loss: 7.065206e-02\n",
            "Loss: 7.064945e-02\n",
            "Loss: 7.064550e-02\n",
            "Loss: 7.064082e-02\n",
            "Loss: 7.063967e-02\n",
            "Loss: 7.063698e-02\n",
            "Loss: 7.063496e-02\n",
            "Loss: 7.063343e-02\n",
            "Loss: 7.063111e-02\n",
            "Loss: 7.063397e-02\n",
            "Loss: 7.062931e-02\n",
            "Loss: 7.062495e-02\n",
            "Loss: 7.062119e-02\n",
            "Loss: 7.061794e-02\n",
            "Loss: 7.061473e-02\n",
            "Loss: 7.061000e-02\n",
            "Loss: 7.060470e-02\n",
            "Loss: 7.059891e-02\n",
            "Loss: 7.059487e-02\n",
            "Loss: 7.059094e-02\n",
            "Loss: 7.058249e-02\n",
            "Loss: 7.057752e-02\n",
            "Loss: 7.057136e-02\n",
            "Loss: 7.056991e-02\n",
            "Loss: 7.056094e-02\n",
            "Loss: 7.055587e-02\n",
            "Loss: 7.054923e-02\n",
            "Loss: 7.055033e-02\n",
            "Loss: 7.054704e-02\n",
            "Loss: 7.054307e-02\n",
            "Loss: 7.053894e-02\n",
            "Loss: 7.053626e-02\n",
            "Loss: 7.053295e-02\n",
            "Loss: 7.052979e-02\n",
            "Loss: 7.052805e-02\n",
            "Loss: 7.052466e-02\n",
            "Loss: 7.052232e-02\n",
            "Loss: 7.052050e-02\n",
            "Loss: 7.051786e-02\n",
            "Loss: 7.052120e-02\n",
            "Loss: 7.051601e-02\n",
            "Loss: 7.051300e-02\n",
            "Loss: 7.050643e-02\n",
            "Loss: 7.050101e-02\n",
            "Loss: 7.050357e-02\n",
            "Loss: 7.049825e-02\n",
            "Loss: 7.049356e-02\n",
            "Loss: 7.049055e-02\n",
            "Loss: 7.048610e-02\n",
            "Loss: 7.048103e-02\n",
            "Loss: 7.047464e-02\n",
            "Loss: 7.046895e-02\n",
            "Loss: 7.046515e-02\n",
            "Loss: 7.046219e-02\n",
            "Loss: 7.045878e-02\n",
            "Loss: 7.045490e-02\n",
            "Loss: 7.045504e-02\n",
            "Loss: 7.045329e-02\n",
            "Loss: 7.045098e-02\n",
            "Loss: 7.044725e-02\n",
            "Loss: 7.044341e-02\n",
            "Loss: 7.043828e-02\n",
            "Loss: 7.044426e-02\n",
            "Loss: 7.043595e-02\n",
            "Loss: 7.043188e-02\n",
            "Loss: 7.042735e-02\n",
            "Loss: 7.042197e-02\n",
            "Loss: 7.041693e-02\n",
            "Loss: 7.041159e-02\n",
            "Loss: 7.040734e-02\n",
            "Loss: 7.040215e-02\n",
            "Loss: 7.039646e-02\n",
            "Loss: 7.038801e-02\n",
            "Loss: 7.038409e-02\n",
            "Loss: 7.037850e-02\n",
            "Loss: 7.037388e-02\n",
            "Loss: 7.036957e-02\n",
            "Loss: 7.036169e-02\n",
            "Loss: 7.035622e-02\n",
            "Loss: 7.034927e-02\n",
            "Loss: 7.034498e-02\n",
            "Loss: 7.034227e-02\n",
            "Loss: 7.033840e-02\n",
            "Loss: 7.032947e-02\n",
            "Loss: 7.034229e-02\n",
            "Loss: 7.032676e-02\n",
            "Loss: 7.032351e-02\n",
            "Loss: 7.032134e-02\n",
            "Loss: 7.031682e-02\n",
            "Loss: 7.031280e-02\n",
            "Loss: 7.030738e-02\n",
            "Loss: 7.029999e-02\n",
            "Loss: 7.029883e-02\n",
            "Loss: 7.029355e-02\n",
            "Loss: 7.028991e-02\n",
            "Loss: 7.028598e-02\n",
            "Loss: 7.028349e-02\n",
            "Loss: 7.027628e-02\n",
            "Loss: 7.027239e-02\n",
            "Loss: 7.026727e-02\n",
            "Loss: 7.026508e-02\n",
            "Loss: 7.026188e-02\n",
            "Loss: 7.025716e-02\n",
            "Loss: 7.024534e-02\n",
            "Loss: 7.024135e-02\n",
            "Loss: 7.023715e-02\n",
            "Loss: 7.023280e-02\n",
            "Loss: 7.022860e-02\n",
            "Loss: 7.022332e-02\n",
            "Loss: 7.022114e-02\n",
            "Loss: 7.021824e-02\n",
            "Loss: 7.021330e-02\n",
            "Loss: 7.021220e-02\n",
            "Loss: 7.020803e-02\n",
            "Loss: 7.020584e-02\n",
            "Loss: 7.020386e-02\n",
            "Loss: 7.020041e-02\n",
            "Loss: 7.019741e-02\n",
            "Loss: 7.019267e-02\n",
            "Loss: 7.019085e-02\n",
            "Loss: 7.018839e-02\n",
            "Loss: 7.018651e-02\n",
            "Loss: 7.018320e-02\n",
            "Loss: 7.018062e-02\n",
            "Loss: 7.017761e-02\n",
            "Loss: 7.017355e-02\n",
            "Loss: 7.017181e-02\n",
            "Loss: 7.016461e-02\n",
            "Loss: 7.016236e-02\n",
            "Loss: 7.015900e-02\n",
            "Loss: 7.015587e-02\n",
            "Loss: 7.015382e-02\n",
            "Loss: 7.015255e-02\n",
            "Loss: 7.014941e-02\n",
            "Loss: 7.014438e-02\n",
            "Loss: 7.013886e-02\n",
            "Loss: 7.013146e-02\n",
            "Loss: 7.012378e-02\n",
            "Loss: 7.011862e-02\n",
            "Loss: 7.011554e-02\n",
            "Loss: 7.011245e-02\n",
            "Loss: 7.010942e-02\n",
            "Loss: 7.010379e-02\n",
            "Loss: 7.010093e-02\n",
            "Loss: 7.009616e-02\n",
            "Loss: 7.009114e-02\n",
            "Loss: 7.008900e-02\n",
            "Loss: 7.008482e-02\n",
            "Loss: 7.008289e-02\n",
            "Loss: 7.008037e-02\n",
            "Loss: 7.007623e-02\n",
            "Loss: 7.007159e-02\n",
            "Loss: 7.006890e-02\n",
            "Loss: 7.006548e-02\n",
            "Loss: 7.006360e-02\n",
            "Loss: 7.006014e-02\n",
            "Loss: 7.005782e-02\n",
            "Loss: 7.005566e-02\n",
            "Loss: 7.005388e-02\n",
            "Loss: 7.005065e-02\n",
            "Loss: 7.004632e-02\n",
            "Loss: 7.004368e-02\n",
            "Loss: 7.004138e-02\n",
            "Loss: 7.003982e-02\n",
            "Loss: 7.003821e-02\n",
            "Loss: 7.003615e-02\n",
            "Loss: 7.003504e-02\n",
            "Loss: 7.003142e-02\n",
            "Loss: 7.002964e-02\n",
            "Loss: 7.002623e-02\n",
            "Loss: 7.002620e-02\n",
            "Loss: 7.002483e-02\n",
            "Loss: 7.002252e-02\n",
            "Loss: 7.002020e-02\n",
            "Loss: 7.001825e-02\n",
            "Loss: 7.001650e-02\n",
            "Loss: 7.001553e-02\n",
            "Loss: 7.001393e-02\n",
            "Loss: 7.001144e-02\n",
            "Loss: 7.000851e-02\n",
            "Loss: 7.000604e-02\n",
            "Loss: 7.000481e-02\n",
            "Loss: 7.000310e-02\n",
            "Loss: 7.000160e-02\n",
            "Loss: 6.999931e-02\n",
            "Loss: 6.999530e-02\n",
            "Loss: 6.999073e-02\n",
            "Loss: 6.998709e-02\n",
            "Loss: 6.998403e-02\n",
            "Loss: 6.998141e-02\n",
            "Loss: 6.998002e-02\n",
            "Loss: 6.997844e-02\n",
            "Loss: 6.997542e-02\n",
            "Loss: 6.997202e-02\n",
            "Loss: 6.996985e-02\n",
            "Loss: 6.996794e-02\n",
            "Loss: 6.996316e-02\n",
            "Loss: 6.995823e-02\n",
            "Loss: 6.995461e-02\n",
            "Loss: 6.995220e-02\n",
            "Loss: 6.994867e-02\n",
            "Loss: 6.994465e-02\n",
            "Loss: 6.993866e-02\n",
            "Loss: 6.993395e-02\n",
            "Loss: 6.992979e-02\n",
            "Loss: 6.992550e-02\n",
            "Loss: 6.991955e-02\n",
            "Loss: 6.991436e-02\n",
            "Loss: 6.991153e-02\n",
            "Loss: 6.990755e-02\n",
            "Loss: 6.990544e-02\n",
            "Loss: 6.990136e-02\n",
            "Loss: 6.989702e-02\n",
            "Loss: 6.989181e-02\n",
            "Loss: 6.988661e-02\n",
            "Loss: 6.988249e-02\n",
            "Loss: 6.987760e-02\n",
            "Loss: 6.987116e-02\n",
            "Loss: 6.986517e-02\n",
            "Loss: 6.987341e-02\n",
            "Loss: 6.986234e-02\n",
            "Loss: 6.985748e-02\n",
            "Loss: 6.985471e-02\n",
            "Loss: 6.984979e-02\n",
            "Loss: 6.984381e-02\n",
            "Loss: 6.983654e-02\n",
            "Loss: 6.983642e-02\n",
            "Loss: 6.983432e-02\n",
            "Loss: 6.983147e-02\n",
            "Loss: 6.982809e-02\n",
            "Loss: 6.982543e-02\n",
            "Loss: 6.981947e-02\n",
            "Loss: 6.981633e-02\n",
            "Loss: 6.981128e-02\n",
            "Loss: 6.980924e-02\n",
            "Loss: 6.980684e-02\n",
            "Loss: 6.980328e-02\n",
            "Loss: 6.979854e-02\n",
            "Loss: 6.979580e-02\n",
            "Loss: 6.979311e-02\n",
            "Loss: 6.978875e-02\n",
            "Loss: 6.977765e-02\n",
            "Loss: 6.977410e-02\n",
            "Loss: 6.977637e-02\n",
            "Loss: 6.977019e-02\n",
            "Loss: 6.976661e-02\n",
            "Loss: 6.976373e-02\n",
            "Loss: 6.976119e-02\n",
            "Loss: 6.976375e-02\n",
            "Loss: 6.976004e-02\n",
            "Loss: 6.975730e-02\n",
            "Loss: 6.974929e-02\n",
            "Loss: 6.974398e-02\n",
            "Loss: 6.973939e-02\n",
            "Loss: 6.973697e-02\n",
            "Loss: 6.973054e-02\n",
            "Loss: 6.972504e-02\n",
            "Loss: 6.972051e-02\n",
            "Loss: 6.971510e-02\n",
            "Loss: 6.971049e-02\n",
            "Loss: 6.970678e-02\n",
            "Loss: 6.970131e-02\n",
            "Loss: 6.969868e-02\n",
            "Loss: 6.969343e-02\n",
            "Loss: 6.969205e-02\n",
            "Loss: 6.968813e-02\n",
            "Loss: 6.968285e-02\n",
            "Loss: 6.967846e-02\n",
            "Loss: 6.967695e-02\n",
            "Loss: 6.967464e-02\n",
            "Loss: 6.967244e-02\n",
            "Loss: 6.966753e-02\n",
            "Loss: 6.966304e-02\n",
            "Loss: 6.966037e-02\n",
            "Loss: 6.965840e-02\n",
            "Loss: 6.965669e-02\n",
            "Loss: 6.965433e-02\n",
            "Loss: 6.965069e-02\n",
            "Loss: 6.965061e-02\n",
            "Loss: 6.964941e-02\n",
            "Loss: 6.964715e-02\n",
            "Loss: 6.964561e-02\n",
            "Loss: 6.964338e-02\n",
            "Loss: 6.964279e-02\n",
            "Loss: 6.964009e-02\n",
            "Loss: 6.963894e-02\n",
            "Loss: 6.963764e-02\n",
            "Loss: 6.963667e-02\n",
            "Loss: 6.963449e-02\n",
            "Loss: 6.963225e-02\n",
            "Loss: 6.963140e-02\n",
            "Loss: 6.962965e-02\n",
            "Loss: 6.962829e-02\n",
            "Loss: 6.962647e-02\n",
            "Loss: 6.962507e-02\n",
            "Loss: 6.962379e-02\n",
            "Loss: 6.962584e-02\n",
            "Loss: 6.962270e-02\n",
            "Loss: 6.962073e-02\n",
            "Loss: 6.961900e-02\n",
            "Loss: 6.961767e-02\n",
            "Loss: 6.961578e-02\n",
            "Loss: 6.961433e-02\n",
            "Loss: 6.963892e-02\n",
            "Loss: 6.961288e-02\n",
            "Loss: 6.960931e-02\n",
            "Loss: 6.960662e-02\n",
            "Loss: 6.960404e-02\n",
            "Loss: 6.960577e-02\n",
            "Loss: 6.960302e-02\n",
            "Loss: 6.960191e-02\n",
            "Loss: 6.959804e-02\n",
            "Loss: 6.959563e-02\n",
            "Loss: 6.959328e-02\n",
            "Loss: 6.959088e-02\n",
            "Loss: 6.958911e-02\n",
            "Loss: 6.958684e-02\n",
            "Loss: 6.958397e-02\n",
            "Loss: 6.958172e-02\n",
            "Loss: 6.957917e-02\n",
            "Loss: 6.957696e-02\n",
            "Loss: 6.957612e-02\n",
            "Loss: 6.957183e-02\n",
            "Loss: 6.957044e-02\n",
            "Loss: 6.956878e-02\n",
            "Loss: 6.956729e-02\n",
            "Loss: 6.956523e-02\n",
            "Loss: 6.956269e-02\n",
            "Loss: 6.956106e-02\n",
            "Loss: 6.956020e-02\n",
            "Loss: 6.955607e-02\n",
            "Loss: 6.955423e-02\n",
            "Loss: 6.955181e-02\n",
            "Loss: 6.954918e-02\n",
            "Loss: 6.954531e-02\n",
            "Loss: 6.954183e-02\n",
            "Loss: 6.954043e-02\n",
            "Loss: 6.953952e-02\n",
            "Loss: 6.953808e-02\n",
            "Loss: 6.953607e-02\n",
            "Loss: 6.953777e-02\n",
            "Loss: 6.953453e-02\n",
            "Loss: 6.953371e-02\n",
            "Loss: 6.953280e-02\n",
            "Loss: 6.953248e-02\n",
            "Loss: 6.952962e-02\n",
            "Loss: 6.952758e-02\n",
            "Loss: 6.952395e-02\n",
            "Loss: 6.952146e-02\n",
            "Loss: 6.952028e-02\n",
            "Loss: 6.951824e-02\n",
            "Loss: 6.951905e-02\n",
            "Loss: 6.951667e-02\n",
            "Loss: 6.951397e-02\n",
            "Loss: 6.951207e-02\n",
            "Loss: 6.950907e-02\n",
            "Loss: 6.951173e-02\n",
            "Loss: 6.950758e-02\n",
            "Loss: 6.950561e-02\n",
            "Loss: 6.950295e-02\n",
            "Loss: 6.950155e-02\n",
            "Loss: 6.950001e-02\n",
            "Loss: 6.949878e-02\n",
            "Loss: 6.949715e-02\n",
            "Loss: 6.949437e-02\n",
            "Loss: 6.949342e-02\n",
            "Loss: 6.949074e-02\n",
            "Loss: 6.948932e-02\n",
            "Loss: 6.948653e-02\n",
            "Loss: 6.948327e-02\n",
            "Loss: 6.948091e-02\n",
            "Loss: 6.947701e-02\n",
            "Loss: 6.947565e-02\n",
            "Loss: 6.947359e-02\n",
            "Loss: 6.947231e-02\n",
            "Loss: 6.946952e-02\n",
            "Loss: 6.946739e-02\n",
            "Loss: 6.946588e-02\n",
            "Loss: 6.946430e-02\n",
            "Loss: 6.946088e-02\n",
            "Loss: 6.946249e-02\n",
            "Loss: 6.945815e-02\n",
            "Loss: 6.945598e-02\n",
            "Loss: 6.945390e-02\n",
            "Loss: 6.945147e-02\n",
            "Loss: 6.944656e-02\n",
            "Loss: 6.945124e-02\n",
            "Loss: 6.944456e-02\n",
            "Loss: 6.944159e-02\n",
            "Loss: 6.943730e-02\n",
            "Loss: 6.943491e-02\n",
            "Loss: 6.943275e-02\n",
            "Loss: 6.943065e-02\n",
            "Loss: 6.942883e-02\n",
            "Loss: 6.942780e-02\n",
            "Loss: 6.942523e-02\n",
            "Loss: 6.942233e-02\n",
            "Loss: 6.941875e-02\n",
            "Loss: 6.941871e-02\n",
            "Loss: 6.941734e-02\n",
            "Loss: 6.941423e-02\n",
            "Loss: 6.941256e-02\n",
            "Loss: 6.941067e-02\n",
            "Loss: 6.940823e-02\n",
            "Loss: 6.940504e-02\n",
            "Loss: 6.940316e-02\n",
            "Loss: 6.940133e-02\n",
            "Loss: 6.939825e-02\n",
            "Loss: 6.939949e-02\n",
            "Loss: 6.939718e-02\n",
            "Loss: 6.939483e-02\n",
            "Loss: 6.939149e-02\n",
            "Loss: 6.938884e-02\n",
            "Loss: 6.938545e-02\n",
            "Loss: 6.939837e-02\n",
            "Loss: 6.938384e-02\n",
            "Loss: 6.938092e-02\n",
            "Loss: 6.937788e-02\n",
            "Loss: 6.937645e-02\n",
            "Loss: 6.937389e-02\n",
            "Loss: 6.937149e-02\n",
            "Loss: 6.936859e-02\n",
            "Loss: 6.936523e-02\n",
            "Loss: 6.936362e-02\n",
            "Loss: 6.935743e-02\n",
            "Loss: 6.935483e-02\n",
            "Loss: 6.935102e-02\n",
            "Loss: 6.934681e-02\n",
            "Loss: 6.934510e-02\n",
            "Loss: 6.934041e-02\n",
            "Loss: 6.933966e-02\n",
            "Loss: 6.933758e-02\n",
            "Loss: 6.933494e-02\n",
            "Loss: 6.933338e-02\n",
            "Loss: 6.932948e-02\n",
            "Loss: 6.932739e-02\n",
            "Loss: 6.932472e-02\n",
            "Loss: 6.932114e-02\n",
            "Loss: 6.934881e-02\n",
            "Loss: 6.931962e-02\n",
            "Loss: 6.931701e-02\n",
            "Loss: 6.931512e-02\n",
            "Loss: 6.931286e-02\n",
            "Loss: 6.930985e-02\n",
            "Loss: 6.930459e-02\n",
            "Loss: 6.931511e-02\n",
            "Loss: 6.930290e-02\n",
            "Loss: 6.930028e-02\n",
            "Loss: 6.929971e-02\n",
            "Loss: 6.929700e-02\n",
            "Loss: 6.929541e-02\n",
            "Loss: 6.929039e-02\n",
            "Loss: 6.928738e-02\n",
            "Loss: 6.928424e-02\n",
            "Loss: 6.928132e-02\n",
            "Loss: 6.928349e-02\n",
            "Loss: 6.928009e-02\n",
            "Loss: 6.927732e-02\n",
            "Loss: 6.927528e-02\n",
            "Loss: 6.927225e-02\n",
            "Loss: 6.926988e-02\n",
            "Loss: 6.927150e-02\n",
            "Loss: 6.926888e-02\n",
            "Loss: 6.926608e-02\n",
            "Loss: 6.926350e-02\n",
            "Loss: 6.926125e-02\n",
            "Loss: 6.925927e-02\n",
            "Loss: 6.926866e-02\n",
            "Loss: 6.925884e-02\n",
            "Loss: 6.925740e-02\n",
            "Loss: 6.925607e-02\n",
            "Loss: 6.925519e-02\n",
            "Loss: 6.925391e-02\n",
            "Loss: 6.925140e-02\n",
            "Loss: 6.924832e-02\n",
            "Loss: 6.924722e-02\n",
            "Loss: 6.924382e-02\n",
            "Loss: 6.924179e-02\n",
            "Loss: 6.924000e-02\n",
            "Loss: 6.923871e-02\n",
            "Loss: 6.923861e-02\n",
            "Loss: 6.923708e-02\n",
            "Loss: 6.923655e-02\n",
            "Loss: 6.923474e-02\n",
            "Loss: 6.923261e-02\n",
            "Loss: 6.922983e-02\n",
            "Loss: 6.923100e-02\n",
            "Loss: 6.922901e-02\n",
            "Loss: 6.922709e-02\n",
            "Loss: 6.922458e-02\n",
            "Loss: 6.922205e-02\n",
            "Loss: 6.921930e-02\n",
            "Loss: 6.921713e-02\n",
            "Loss: 6.921548e-02\n",
            "Loss: 6.921460e-02\n",
            "Loss: 6.921253e-02\n",
            "Loss: 6.920937e-02\n",
            "Loss: 6.920541e-02\n",
            "Loss: 6.920982e-02\n",
            "Loss: 6.920317e-02\n",
            "Loss: 6.920046e-02\n",
            "Loss: 6.919866e-02\n",
            "Loss: 6.919682e-02\n",
            "Loss: 6.919429e-02\n",
            "Loss: 6.919035e-02\n",
            "Loss: 6.918621e-02\n",
            "Loss: 6.918360e-02\n",
            "Loss: 6.918199e-02\n",
            "Loss: 6.917995e-02\n",
            "Loss: 6.917779e-02\n",
            "Loss: 6.917549e-02\n",
            "Loss: 6.917463e-02\n",
            "Loss: 6.917158e-02\n",
            "Loss: 6.916989e-02\n",
            "Loss: 6.916778e-02\n",
            "Loss: 6.916495e-02\n",
            "Loss: 6.916253e-02\n",
            "Loss: 6.916042e-02\n",
            "Loss: 6.915874e-02\n",
            "Loss: 6.915697e-02\n",
            "Loss: 6.915333e-02\n",
            "Loss: 6.915183e-02\n",
            "Loss: 6.914701e-02\n",
            "Loss: 6.915091e-02\n",
            "Loss: 6.914535e-02\n",
            "Loss: 6.914356e-02\n",
            "Loss: 6.914227e-02\n",
            "Loss: 6.913978e-02\n",
            "Loss: 6.913763e-02\n",
            "Loss: 6.913456e-02\n",
            "Loss: 6.914233e-02\n",
            "Loss: 6.913237e-02\n",
            "Loss: 6.912903e-02\n",
            "Loss: 6.912588e-02\n",
            "Loss: 6.912304e-02\n",
            "Loss: 6.911983e-02\n",
            "Loss: 6.911545e-02\n",
            "Loss: 6.912077e-02\n",
            "Loss: 6.911400e-02\n",
            "Loss: 6.911155e-02\n",
            "Loss: 6.910983e-02\n",
            "Loss: 6.910841e-02\n",
            "Loss: 6.910647e-02\n",
            "Loss: 6.910436e-02\n",
            "Loss: 6.910250e-02\n",
            "Loss: 6.910066e-02\n",
            "Loss: 6.909896e-02\n",
            "Loss: 6.909691e-02\n",
            "Loss: 6.909537e-02\n",
            "Loss: 6.909329e-02\n",
            "Loss: 6.909159e-02\n",
            "Loss: 6.908848e-02\n",
            "Loss: 6.909274e-02\n",
            "Loss: 6.908669e-02\n",
            "Loss: 6.908475e-02\n",
            "Loss: 6.908350e-02\n",
            "Loss: 6.908181e-02\n",
            "Loss: 6.907861e-02\n",
            "Loss: 6.907561e-02\n",
            "Loss: 6.907364e-02\n",
            "Loss: 6.907059e-02\n",
            "Loss: 6.906818e-02\n",
            "Loss: 6.906305e-02\n",
            "Loss: 6.907253e-02\n",
            "Loss: 6.906100e-02\n",
            "Loss: 6.905878e-02\n",
            "Loss: 6.905600e-02\n",
            "Loss: 6.905343e-02\n",
            "Loss: 6.905151e-02\n",
            "Loss: 6.904997e-02\n",
            "Loss: 6.904858e-02\n",
            "Loss: 6.904814e-02\n",
            "Loss: 6.904553e-02\n",
            "Loss: 6.904414e-02\n",
            "Loss: 6.904192e-02\n",
            "Loss: 6.903890e-02\n",
            "Loss: 6.904636e-02\n",
            "Loss: 6.903666e-02\n",
            "Loss: 6.903333e-02\n",
            "Loss: 6.903171e-02\n",
            "Loss: 6.902976e-02\n",
            "Loss: 6.902522e-02\n",
            "Loss: 6.906359e-02\n",
            "Loss: 6.902403e-02\n",
            "Loss: 6.902066e-02\n",
            "Loss: 6.901862e-02\n",
            "Loss: 6.901561e-02\n",
            "Loss: 6.901202e-02\n",
            "Loss: 6.900603e-02\n",
            "Loss: 6.902888e-02\n",
            "Loss: 6.900331e-02\n",
            "Loss: 6.899895e-02\n",
            "Loss: 6.899574e-02\n",
            "Loss: 6.899425e-02\n",
            "Loss: 6.899323e-02\n",
            "Loss: 6.898905e-02\n",
            "Loss: 6.898934e-02\n",
            "Loss: 6.898802e-02\n",
            "Loss: 6.898613e-02\n",
            "Loss: 6.898403e-02\n",
            "Loss: 6.898122e-02\n",
            "Loss: 6.898154e-02\n",
            "Loss: 6.897964e-02\n",
            "Loss: 6.897758e-02\n",
            "Loss: 6.897597e-02\n",
            "Loss: 6.897414e-02\n",
            "Loss: 6.897327e-02\n",
            "Loss: 6.897109e-02\n",
            "Loss: 6.896944e-02\n",
            "Loss: 6.896754e-02\n",
            "Loss: 6.896506e-02\n",
            "Loss: 6.896397e-02\n",
            "Loss: 6.896178e-02\n",
            "Loss: 6.896122e-02\n",
            "Loss: 6.895851e-02\n",
            "Loss: 6.895645e-02\n",
            "Loss: 6.895343e-02\n",
            "Loss: 6.895397e-02\n",
            "Loss: 6.895159e-02\n",
            "Loss: 6.894998e-02\n",
            "Loss: 6.894858e-02\n",
            "Loss: 6.894739e-02\n",
            "Loss: 6.894671e-02\n",
            "Loss: 6.894579e-02\n",
            "Loss: 6.894515e-02\n",
            "Loss: 6.894469e-02\n",
            "Loss: 6.894320e-02\n",
            "Loss: 6.894430e-02\n",
            "Loss: 6.894222e-02\n",
            "Loss: 6.894073e-02\n",
            "Loss: 6.894016e-02\n",
            "Loss: 6.893989e-02\n",
            "Loss: 6.893953e-02\n",
            "Loss: 6.893855e-02\n",
            "Loss: 6.893811e-02\n",
            "Loss: 6.893679e-02\n",
            "Loss: 6.893717e-02\n",
            "Loss: 6.893591e-02\n",
            "Loss: 6.893469e-02\n",
            "Loss: 6.893385e-02\n",
            "Loss: 6.893202e-02\n",
            "Loss: 6.893136e-02\n",
            "Loss: 6.893698e-02\n",
            "Loss: 6.893105e-02\n",
            "Loss: 6.892984e-02\n",
            "Loss: 6.892893e-02\n",
            "Loss: 6.892740e-02\n",
            "Loss: 6.892547e-02\n",
            "Loss: 6.892221e-02\n",
            "Loss: 6.892094e-02\n",
            "Loss: 6.891838e-02\n",
            "Loss: 6.891721e-02\n",
            "Loss: 6.891587e-02\n",
            "Loss: 6.891346e-02\n",
            "Loss: 6.891641e-02\n",
            "Loss: 6.891260e-02\n",
            "Loss: 6.891055e-02\n",
            "Loss: 6.891008e-02\n",
            "Loss: 6.890862e-02\n",
            "Loss: 6.890731e-02\n",
            "Loss: 6.890514e-02\n",
            "Loss: 6.890446e-02\n",
            "Loss: 6.890213e-02\n",
            "Loss: 6.890120e-02\n",
            "Loss: 6.889988e-02\n",
            "Loss: 6.889801e-02\n",
            "Loss: 6.889650e-02\n",
            "Loss: 6.889438e-02\n",
            "Loss: 6.889350e-02\n",
            "Loss: 6.889133e-02\n",
            "Loss: 6.888910e-02\n",
            "Loss: 6.888743e-02\n",
            "Loss: 6.888526e-02\n",
            "Loss: 6.888372e-02\n",
            "Loss: 6.888275e-02\n",
            "Loss: 6.888157e-02\n",
            "Loss: 6.888031e-02\n",
            "Loss: 6.887977e-02\n",
            "Loss: 6.887805e-02\n",
            "Loss: 6.887729e-02\n",
            "Loss: 6.887589e-02\n",
            "Loss: 6.887498e-02\n",
            "Loss: 6.887250e-02\n",
            "Loss: 6.887068e-02\n",
            "Loss: 6.887151e-02\n",
            "Loss: 6.886917e-02\n",
            "Loss: 6.886789e-02\n",
            "Loss: 6.886724e-02\n",
            "Loss: 6.886663e-02\n",
            "Loss: 6.886592e-02\n",
            "Loss: 6.886464e-02\n",
            "Loss: 6.886416e-02\n",
            "Loss: 6.886333e-02\n",
            "Loss: 6.886210e-02\n",
            "Loss: 6.886125e-02\n",
            "Loss: 6.885968e-02\n",
            "Loss: 6.885886e-02\n",
            "Loss: 6.885848e-02\n",
            "Loss: 6.885791e-02\n",
            "Loss: 6.885604e-02\n",
            "Loss: 6.885558e-02\n",
            "Loss: 6.885427e-02\n",
            "Loss: 6.885311e-02\n",
            "Loss: 6.885174e-02\n",
            "Loss: 6.885104e-02\n",
            "Loss: 6.885036e-02\n",
            "Loss: 6.884986e-02\n",
            "Loss: 6.884870e-02\n",
            "Loss: 6.884753e-02\n",
            "Loss: 6.884677e-02\n",
            "Loss: 6.884621e-02\n",
            "Loss: 6.884544e-02\n",
            "Loss: 6.884516e-02\n",
            "Loss: 6.884400e-02\n",
            "Loss: 6.884283e-02\n",
            "Loss: 6.884150e-02\n",
            "Loss: 6.884065e-02\n",
            "Loss: 6.884003e-02\n",
            "Loss: 6.883892e-02\n",
            "Loss: 6.883684e-02\n",
            "Loss: 6.883607e-02\n",
            "Loss: 6.883436e-02\n",
            "Loss: 6.883337e-02\n",
            "Loss: 6.883076e-02\n",
            "Loss: 6.882960e-02\n",
            "Loss: 6.882814e-02\n",
            "Loss: 6.882661e-02\n",
            "Loss: 6.882437e-02\n",
            "Loss: 6.882682e-02\n",
            "Loss: 6.882317e-02\n",
            "Loss: 6.882133e-02\n",
            "Loss: 6.882046e-02\n",
            "Loss: 6.881870e-02\n",
            "Loss: 6.881639e-02\n",
            "Loss: 6.881523e-02\n",
            "Loss: 6.881236e-02\n",
            "Loss: 6.881155e-02\n",
            "Loss: 6.880937e-02\n",
            "Loss: 6.880806e-02\n",
            "Loss: 6.880625e-02\n",
            "Loss: 6.880479e-02\n",
            "Loss: 6.880349e-02\n",
            "Loss: 6.880225e-02\n",
            "Loss: 6.880083e-02\n",
            "Loss: 6.879931e-02\n",
            "Loss: 6.879724e-02\n",
            "Loss: 6.879438e-02\n",
            "Loss: 6.879398e-02\n",
            "Loss: 6.879099e-02\n",
            "Loss: 6.878983e-02\n",
            "Loss: 6.878848e-02\n",
            "Loss: 6.879012e-02\n",
            "Loss: 6.878742e-02\n",
            "Loss: 6.878580e-02\n",
            "Loss: 6.878391e-02\n",
            "Loss: 6.878223e-02\n",
            "Loss: 6.877892e-02\n",
            "Loss: 6.877531e-02\n",
            "Loss: 6.877749e-02\n",
            "Loss: 6.877386e-02\n",
            "Loss: 6.877266e-02\n",
            "Loss: 6.877139e-02\n",
            "Loss: 6.877049e-02\n",
            "Loss: 6.876841e-02\n",
            "Loss: 6.876735e-02\n",
            "Loss: 6.876470e-02\n",
            "Loss: 6.876335e-02\n",
            "Loss: 6.876153e-02\n",
            "Loss: 6.876019e-02\n",
            "Loss: 6.875795e-02\n",
            "Loss: 6.875661e-02\n",
            "Loss: 6.875506e-02\n",
            "Loss: 6.875435e-02\n",
            "Loss: 6.875309e-02\n",
            "Loss: 6.875155e-02\n",
            "Loss: 6.875290e-02\n",
            "Loss: 6.875093e-02\n",
            "Loss: 6.874985e-02\n",
            "Loss: 6.874938e-02\n",
            "Loss: 6.874783e-02\n",
            "Loss: 6.874635e-02\n",
            "Loss: 6.875435e-02\n",
            "Loss: 6.874586e-02\n",
            "Loss: 6.874408e-02\n",
            "Loss: 6.874206e-02\n",
            "Loss: 6.874005e-02\n",
            "Loss: 6.873892e-02\n",
            "Loss: 6.873720e-02\n",
            "Loss: 6.873637e-02\n",
            "Loss: 6.873445e-02\n",
            "Loss: 6.873297e-02\n",
            "Loss: 6.873189e-02\n",
            "Loss: 6.872975e-02\n",
            "Loss: 6.872907e-02\n",
            "Loss: 6.872716e-02\n",
            "Loss: 6.872512e-02\n",
            "Loss: 6.872280e-02\n",
            "Loss: 6.872474e-02\n",
            "Loss: 6.872191e-02\n",
            "Loss: 6.872027e-02\n",
            "Loss: 6.871808e-02\n",
            "Loss: 6.871686e-02\n",
            "Loss: 6.871479e-02\n",
            "Loss: 6.871597e-02\n",
            "Loss: 6.871370e-02\n",
            "Loss: 6.871206e-02\n",
            "Loss: 6.871048e-02\n",
            "Loss: 6.871113e-02\n",
            "Loss: 6.870955e-02\n",
            "Loss: 6.870785e-02\n",
            "Loss: 6.870677e-02\n",
            "Loss: 6.870542e-02\n",
            "Loss: 6.870493e-02\n",
            "Loss: 6.870332e-02\n",
            "Loss: 6.870094e-02\n",
            "Loss: 6.870466e-02\n",
            "Loss: 6.870048e-02\n",
            "Loss: 6.869909e-02\n",
            "Loss: 6.869820e-02\n",
            "Loss: 6.869689e-02\n",
            "Loss: 6.869499e-02\n",
            "Loss: 6.869935e-02\n",
            "Loss: 6.869422e-02\n",
            "Loss: 6.869258e-02\n",
            "Loss: 6.869056e-02\n",
            "Loss: 6.868893e-02\n",
            "Loss: 6.868726e-02\n",
            "Loss: 6.868550e-02\n",
            "Loss: 6.868348e-02\n",
            "Loss: 6.868199e-02\n",
            "Loss: 6.868206e-02\n",
            "Loss: 6.868099e-02\n",
            "Loss: 6.867968e-02\n",
            "Loss: 6.867861e-02\n",
            "Loss: 6.867750e-02\n",
            "Loss: 6.867657e-02\n",
            "Loss: 6.868067e-02\n",
            "Loss: 6.867623e-02\n",
            "Loss: 6.867529e-02\n",
            "Loss: 6.867415e-02\n",
            "Loss: 6.867301e-02\n",
            "Loss: 6.867237e-02\n",
            "Loss: 6.867067e-02\n",
            "Loss: 6.866956e-02\n",
            "Loss: 6.866707e-02\n",
            "Loss: 6.866459e-02\n",
            "Loss: 6.866317e-02\n",
            "Loss: 6.866003e-02\n",
            "Loss: 6.866311e-02\n",
            "Loss: 6.865952e-02\n",
            "Loss: 6.865846e-02\n",
            "Loss: 6.865669e-02\n",
            "Loss: 6.865424e-02\n",
            "Loss: 6.865668e-02\n",
            "Loss: 6.865259e-02\n",
            "Loss: 6.865036e-02\n",
            "Loss: 6.864910e-02\n",
            "Loss: 6.864751e-02\n",
            "Loss: 6.864575e-02\n",
            "Loss: 6.864544e-02\n",
            "Loss: 6.864423e-02\n",
            "Loss: 6.864134e-02\n",
            "Loss: 6.863926e-02\n",
            "Loss: 6.863792e-02\n",
            "Loss: 6.863648e-02\n",
            "Loss: 6.863475e-02\n",
            "Loss: 6.863313e-02\n",
            "Loss: 6.863201e-02\n",
            "Loss: 6.863148e-02\n",
            "Loss: 6.863001e-02\n",
            "Loss: 6.862868e-02\n",
            "Loss: 6.862829e-02\n",
            "Loss: 6.862719e-02\n",
            "Loss: 6.862646e-02\n",
            "Loss: 6.862566e-02\n",
            "Loss: 6.862415e-02\n",
            "Loss: 6.862538e-02\n",
            "Loss: 6.862365e-02\n",
            "Loss: 6.862294e-02\n",
            "Loss: 6.862240e-02\n",
            "Loss: 6.862019e-02\n",
            "Loss: 6.861843e-02\n",
            "Loss: 6.861805e-02\n",
            "Loss: 6.861564e-02\n",
            "Loss: 6.861509e-02\n",
            "Loss: 6.861447e-02\n",
            "Loss: 6.861286e-02\n",
            "Loss: 6.861164e-02\n",
            "Loss: 6.861091e-02\n",
            "Loss: 6.861027e-02\n",
            "Loss: 6.860973e-02\n",
            "Loss: 6.860878e-02\n",
            "Loss: 6.860790e-02\n",
            "Loss: 6.860753e-02\n",
            "Loss: 6.860626e-02\n",
            "Loss: 6.860583e-02\n",
            "Loss: 6.860496e-02\n",
            "Loss: 6.860438e-02\n",
            "Loss: 6.860318e-02\n",
            "Loss: 6.860187e-02\n",
            "Loss: 6.859998e-02\n",
            "Loss: 6.859844e-02\n",
            "Loss: 6.859691e-02\n",
            "Loss: 6.859580e-02\n",
            "Loss: 6.859463e-02\n",
            "Loss: 6.859184e-02\n",
            "Loss: 6.860572e-02\n",
            "Loss: 6.859129e-02\n",
            "Loss: 6.858937e-02\n",
            "Loss: 6.858794e-02\n",
            "Loss: 6.858661e-02\n",
            "Loss: 6.858511e-02\n",
            "Loss: 6.858423e-02\n",
            "Loss: 6.858277e-02\n",
            "Loss: 6.858149e-02\n",
            "Loss: 6.858020e-02\n",
            "Loss: 6.857897e-02\n",
            "Loss: 6.857785e-02\n",
            "Loss: 6.857674e-02\n",
            "Loss: 6.857496e-02\n",
            "Loss: 6.857453e-02\n",
            "Loss: 6.857334e-02\n",
            "Loss: 6.857289e-02\n",
            "Loss: 6.857166e-02\n",
            "Loss: 6.857063e-02\n",
            "Loss: 6.856939e-02\n",
            "Loss: 6.856888e-02\n",
            "Loss: 6.856782e-02\n",
            "Loss: 6.856731e-02\n",
            "Loss: 6.856639e-02\n",
            "Loss: 6.856538e-02\n",
            "Loss: 6.856401e-02\n",
            "Loss: 6.856328e-02\n",
            "Loss: 6.856212e-02\n",
            "Loss: 6.856053e-02\n",
            "Loss: 6.855755e-02\n",
            "Loss: 6.855649e-02\n",
            "Loss: 6.855345e-02\n",
            "Loss: 6.855203e-02\n",
            "Loss: 6.855095e-02\n",
            "Loss: 6.855270e-02\n",
            "Loss: 6.854965e-02\n",
            "Loss: 6.854903e-02\n",
            "Loss: 6.854755e-02\n",
            "Loss: 6.854682e-02\n",
            "Loss: 6.854623e-02\n",
            "Loss: 6.854513e-02\n",
            "Loss: 6.854217e-02\n",
            "Loss: 6.854013e-02\n",
            "Loss: 6.854068e-02\n",
            "Loss: 6.853961e-02\n",
            "Loss: 6.853820e-02\n",
            "Loss: 6.853655e-02\n",
            "Loss: 6.853541e-02\n",
            "Loss: 6.853372e-02\n",
            "Loss: 6.853125e-02\n",
            "Loss: 6.853893e-02\n",
            "Loss: 6.853040e-02\n",
            "Loss: 6.852851e-02\n",
            "Loss: 6.852749e-02\n",
            "Loss: 6.852568e-02\n",
            "Loss: 6.852347e-02\n",
            "Loss: 6.852106e-02\n",
            "Loss: 6.851967e-02\n",
            "Loss: 6.851728e-02\n",
            "Loss: 6.851545e-02\n",
            "Loss: 6.851371e-02\n",
            "Loss: 6.851093e-02\n",
            "Loss: 6.850965e-02\n",
            "Loss: 6.850748e-02\n",
            "Loss: 6.850950e-02\n",
            "Loss: 6.850653e-02\n",
            "Loss: 6.850532e-02\n",
            "Loss: 6.850409e-02\n",
            "Loss: 6.850174e-02\n",
            "Loss: 6.850000e-02\n",
            "Loss: 6.849766e-02\n",
            "Loss: 6.849603e-02\n",
            "Loss: 6.849417e-02\n",
            "Loss: 6.849252e-02\n",
            "Loss: 6.849020e-02\n",
            "Loss: 6.848832e-02\n",
            "Loss: 6.848799e-02\n",
            "Loss: 6.848744e-02\n",
            "Loss: 6.848682e-02\n",
            "Loss: 6.848618e-02\n",
            "Loss: 6.848485e-02\n",
            "Loss: 6.848471e-02\n",
            "Loss: 6.848300e-02\n",
            "Loss: 6.848249e-02\n",
            "Loss: 6.848121e-02\n",
            "Loss: 6.847948e-02\n",
            "Loss: 6.847697e-02\n",
            "Loss: 6.847712e-02\n",
            "Loss: 6.847596e-02\n",
            "Loss: 6.847505e-02\n",
            "Loss: 6.847351e-02\n",
            "Loss: 6.847277e-02\n",
            "Loss: 6.847111e-02\n",
            "Loss: 6.846945e-02\n",
            "Loss: 6.846719e-02\n",
            "Loss: 6.846589e-02\n",
            "Loss: 6.846358e-02\n",
            "Loss: 6.846211e-02\n",
            "Loss: 6.846133e-02\n",
            "Loss: 6.846088e-02\n",
            "Loss: 6.846048e-02\n",
            "Loss: 6.845955e-02\n",
            "Loss: 6.845861e-02\n",
            "Loss: 6.845701e-02\n",
            "Loss: 6.845427e-02\n",
            "Loss: 6.846517e-02\n",
            "Loss: 6.845364e-02\n",
            "Loss: 6.845250e-02\n",
            "Loss: 6.845208e-02\n",
            "Loss: 6.845134e-02\n",
            "Loss: 6.844991e-02\n",
            "Loss: 6.844943e-02\n",
            "Loss: 6.844764e-02\n",
            "Loss: 6.844684e-02\n",
            "Loss: 6.844586e-02\n",
            "Loss: 6.844486e-02\n",
            "Loss: 6.844378e-02\n",
            "Loss: 6.844223e-02\n",
            "Loss: 6.844160e-02\n",
            "Loss: 6.844009e-02\n",
            "Loss: 6.843934e-02\n",
            "Loss: 6.843782e-02\n",
            "Loss: 6.843656e-02\n",
            "Loss: 6.843528e-02\n",
            "Loss: 6.843345e-02\n",
            "Loss: 6.843539e-02\n",
            "Loss: 6.843273e-02\n",
            "Loss: 6.843104e-02\n",
            "Loss: 6.843000e-02\n",
            "Loss: 6.842856e-02\n",
            "Loss: 6.842751e-02\n",
            "Loss: 6.842633e-02\n",
            "Loss: 6.842498e-02\n",
            "Loss: 6.842419e-02\n",
            "Loss: 6.842308e-02\n",
            "Loss: 6.842157e-02\n",
            "Loss: 6.842048e-02\n",
            "Loss: 6.841898e-02\n",
            "Loss: 6.841877e-02\n",
            "Loss: 6.841821e-02\n",
            "Loss: 6.841703e-02\n",
            "Loss: 6.841507e-02\n",
            "Loss: 6.841298e-02\n",
            "Loss: 6.840961e-02\n",
            "Loss: 6.841318e-02\n",
            "Loss: 6.840859e-02\n",
            "Loss: 6.840652e-02\n",
            "Loss: 6.840511e-02\n",
            "Loss: 6.840355e-02\n",
            "Loss: 6.840295e-02\n",
            "Loss: 6.840140e-02\n",
            "Loss: 6.840079e-02\n",
            "Loss: 6.839995e-02\n",
            "Loss: 6.839862e-02\n",
            "Loss: 6.840036e-02\n",
            "Loss: 6.839769e-02\n",
            "Loss: 6.839694e-02\n",
            "Loss: 6.839538e-02\n",
            "Loss: 6.839526e-02\n",
            "Loss: 6.839326e-02\n",
            "Loss: 6.839433e-02\n",
            "Loss: 6.839273e-02\n",
            "Loss: 6.839029e-02\n",
            "Loss: 6.838908e-02\n",
            "Loss: 6.838743e-02\n",
            "Loss: 6.838676e-02\n",
            "Loss: 6.838530e-02\n",
            "Loss: 6.838395e-02\n",
            "Loss: 6.838324e-02\n",
            "Loss: 6.838122e-02\n",
            "Loss: 6.837992e-02\n",
            "Loss: 6.837881e-02\n",
            "Loss: 6.837752e-02\n",
            "Loss: 6.837588e-02\n",
            "Loss: 6.837457e-02\n",
            "Loss: 6.837247e-02\n",
            "Loss: 6.837149e-02\n",
            "Loss: 6.837030e-02\n",
            "Loss: 6.836797e-02\n",
            "Loss: 6.836703e-02\n",
            "Loss: 6.836515e-02\n",
            "Loss: 6.836380e-02\n",
            "Loss: 6.836203e-02\n",
            "Loss: 6.837153e-02\n",
            "Loss: 6.836068e-02\n",
            "Loss: 6.835803e-02\n",
            "Loss: 6.835593e-02\n",
            "Loss: 6.835455e-02\n",
            "Loss: 6.835356e-02\n",
            "Loss: 6.835264e-02\n",
            "Loss: 6.835135e-02\n",
            "Loss: 6.835013e-02\n",
            "Loss: 6.834844e-02\n",
            "Loss: 6.834743e-02\n",
            "Loss: 6.834544e-02\n",
            "Loss: 6.834410e-02\n",
            "Loss: 6.834254e-02\n",
            "Loss: 6.834029e-02\n",
            "Loss: 6.834054e-02\n",
            "Loss: 6.833897e-02\n",
            "Loss: 6.833770e-02\n",
            "Loss: 6.833660e-02\n",
            "Loss: 6.833551e-02\n",
            "Loss: 6.833442e-02\n",
            "Loss: 6.833243e-02\n",
            "Loss: 6.833055e-02\n",
            "Loss: 6.832730e-02\n",
            "Loss: 6.833087e-02\n",
            "Loss: 6.832558e-02\n",
            "Loss: 6.832296e-02\n",
            "Loss: 6.832068e-02\n",
            "Loss: 6.831989e-02\n",
            "Loss: 6.831887e-02\n",
            "Loss: 6.831797e-02\n",
            "Loss: 6.831672e-02\n",
            "Loss: 6.831618e-02\n",
            "Loss: 6.831369e-02\n",
            "Loss: 6.831536e-02\n",
            "Loss: 6.831272e-02\n",
            "Loss: 6.831051e-02\n",
            "Loss: 6.830768e-02\n",
            "Loss: 6.830593e-02\n",
            "Loss: 6.830403e-02\n",
            "Loss: 6.830263e-02\n",
            "Loss: 6.830063e-02\n",
            "Loss: 6.829864e-02\n",
            "Loss: 6.829652e-02\n",
            "Loss: 6.829534e-02\n",
            "Loss: 6.829366e-02\n",
            "Loss: 6.829073e-02\n",
            "Loss: 6.829108e-02\n",
            "Loss: 6.828883e-02\n",
            "Loss: 6.828614e-02\n",
            "Loss: 6.828529e-02\n",
            "Loss: 6.828412e-02\n",
            "Loss: 6.828208e-02\n",
            "Loss: 6.827992e-02\n",
            "Loss: 6.827806e-02\n",
            "Loss: 6.827616e-02\n",
            "Loss: 6.827513e-02\n",
            "Loss: 6.827390e-02\n",
            "Loss: 6.827054e-02\n",
            "Loss: 6.826960e-02\n",
            "Loss: 6.826512e-02\n",
            "Loss: 6.826332e-02\n",
            "Loss: 6.826182e-02\n",
            "Loss: 6.826088e-02\n",
            "Loss: 6.826013e-02\n",
            "Loss: 6.825877e-02\n",
            "Loss: 6.825764e-02\n",
            "Loss: 6.825530e-02\n",
            "Loss: 6.825399e-02\n",
            "Loss: 6.825171e-02\n",
            "Loss: 6.825063e-02\n",
            "Loss: 6.824896e-02\n",
            "Loss: 6.824613e-02\n",
            "Loss: 6.825259e-02\n",
            "Loss: 6.824557e-02\n",
            "Loss: 6.824362e-02\n",
            "Loss: 6.824265e-02\n",
            "Loss: 6.824041e-02\n",
            "Loss: 6.823902e-02\n",
            "Loss: 6.823633e-02\n",
            "Loss: 6.823473e-02\n",
            "Loss: 6.823267e-02\n",
            "Loss: 6.823035e-02\n",
            "Loss: 6.822816e-02\n",
            "Loss: 6.822609e-02\n",
            "Loss: 6.822523e-02\n",
            "Loss: 6.822389e-02\n",
            "Loss: 6.822204e-02\n",
            "Loss: 6.821927e-02\n",
            "Loss: 6.821838e-02\n",
            "Loss: 6.821556e-02\n",
            "Loss: 6.821394e-02\n",
            "Loss: 6.821243e-02\n",
            "Loss: 6.821038e-02\n",
            "Loss: 6.820821e-02\n",
            "Loss: 6.820615e-02\n",
            "Loss: 6.820501e-02\n",
            "Loss: 6.820374e-02\n",
            "Loss: 6.820410e-02\n",
            "Loss: 6.820337e-02\n",
            "Loss: 6.820187e-02\n",
            "Loss: 6.820139e-02\n",
            "Loss: 6.819962e-02\n",
            "Loss: 6.819957e-02\n",
            "Loss: 6.819823e-02\n",
            "Loss: 6.819777e-02\n",
            "Loss: 6.819703e-02\n",
            "Loss: 6.819592e-02\n",
            "Loss: 6.819330e-02\n",
            "Loss: 6.819333e-02\n",
            "Loss: 6.819163e-02\n",
            "Loss: 6.818949e-02\n",
            "Loss: 6.818689e-02\n",
            "Loss: 6.818490e-02\n",
            "Loss: 6.818171e-02\n",
            "Loss: 6.818621e-02\n",
            "Loss: 6.818028e-02\n",
            "Loss: 6.817800e-02\n",
            "Loss: 6.817611e-02\n",
            "Loss: 6.817536e-02\n",
            "Loss: 6.817363e-02\n",
            "Loss: 6.817328e-02\n",
            "Loss: 6.817222e-02\n",
            "Loss: 6.817172e-02\n",
            "Loss: 6.816936e-02\n",
            "Loss: 6.816835e-02\n",
            "Loss: 6.816571e-02\n",
            "Loss: 6.816491e-02\n",
            "Loss: 6.816314e-02\n",
            "Loss: 6.816222e-02\n",
            "Loss: 6.816070e-02\n",
            "Loss: 6.815918e-02\n",
            "Loss: 6.815736e-02\n",
            "Loss: 6.815703e-02\n",
            "Loss: 6.815606e-02\n",
            "Loss: 6.815507e-02\n",
            "Loss: 6.815536e-02\n",
            "Loss: 6.815417e-02\n",
            "Loss: 6.815298e-02\n",
            "Loss: 6.815277e-02\n",
            "Loss: 6.815187e-02\n",
            "Loss: 6.815159e-02\n",
            "Loss: 6.815083e-02\n",
            "Loss: 6.814994e-02\n",
            "Loss: 6.814785e-02\n",
            "Loss: 6.814787e-02\n",
            "Loss: 6.814697e-02\n",
            "Loss: 6.814523e-02\n",
            "Loss: 6.814367e-02\n",
            "Loss: 6.814262e-02\n",
            "Loss: 6.814102e-02\n",
            "Loss: 6.814475e-02\n",
            "Loss: 6.814024e-02\n",
            "Loss: 6.813891e-02\n",
            "Loss: 6.813760e-02\n",
            "Loss: 6.813605e-02\n",
            "Loss: 6.813514e-02\n",
            "Loss: 6.813350e-02\n",
            "Loss: 6.813244e-02\n",
            "Loss: 6.813098e-02\n",
            "Loss: 6.812951e-02\n",
            "Loss: 6.812746e-02\n",
            "Loss: 6.812559e-02\n",
            "Loss: 6.812233e-02\n",
            "Loss: 6.812091e-02\n",
            "Loss: 6.812014e-02\n",
            "Loss: 6.811842e-02\n",
            "Loss: 6.811742e-02\n",
            "Loss: 6.811558e-02\n",
            "Loss: 6.811431e-02\n",
            "Loss: 6.811340e-02\n",
            "Loss: 6.811223e-02\n",
            "Loss: 6.811121e-02\n",
            "Loss: 6.811030e-02\n",
            "Loss: 6.810955e-02\n",
            "Loss: 6.810880e-02\n",
            "Loss: 6.810810e-02\n",
            "Loss: 6.810750e-02\n",
            "Loss: 6.810591e-02\n",
            "Loss: 6.810512e-02\n",
            "Loss: 6.810401e-02\n",
            "Loss: 6.810313e-02\n",
            "Loss: 6.810214e-02\n",
            "Loss: 6.810047e-02\n",
            "Loss: 6.809899e-02\n",
            "Loss: 6.809746e-02\n",
            "Loss: 6.809686e-02\n",
            "Loss: 6.809574e-02\n",
            "Loss: 6.809372e-02\n",
            "Loss: 6.809180e-02\n",
            "Loss: 6.808927e-02\n",
            "Loss: 6.808766e-02\n",
            "Loss: 6.808596e-02\n",
            "Loss: 6.808492e-02\n",
            "Loss: 6.808342e-02\n",
            "Loss: 6.808283e-02\n",
            "Loss: 6.808157e-02\n",
            "Loss: 6.807960e-02\n",
            "Loss: 6.807742e-02\n",
            "Loss: 6.808160e-02\n",
            "Loss: 6.807667e-02\n",
            "Loss: 6.807563e-02\n",
            "Loss: 6.807461e-02\n",
            "Loss: 6.807283e-02\n",
            "Loss: 6.807055e-02\n",
            "Loss: 6.807533e-02\n",
            "Loss: 6.806898e-02\n",
            "Loss: 6.806698e-02\n",
            "Loss: 6.806516e-02\n",
            "Loss: 6.806353e-02\n",
            "Loss: 6.806180e-02\n",
            "Loss: 6.805974e-02\n",
            "Loss: 6.807790e-02\n",
            "Loss: 6.805853e-02\n",
            "Loss: 6.805710e-02\n",
            "Loss: 6.805552e-02\n",
            "Loss: 6.805377e-02\n",
            "Loss: 6.805249e-02\n",
            "Loss: 6.805123e-02\n",
            "Loss: 6.804939e-02\n",
            "Loss: 6.804831e-02\n",
            "Loss: 6.804695e-02\n",
            "Loss: 6.804425e-02\n",
            "Loss: 6.804378e-02\n",
            "Loss: 6.804267e-02\n",
            "Loss: 6.804150e-02\n",
            "Loss: 6.803990e-02\n",
            "Loss: 6.803852e-02\n",
            "Loss: 6.803766e-02\n",
            "Loss: 6.803656e-02\n",
            "Loss: 6.803571e-02\n",
            "Loss: 6.803392e-02\n",
            "Loss: 6.803309e-02\n",
            "Loss: 6.803194e-02\n",
            "Loss: 6.803104e-02\n",
            "Loss: 6.802989e-02\n",
            "Loss: 6.802862e-02\n",
            "Loss: 6.802726e-02\n",
            "Loss: 6.802523e-02\n",
            "Loss: 6.802464e-02\n",
            "Loss: 6.802280e-02\n",
            "Loss: 6.802091e-02\n",
            "Loss: 6.802483e-02\n",
            "Loss: 6.801973e-02\n",
            "Loss: 6.801793e-02\n",
            "Loss: 6.801564e-02\n",
            "Loss: 6.801426e-02\n",
            "Loss: 6.801245e-02\n",
            "Loss: 6.801052e-02\n",
            "Loss: 6.800961e-02\n",
            "Loss: 6.801071e-02\n",
            "Loss: 6.800911e-02\n",
            "Loss: 6.800841e-02\n",
            "Loss: 6.800728e-02\n",
            "Loss: 6.800594e-02\n",
            "Loss: 6.800397e-02\n",
            "Loss: 6.800621e-02\n",
            "Loss: 6.800319e-02\n",
            "Loss: 6.800187e-02\n",
            "Loss: 6.800082e-02\n",
            "Loss: 6.799870e-02\n",
            "Loss: 6.799626e-02\n",
            "Loss: 6.799972e-02\n",
            "Loss: 6.799522e-02\n",
            "Loss: 6.799321e-02\n",
            "Loss: 6.799217e-02\n",
            "Loss: 6.799064e-02\n",
            "Loss: 6.798812e-02\n",
            "Loss: 6.798892e-02\n",
            "Loss: 6.798703e-02\n",
            "Loss: 6.798365e-02\n",
            "Loss: 6.798265e-02\n",
            "Loss: 6.798063e-02\n",
            "Loss: 6.797914e-02\n",
            "Loss: 6.797716e-02\n",
            "Loss: 6.797540e-02\n",
            "Loss: 6.797382e-02\n",
            "Loss: 6.797229e-02\n",
            "Loss: 6.797107e-02\n",
            "Loss: 6.797058e-02\n",
            "Loss: 6.796988e-02\n",
            "Loss: 6.796897e-02\n",
            "Loss: 6.796791e-02\n",
            "Loss: 6.796708e-02\n",
            "Loss: 6.796495e-02\n",
            "Loss: 6.796570e-02\n",
            "Loss: 6.796378e-02\n",
            "Loss: 6.796191e-02\n",
            "Loss: 6.796082e-02\n",
            "Loss: 6.796004e-02\n",
            "Loss: 6.795919e-02\n",
            "Loss: 6.795853e-02\n",
            "Loss: 6.795765e-02\n",
            "Loss: 6.795678e-02\n",
            "Loss: 6.795534e-02\n",
            "Loss: 6.795566e-02\n",
            "Loss: 6.795454e-02\n",
            "Loss: 6.795390e-02\n",
            "Loss: 6.795345e-02\n",
            "Loss: 6.795330e-02\n",
            "Loss: 6.795170e-02\n",
            "Loss: 6.795041e-02\n",
            "Loss: 6.794873e-02\n",
            "Loss: 6.794797e-02\n",
            "Loss: 6.794740e-02\n",
            "Loss: 6.794615e-02\n",
            "Loss: 6.794544e-02\n",
            "Loss: 6.794465e-02\n",
            "Loss: 6.794417e-02\n",
            "Loss: 6.794392e-02\n",
            "Loss: 6.794299e-02\n",
            "Loss: 6.794111e-02\n",
            "Loss: 6.793983e-02\n",
            "Loss: 6.793813e-02\n",
            "Loss: 6.793731e-02\n",
            "Loss: 6.793608e-02\n",
            "Loss: 6.793468e-02\n",
            "Loss: 6.793374e-02\n",
            "Loss: 6.793240e-02\n",
            "Loss: 6.793217e-02\n",
            "Loss: 6.793167e-02\n",
            "Loss: 6.793054e-02\n",
            "Loss: 6.792931e-02\n",
            "Loss: 6.792782e-02\n",
            "Loss: 6.792694e-02\n",
            "Loss: 6.792609e-02\n",
            "Loss: 6.792645e-02\n",
            "Loss: 6.792530e-02\n",
            "Loss: 6.792390e-02\n",
            "Loss: 6.792294e-02\n",
            "Loss: 6.792203e-02\n",
            "Loss: 6.792112e-02\n",
            "Loss: 6.792027e-02\n",
            "Loss: 6.791924e-02\n",
            "Loss: 6.791783e-02\n",
            "Loss: 6.791884e-02\n",
            "Loss: 6.791735e-02\n",
            "Loss: 6.791639e-02\n",
            "Loss: 6.791551e-02\n",
            "Loss: 6.791509e-02\n",
            "Loss: 6.791426e-02\n",
            "Loss: 6.791307e-02\n",
            "Loss: 6.791130e-02\n",
            "Loss: 6.791072e-02\n",
            "Loss: 6.790990e-02\n",
            "Loss: 6.790897e-02\n",
            "Loss: 6.790750e-02\n",
            "Loss: 6.790689e-02\n",
            "Loss: 6.790593e-02\n",
            "Loss: 6.790483e-02\n",
            "Loss: 6.791411e-02\n",
            "Loss: 6.790419e-02\n",
            "Loss: 6.790306e-02\n",
            "Loss: 6.790214e-02\n",
            "Loss: 6.790046e-02\n",
            "Loss: 6.790014e-02\n",
            "Loss: 6.789952e-02\n",
            "Loss: 6.789870e-02\n",
            "Loss: 6.789820e-02\n",
            "Loss: 6.789760e-02\n",
            "Loss: 6.789563e-02\n",
            "Loss: 6.789473e-02\n",
            "Loss: 6.789330e-02\n",
            "Loss: 6.789254e-02\n",
            "Loss: 6.789172e-02\n",
            "Loss: 6.789056e-02\n",
            "Loss: 6.789700e-02\n",
            "Loss: 6.788992e-02\n",
            "Loss: 6.788920e-02\n",
            "Loss: 6.788847e-02\n",
            "Loss: 6.788787e-02\n",
            "Loss: 6.788746e-02\n",
            "Loss: 6.788456e-02\n",
            "Loss: 6.788415e-02\n",
            "Loss: 6.788362e-02\n",
            "Loss: 6.788213e-02\n",
            "Loss: 6.788027e-02\n",
            "Loss: 6.787916e-02\n",
            "Loss: 6.787688e-02\n",
            "Loss: 6.787799e-02\n",
            "Loss: 6.787580e-02\n",
            "Loss: 6.787480e-02\n",
            "Loss: 6.787360e-02\n",
            "Loss: 6.787138e-02\n",
            "Loss: 6.786934e-02\n",
            "Loss: 6.786729e-02\n",
            "Loss: 6.786586e-02\n",
            "Loss: 6.786499e-02\n",
            "Loss: 6.786386e-02\n",
            "Loss: 6.786294e-02\n",
            "Loss: 6.786232e-02\n",
            "Loss: 6.786105e-02\n",
            "Loss: 6.786057e-02\n",
            "Loss: 6.785969e-02\n",
            "Loss: 6.785795e-02\n",
            "Loss: 6.785650e-02\n",
            "Loss: 6.785708e-02\n",
            "Loss: 6.785540e-02\n",
            "Loss: 6.785446e-02\n",
            "Loss: 6.785302e-02\n",
            "Loss: 6.785245e-02\n",
            "Loss: 6.785184e-02\n",
            "Loss: 6.785100e-02\n",
            "Loss: 6.785017e-02\n",
            "Loss: 6.784926e-02\n",
            "Loss: 6.784685e-02\n",
            "Loss: 6.784542e-02\n",
            "Loss: 6.784392e-02\n",
            "Loss: 6.784337e-02\n",
            "Loss: 6.784224e-02\n",
            "Loss: 6.784039e-02\n",
            "Loss: 6.783839e-02\n",
            "Loss: 6.784270e-02\n",
            "Loss: 6.783707e-02\n",
            "Loss: 6.783535e-02\n",
            "Loss: 6.783438e-02\n",
            "Loss: 6.783307e-02\n",
            "Loss: 6.783085e-02\n",
            "Loss: 6.784859e-02\n",
            "Loss: 6.783041e-02\n",
            "Loss: 6.782965e-02\n",
            "Loss: 6.782889e-02\n",
            "Loss: 6.782767e-02\n",
            "Loss: 6.782582e-02\n",
            "Loss: 6.782308e-02\n",
            "Loss: 6.782133e-02\n",
            "Loss: 6.782016e-02\n",
            "Loss: 6.781920e-02\n",
            "Loss: 6.781774e-02\n",
            "Loss: 6.781545e-02\n",
            "Loss: 6.781559e-02\n",
            "Loss: 6.781389e-02\n",
            "Loss: 6.781315e-02\n",
            "Loss: 6.781204e-02\n",
            "Loss: 6.781095e-02\n",
            "Loss: 6.780967e-02\n",
            "Loss: 6.780866e-02\n",
            "Loss: 6.780774e-02\n",
            "Loss: 6.780723e-02\n",
            "Loss: 6.780636e-02\n",
            "Loss: 6.780481e-02\n",
            "Loss: 6.780740e-02\n",
            "Loss: 6.780417e-02\n",
            "Loss: 6.780257e-02\n",
            "Loss: 6.780145e-02\n",
            "Loss: 6.780032e-02\n",
            "Loss: 6.779875e-02\n",
            "Loss: 6.779687e-02\n",
            "Loss: 6.780387e-02\n",
            "Loss: 6.779637e-02\n",
            "Loss: 6.779502e-02\n",
            "Loss: 6.779410e-02\n",
            "Loss: 6.779270e-02\n",
            "Loss: 6.779014e-02\n",
            "Loss: 6.778660e-02\n",
            "Loss: 6.778514e-02\n",
            "Loss: 6.778230e-02\n",
            "Loss: 6.778099e-02\n",
            "Loss: 6.778041e-02\n",
            "Loss: 6.777927e-02\n",
            "Loss: 6.777849e-02\n",
            "Loss: 6.777816e-02\n",
            "Loss: 6.777746e-02\n",
            "Loss: 6.777658e-02\n",
            "Loss: 6.777556e-02\n",
            "Loss: 6.777444e-02\n",
            "Loss: 6.777415e-02\n",
            "Loss: 6.777298e-02\n",
            "Loss: 6.776948e-02\n",
            "Loss: 6.776860e-02\n",
            "Loss: 6.776667e-02\n",
            "Loss: 6.776578e-02\n",
            "Loss: 6.776503e-02\n",
            "Loss: 6.776273e-02\n",
            "Loss: 6.776431e-02\n",
            "Loss: 6.776246e-02\n",
            "Loss: 6.776150e-02\n",
            "Loss: 6.776132e-02\n",
            "Loss: 6.776045e-02\n",
            "Loss: 6.775949e-02\n",
            "Loss: 6.775737e-02\n",
            "Loss: 6.776065e-02\n",
            "Loss: 6.775647e-02\n",
            "Loss: 6.775528e-02\n",
            "Loss: 6.775396e-02\n",
            "Loss: 6.775306e-02\n",
            "Loss: 6.775177e-02\n",
            "Loss: 6.775435e-02\n",
            "Loss: 6.775102e-02\n",
            "Loss: 6.774954e-02\n",
            "Loss: 6.774823e-02\n",
            "Loss: 6.774695e-02\n",
            "Loss: 6.774600e-02\n",
            "Loss: 6.774504e-02\n",
            "Loss: 6.774393e-02\n",
            "Loss: 6.774380e-02\n",
            "Loss: 6.774324e-02\n",
            "Loss: 6.774171e-02\n",
            "Loss: 6.774037e-02\n",
            "Loss: 6.773911e-02\n",
            "Loss: 6.773775e-02\n",
            "Loss: 6.773721e-02\n",
            "Loss: 6.773670e-02\n",
            "Loss: 6.773547e-02\n",
            "Loss: 6.773480e-02\n",
            "Loss: 6.773474e-02\n",
            "Loss: 6.773378e-02\n",
            "Loss: 6.773312e-02\n",
            "Loss: 6.773242e-02\n",
            "Loss: 6.773126e-02\n",
            "Loss: 6.773072e-02\n",
            "Loss: 6.772930e-02\n",
            "Loss: 6.772830e-02\n",
            "Loss: 6.772877e-02\n",
            "Loss: 6.772814e-02\n",
            "Loss: 6.772683e-02\n",
            "Loss: 6.772570e-02\n",
            "Loss: 6.772506e-02\n",
            "Loss: 6.772311e-02\n",
            "Loss: 6.772223e-02\n",
            "Loss: 6.772137e-02\n",
            "Loss: 6.772059e-02\n",
            "Loss: 6.771951e-02\n",
            "Loss: 6.771823e-02\n",
            "Loss: 6.771637e-02\n",
            "Loss: 6.771637e-02\n",
            "Loss: 6.771530e-02\n",
            "Loss: 6.771405e-02\n",
            "Loss: 6.771316e-02\n",
            "Loss: 6.771229e-02\n",
            "Loss: 6.771079e-02\n",
            "Loss: 6.771505e-02\n",
            "Loss: 6.771077e-02\n",
            "Loss: 6.770984e-02\n",
            "Loss: 6.770890e-02\n",
            "Loss: 6.770761e-02\n",
            "Loss: 6.770581e-02\n",
            "Loss: 6.770501e-02\n",
            "Loss: 6.770358e-02\n",
            "Loss: 6.770349e-02\n",
            "Loss: 6.770184e-02\n",
            "Loss: 6.770157e-02\n",
            "Loss: 6.770020e-02\n",
            "Loss: 6.769937e-02\n",
            "Loss: 6.769855e-02\n",
            "Loss: 6.769709e-02\n",
            "Loss: 6.769572e-02\n",
            "Loss: 6.769394e-02\n",
            "Loss: 6.769250e-02\n",
            "Loss: 6.769136e-02\n",
            "Loss: 6.769007e-02\n",
            "Loss: 6.768897e-02\n",
            "Loss: 6.768788e-02\n",
            "Loss: 6.768692e-02\n",
            "Loss: 6.768464e-02\n",
            "Loss: 6.768547e-02\n",
            "Loss: 6.768359e-02\n",
            "Loss: 6.768287e-02\n",
            "Loss: 6.768233e-02\n",
            "Loss: 6.768211e-02\n",
            "Loss: 6.768116e-02\n",
            "Loss: 6.768044e-02\n",
            "Loss: 6.768013e-02\n",
            "Loss: 6.767867e-02\n",
            "Loss: 6.767929e-02\n",
            "Loss: 6.767729e-02\n",
            "Loss: 6.767524e-02\n",
            "Loss: 6.767415e-02\n",
            "Loss: 6.767348e-02\n",
            "Loss: 6.767287e-02\n",
            "Loss: 6.767182e-02\n",
            "Loss: 6.767134e-02\n",
            "Loss: 6.767076e-02\n",
            "Loss: 6.766949e-02\n",
            "Loss: 6.766878e-02\n",
            "Loss: 6.766746e-02\n",
            "Loss: 6.766739e-02\n",
            "Loss: 6.766655e-02\n",
            "Loss: 6.766523e-02\n",
            "Loss: 6.766446e-02\n",
            "Loss: 6.766341e-02\n",
            "Loss: 6.766281e-02\n",
            "Loss: 6.766193e-02\n",
            "Loss: 6.766102e-02\n",
            "Loss: 6.765932e-02\n",
            "Loss: 6.766814e-02\n",
            "Loss: 6.765896e-02\n",
            "Loss: 6.765766e-02\n",
            "Loss: 6.765608e-02\n",
            "Loss: 6.765436e-02\n",
            "Loss: 6.765294e-02\n",
            "Loss: 6.765121e-02\n",
            "Loss: 6.765042e-02\n",
            "Loss: 6.764872e-02\n",
            "Loss: 6.764786e-02\n",
            "Loss: 6.764720e-02\n",
            "Loss: 6.764582e-02\n",
            "Loss: 6.764358e-02\n",
            "Loss: 6.764876e-02\n",
            "Loss: 6.764304e-02\n",
            "Loss: 6.764121e-02\n",
            "Loss: 6.763994e-02\n",
            "Loss: 6.763851e-02\n",
            "Loss: 6.763781e-02\n",
            "Loss: 6.763683e-02\n",
            "Loss: 6.763617e-02\n",
            "Loss: 6.763561e-02\n",
            "Loss: 6.763457e-02\n",
            "Loss: 6.763317e-02\n",
            "Loss: 6.763179e-02\n",
            "Loss: 6.763115e-02\n",
            "Loss: 6.762979e-02\n",
            "Loss: 6.762869e-02\n",
            "Loss: 6.762727e-02\n",
            "Loss: 6.762666e-02\n",
            "Loss: 6.762584e-02\n",
            "Loss: 6.762569e-02\n",
            "Loss: 6.762517e-02\n",
            "Loss: 6.762456e-02\n",
            "Loss: 6.762212e-02\n",
            "Loss: 6.762160e-02\n",
            "Loss: 6.762070e-02\n",
            "Loss: 6.761973e-02\n",
            "Loss: 6.761864e-02\n",
            "Loss: 6.761891e-02\n",
            "Loss: 6.761833e-02\n",
            "Loss: 6.761714e-02\n",
            "Loss: 6.761603e-02\n",
            "Loss: 6.761555e-02\n",
            "Loss: 6.761396e-02\n",
            "Loss: 6.761598e-02\n",
            "Loss: 6.761301e-02\n",
            "Loss: 6.761158e-02\n",
            "Loss: 6.761083e-02\n",
            "Loss: 6.760927e-02\n",
            "Loss: 6.760823e-02\n",
            "Loss: 6.760702e-02\n",
            "Loss: 6.760620e-02\n",
            "Loss: 6.760401e-02\n",
            "Loss: 6.760313e-02\n",
            "Loss: 6.760155e-02\n",
            "Loss: 6.760048e-02\n",
            "Loss: 6.760197e-02\n",
            "Loss: 6.759996e-02\n",
            "Loss: 6.759945e-02\n",
            "Loss: 6.759855e-02\n",
            "Loss: 6.759812e-02\n",
            "Loss: 6.759717e-02\n",
            "Loss: 6.759604e-02\n",
            "Loss: 6.759464e-02\n",
            "Loss: 6.759356e-02\n",
            "Loss: 6.759192e-02\n",
            "Loss: 6.759081e-02\n",
            "Loss: 6.759030e-02\n",
            "Loss: 6.758809e-02\n",
            "Loss: 6.758727e-02\n",
            "Loss: 6.758621e-02\n",
            "Loss: 6.758522e-02\n",
            "Loss: 6.758385e-02\n",
            "Loss: 6.758256e-02\n",
            "Loss: 6.758133e-02\n",
            "Loss: 6.757967e-02\n",
            "Loss: 6.757808e-02\n",
            "Loss: 6.757643e-02\n",
            "Loss: 6.757441e-02\n",
            "Loss: 6.757360e-02\n",
            "Loss: 6.757136e-02\n",
            "Loss: 6.757053e-02\n",
            "Loss: 6.756845e-02\n",
            "Loss: 6.757841e-02\n",
            "Loss: 6.756792e-02\n",
            "Loss: 6.756623e-02\n",
            "Loss: 6.756481e-02\n",
            "Loss: 6.756362e-02\n",
            "Loss: 6.756312e-02\n",
            "Loss: 6.756154e-02\n",
            "Loss: 6.756067e-02\n",
            "Loss: 6.755865e-02\n",
            "Loss: 6.756320e-02\n",
            "Loss: 6.755826e-02\n",
            "Loss: 6.755726e-02\n",
            "Loss: 6.755686e-02\n",
            "Loss: 6.755653e-02\n",
            "Loss: 6.755582e-02\n",
            "Loss: 6.755479e-02\n",
            "Loss: 6.755325e-02\n",
            "Loss: 6.755307e-02\n",
            "Loss: 6.755098e-02\n",
            "Loss: 6.755029e-02\n",
            "Loss: 6.754923e-02\n",
            "Loss: 6.754705e-02\n",
            "Loss: 6.754424e-02\n",
            "Loss: 6.754228e-02\n",
            "Loss: 6.754109e-02\n",
            "Loss: 6.753990e-02\n",
            "Loss: 6.753772e-02\n",
            "Loss: 6.753562e-02\n",
            "Loss: 6.753418e-02\n",
            "Loss: 6.753455e-02\n",
            "Loss: 6.753361e-02\n",
            "Loss: 6.753311e-02\n",
            "Loss: 6.753203e-02\n",
            "Loss: 6.753105e-02\n",
            "Loss: 6.753054e-02\n",
            "Loss: 6.752954e-02\n",
            "Loss: 6.752893e-02\n",
            "Loss: 6.752809e-02\n",
            "Loss: 6.752658e-02\n",
            "Loss: 6.752451e-02\n",
            "Loss: 6.752233e-02\n",
            "Loss: 6.752156e-02\n",
            "Loss: 6.751995e-02\n",
            "Loss: 6.751920e-02\n",
            "Loss: 6.751838e-02\n",
            "Loss: 6.751736e-02\n",
            "Loss: 6.751589e-02\n",
            "Loss: 6.751589e-02\n",
            "Loss: 6.751561e-02\n",
            "Loss: 6.751465e-02\n",
            "Loss: 6.751467e-02\n",
            "Loss: 6.751454e-02\n",
            "Loss: 6.751376e-02\n",
            "Loss: 6.751283e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751421e-02\n",
            "Loss: 6.751121e-02\n",
            "Loss: 6.751127e-02\n",
            "Loss: 6.751158e-02\n",
            "Loss: 6.751136e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751124e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751131e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751131e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751131e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751119e-02\n",
            "Loss: 6.751403e-02\n",
            "Loss: 6.751099e-02\n",
            "Loss: 6.751020e-02\n",
            "Loss: 6.750949e-02\n",
            "Loss: 6.750887e-02\n",
            "Loss: 6.750759e-02\n",
            "Loss: 6.750636e-02\n",
            "Loss: 6.750548e-02\n",
            "Loss: 6.750470e-02\n",
            "Loss: 6.750414e-02\n",
            "Loss: 6.750286e-02\n",
            "Loss: 6.750201e-02\n",
            "Loss: 6.750057e-02\n",
            "Loss: 6.749947e-02\n",
            "Loss: 6.749850e-02\n",
            "Loss: 6.749689e-02\n",
            "Loss: 6.749737e-02\n",
            "Loss: 6.749611e-02\n",
            "Loss: 6.749489e-02\n",
            "Loss: 6.749392e-02\n",
            "Loss: 6.749489e-02\n",
            "Loss: 6.749366e-02\n",
            "Loss: 6.749279e-02\n",
            "Loss: 6.749228e-02\n",
            "Loss: 6.749135e-02\n",
            "Loss: 6.749070e-02\n",
            "Loss: 6.748945e-02\n",
            "Loss: 6.748886e-02\n",
            "Loss: 6.748685e-02\n",
            "Loss: 6.748538e-02\n",
            "Loss: 6.748457e-02\n",
            "Loss: 6.748321e-02\n",
            "Loss: 6.748366e-02\n",
            "Loss: 6.748305e-02\n",
            "Loss: 6.748246e-02\n",
            "Loss: 6.748226e-02\n",
            "Loss: 6.748165e-02\n",
            "Loss: 6.748041e-02\n",
            "Loss: 6.747951e-02\n",
            "Loss: 6.747831e-02\n",
            "Loss: 6.747656e-02\n",
            "Loss: 6.747411e-02\n",
            "Loss: 6.747606e-02\n",
            "Loss: 6.747328e-02\n",
            "Loss: 6.747167e-02\n",
            "Loss: 6.747080e-02\n",
            "Loss: 6.746957e-02\n",
            "Loss: 6.747023e-02\n",
            "Loss: 6.746862e-02\n",
            "Loss: 6.746738e-02\n",
            "Loss: 6.746636e-02\n",
            "Loss: 6.746541e-02\n",
            "Loss: 6.746466e-02\n",
            "Loss: 6.746358e-02\n",
            "Loss: 6.746294e-02\n",
            "Loss: 6.746232e-02\n",
            "Loss: 6.746119e-02\n",
            "Loss: 6.745803e-02\n",
            "Loss: 6.746485e-02\n",
            "Loss: 6.745739e-02\n",
            "Loss: 6.745491e-02\n",
            "Loss: 6.745383e-02\n",
            "Loss: 6.745367e-02\n",
            "Loss: 6.745231e-02\n",
            "Loss: 6.745194e-02\n",
            "Loss: 6.745051e-02\n",
            "Loss: 6.744990e-02\n",
            "Loss: 6.744964e-02\n",
            "Loss: 6.744854e-02\n",
            "Loss: 6.744803e-02\n",
            "Loss: 6.744712e-02\n",
            "Loss: 6.744629e-02\n",
            "Loss: 6.744495e-02\n",
            "Loss: 6.744421e-02\n",
            "Loss: 6.744386e-02\n",
            "Loss: 6.744297e-02\n",
            "Loss: 6.744211e-02\n",
            "Loss: 6.744078e-02\n",
            "Loss: 6.743970e-02\n",
            "Loss: 6.743830e-02\n",
            "Loss: 6.743981e-02\n",
            "Loss: 6.743784e-02\n",
            "Loss: 6.743740e-02\n",
            "Loss: 6.743593e-02\n",
            "Loss: 6.743547e-02\n",
            "Loss: 6.743316e-02\n",
            "Loss: 6.743251e-02\n",
            "Loss: 6.743208e-02\n",
            "Loss: 6.743118e-02\n",
            "Loss: 6.743038e-02\n",
            "Loss: 6.743015e-02\n",
            "Loss: 6.742951e-02\n",
            "Loss: 6.742874e-02\n",
            "Loss: 6.743326e-02\n",
            "Loss: 6.742824e-02\n",
            "Loss: 6.742784e-02\n",
            "Loss: 6.742749e-02\n",
            "Loss: 6.742653e-02\n",
            "Loss: 6.742591e-02\n",
            "Loss: 6.742505e-02\n",
            "Loss: 6.742378e-02\n",
            "Loss: 6.742288e-02\n",
            "Loss: 6.742203e-02\n",
            "Loss: 6.742025e-02\n",
            "Loss: 6.741932e-02\n",
            "Loss: 6.741846e-02\n",
            "Loss: 6.741785e-02\n",
            "Loss: 6.741666e-02\n",
            "Loss: 6.741589e-02\n",
            "Loss: 6.741504e-02\n",
            "Loss: 6.741429e-02\n",
            "Loss: 6.741320e-02\n",
            "Loss: 6.741168e-02\n",
            "Loss: 6.741229e-02\n",
            "Loss: 6.741055e-02\n",
            "Loss: 6.740966e-02\n",
            "Loss: 6.740785e-02\n",
            "Loss: 6.740681e-02\n",
            "Loss: 6.740563e-02\n",
            "Loss: 6.740466e-02\n",
            "Loss: 6.740383e-02\n",
            "Loss: 6.740201e-02\n",
            "Loss: 6.740074e-02\n",
            "Loss: 6.740052e-02\n",
            "Loss: 6.739936e-02\n",
            "Loss: 6.739923e-02\n",
            "Loss: 6.739926e-02\n",
            "Loss: 6.739920e-02\n",
            "Loss: 6.740029e-02\n",
            "Loss: 6.739900e-02\n",
            "Loss: 6.739817e-02\n",
            "Loss: 6.739815e-02\n",
            "Loss: 6.739774e-02\n",
            "Loss: 6.739695e-02\n",
            "Loss: 6.739640e-02\n",
            "Loss: 6.739666e-02\n",
            "Loss: 6.739573e-02\n",
            "Loss: 6.739501e-02\n",
            "Loss: 6.739442e-02\n",
            "Loss: 6.739445e-02\n",
            "Loss: 6.739464e-02\n",
            "Loss: 6.739457e-02\n",
            "Loss: 6.739442e-02\n",
            "Loss: 6.739433e-02\n",
            "Loss: 6.739433e-02\n",
            "Loss: 6.739444e-02\n",
            "Loss: 6.739433e-02\n",
            "Loss: 6.739433e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739435e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739433e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739433e-02\n",
            "Loss: 6.739432e-02\n",
            "Loss: 6.739412e-02\n",
            "Loss: 6.739339e-02\n",
            "Loss: 6.739271e-02\n",
            "Loss: 6.739240e-02\n",
            "Loss: 6.739154e-02\n",
            "Loss: 6.739061e-02\n",
            "Loss: 6.739142e-02\n",
            "Loss: 6.739074e-02\n",
            "Loss: 6.739094e-02\n",
            "Loss: 6.739110e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739068e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739070e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739061e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739061e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739058e-02\n",
            "Loss: 6.739146e-02\n",
            "Loss: 6.739056e-02\n",
            "Loss: 6.739016e-02\n",
            "Loss: 6.738976e-02\n",
            "Loss: 6.738929e-02\n",
            "Loss: 6.738822e-02\n",
            "Loss: 6.738695e-02\n",
            "Loss: 6.738614e-02\n",
            "Loss: 6.738542e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738515e-02\n",
            "Loss: 6.738514e-02\n",
            "Loss: 6.738504e-02\n",
            "Loss: 6.738493e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738493e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "Loss: 6.738491e-02\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
            "  Objective function value: 0.067385\n",
            "  Number of iterations: 6737\n",
            "  Number of functions evaluations: 7305\n",
            "Error u: 4.252112e-02\n",
            "Error u (idn): 3.321437e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6erME2d8M1Jx"
      },
      "source": [
        "## Plotting: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4dnerEv8ied"
      },
      "source": [
        "## PLOTTING: \n",
        "import matplotlib as mpl\n",
        "mpl.use('pgf')\n",
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
        "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
        "    \"text.usetex\": True,                # use LaTeX to write all text\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
        "    \"font.sans-serif\": [],\n",
        "    \"font.monospace\": [],\n",
        "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
        "    \"font.size\": 10,\n",
        "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
        "    \"pgf.preamble\": [\n",
        "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
        "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
        "        ]\n",
        "    }\n",
        "mpl.rcParams.update(pgf_with_latex)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots))\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def savefig(filename, crop = True):\n",
        "    if crop == True:\n",
        "        plt.savefig('{}.png'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "     #   plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "        plt.savefig('{}.png'.format(filename))\n",
        "        plt.savefig('{}.pdf'.format(filename))\n",
        "     #   plt.savefig('{}.eps'.format(filename))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OIA8gDKNMV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfe4f34-3452-487b-cd31-2d74f53102a8"
      },
      "source": [
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "rc('text', usetex=True)\n",
        "mpl.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n",
        "!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cm-super-minimal fonts-adf-accanthis fonts-adf-berenis fonts-adf-gillius\n",
            "  fonts-adf-universalis fonts-cabin fonts-comfortaa fonts-croscore\n",
            "  fonts-crosextra-caladea fonts-crosextra-carlito fonts-dejavu-core\n",
            "  fonts-dejavu-extra fonts-droid-fallback fonts-ebgaramond\n",
            "  fonts-ebgaramond-extra fonts-font-awesome fonts-freefont-otf\n",
            "  fonts-freefont-ttf fonts-gfs-artemisia fonts-gfs-complutum fonts-gfs-didot\n",
            "  fonts-gfs-neohellenic fonts-gfs-olga fonts-gfs-solomos fonts-go\n",
            "  fonts-junicode fonts-lato fonts-linuxlibertine fonts-lmodern fonts-lobster\n",
            "  fonts-lobstertwo fonts-noto-hinted fonts-noto-mono fonts-oflb-asana-math\n",
            "  fonts-open-sans fonts-roboto-hinted fonts-sil-gentium\n",
            "  fonts-sil-gentium-basic fonts-sil-gentiumplus fonts-sil-gentiumplus-compact\n",
            "  fonts-stix fonts-texgyre ghostscript gsfonts javascript-common\n",
            "  libcupsfilters1 libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0\n",
            "  libjs-jquery libkpathsea6 libpotrace0 libptexenc1 libruby2.5 libsynctex1\n",
            "  libtexlua52 libtexluajit2 libzzip-0-13 lmodern pfb2t1c2pfb poppler-data\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-fonts-extra-links texlive-latex-base\n",
            "  texlive-latex-extra texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa\n",
            "Suggested packages:\n",
            "  fonts-noto fontforge ghostscript-x apache2 | lighttpd | httpd poppler-utils\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic\n",
            "  | fonts-ipafont-gothic fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri\n",
            "  ruby-dev bundler debhelper perl-tk xpdf-reader | pdf-viewer\n",
            "  texlive-fonts-extra-doc texlive-fonts-recommended-doc texlive-latex-base-doc\n",
            "  python-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex ruby-tcltk\n",
            "  | libtcltk-ruby texlive-pictures-doc vprerex\n",
            "The following NEW packages will be installed:\n",
            "  cm-super cm-super-minimal dvipng fonts-adf-accanthis fonts-adf-berenis\n",
            "  fonts-adf-gillius fonts-adf-universalis fonts-cabin fonts-comfortaa\n",
            "  fonts-croscore fonts-crosextra-caladea fonts-crosextra-carlito\n",
            "  fonts-dejavu-core fonts-dejavu-extra fonts-droid-fallback fonts-ebgaramond\n",
            "  fonts-ebgaramond-extra fonts-font-awesome fonts-freefont-otf\n",
            "  fonts-freefont-ttf fonts-gfs-artemisia fonts-gfs-complutum fonts-gfs-didot\n",
            "  fonts-gfs-neohellenic fonts-gfs-olga fonts-gfs-solomos fonts-go\n",
            "  fonts-junicode fonts-lato fonts-linuxlibertine fonts-lmodern fonts-lobster\n",
            "  fonts-lobstertwo fonts-noto-hinted fonts-noto-mono fonts-oflb-asana-math\n",
            "  fonts-open-sans fonts-roboto-hinted fonts-sil-gentium\n",
            "  fonts-sil-gentium-basic fonts-sil-gentiumplus fonts-sil-gentiumplus-compact\n",
            "  fonts-stix fonts-texgyre ghostscript gsfonts javascript-common\n",
            "  libcupsfilters1 libcupsimage2 libgs9 libgs9-common libijs-0.35 libjbig2dec0\n",
            "  libjs-jquery libkpathsea6 libpotrace0 libptexenc1 libruby2.5 libsynctex1\n",
            "  libtexlua52 libtexluajit2 libzzip-0-13 lmodern pfb2t1c2pfb poppler-data\n",
            "  preview-latex-style rake ruby ruby-did-you-mean ruby-minitest\n",
            "  ruby-net-telnet ruby-power-assert ruby-test-unit ruby2.5\n",
            "  rubygems-integration t1utils tex-common tex-gyre texlive-base\n",
            "  texlive-binaries texlive-fonts-extra texlive-fonts-extra-links\n",
            "  texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic tipa\n",
            "0 upgraded, 89 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 554 MB of archives.\n",
            "After this operation, 1,550 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lato all 2.0-2 [2,698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 poppler-data all 0.4.8-2 [1,479 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 tex-common all 6.09 [33.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkpathsea6 amd64 2017.20170613.44572-8ubuntu0.1 [54.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libptexenc1 amd64 2017.20170613.44572-8ubuntu0.1 [34.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsynctex1 amd64 2017.20170613.44572-8ubuntu0.1 [41.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexlua52 amd64 2017.20170613.44572-8ubuntu0.1 [91.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtexluajit2 amd64 2017.20170613.44572-8ubuntu0.1 [230 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 t1utils amd64 1.41-2 [56.0 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsimage2 amd64 2.2.7-1ubuntu2.8 [18.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libijs-0.35 amd64 0.35-13 [15.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig2dec0 amd64 0.13-6 [55.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9-common all 9.26~dfsg+0-0ubuntu0.18.04.14 [5,092 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgs9 amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [2,265 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpotrace0 amd64 1.14-2 [17.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzzip-0-13 amd64 0.13.62-3.1ubuntu0.18.04.1 [26.0 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 texlive-binaries amd64 2017.20170613.44572-8ubuntu0.1 [8,179 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-base all 2017.20180305-1 [18.7 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-lmodern all 2.004.5-3 [4,551 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-base all 2017.20180305-1 [951 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 texlive-latex-recommended all 2017.20180305-1 [14.9 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super-minimal all 0.3.4-11 [5,810 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pfb2t1c2pfb amd64 0.3-11 [9,342 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cm-super all 0.3.4-11 [18.7 MB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ghostscript amd64 9.26~dfsg+0-0ubuntu0.18.04.14 [51.3 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dvipng amd64 1.15-1 [78.2 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-adf-accanthis all 0.20110505-1 [202 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-adf-berenis all 0.20110505-1 [281 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-adf-gillius all 0.20110505-1 [190 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-adf-universalis all 0.20110505-1 [111 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-cabin all 1.5-2 [140 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-comfortaa all 3.001-2 [129 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-croscore all 20171026-2 [2,135 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-crosextra-caladea all 20130214-2 [82.4 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-crosextra-carlito all 20130920-1 [742 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-ebgaramond all 0.016-1 [474 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-ebgaramond-extra all 0.016-1 [2,157 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-font-awesome all 4.7.0~dfsg-3 [513 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-freefont-otf all 20120503-7 [3,055 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-freefont-ttf all 20120503-7 [4,202 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-gfs-artemisia all 1.1-5 [260 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-gfs-complutum all 1.1-6 [41.6 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-gfs-didot all 1.1-6 [278 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-gfs-neohellenic all 1.1-6 [215 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-gfs-olga all 1.1-5 [33.4 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-gfs-solomos all 1.1-5 [40.7 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-go all 0~20161116-1 [348 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-junicode all 1.001-2 [684 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-linuxlibertine all 5.3.0-4 [1,627 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-lobster all 2.0-2 [38.7 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-lobstertwo all 2.0-2 [92.7 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-noto-hinted all 20171026-2 [6,653 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-noto-mono all 20171026-2 [75.5 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-oflb-asana-math all 000.907-6 [246 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-open-sans all 1.11-1 [575 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-roboto-hinted all 2:0~20160106-2 [2,918 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-sil-gentium all 20081126:1.03-2 [245 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-sil-gentium-basic all 1.102-1 [384 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-sil-gentiumplus all 5.000-2 [2,807 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-sil-gentiumplus-compact all 5.000-2 [1,514 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-texgyre all 20160520-1 [8,761 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic/main amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic/main amd64 javascript-common all 11 [6,066 B]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcupsfilters1 amd64 1.20.2-0ubuntu3.1 [108 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjs-jquery all 3.2.1-1 [152 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu bionic/main amd64 rubygems-integration all 1.11 [4,994 B]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ruby2.5 amd64 2.5.1-1ubuntu1.7 [48.6 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby amd64 1:2.5.1 [5,712 B]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 rake all 12.3.1-1ubuntu0.1 [44.9 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-did-you-mean all 1.2.0-2 [9,700 B]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-minitest all 5.10.3-1 [38.6 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-power-assert all 0.3.0-1 [7,952 B]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu bionic/main amd64 ruby-test-unit all 3.2.5-1 [61.1 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libruby2.5 amd64 2.5.1-1ubuntu1.7 [3,068 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu bionic/main amd64 lmodern all 2.004.5-3 [9,631 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu bionic/main amd64 preview-latex-style all 11.91-1ubuntu1 [185 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tex-gyre all 20160520-1 [4,998 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-extra all 2017.20180305-2 [354 MB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-stix all 1.1.1-4 [591 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-extra-links all 2017.20180305-2 [20.6 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-fonts-recommended all 2017.20180305-1 [5,262 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-pictures all 2017.20180305-1 [4,026 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-latex-extra all 2017.20180305-2 [10.6 MB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu bionic/universe amd64 texlive-plain-generic all 2017.20180305-2 [23.6 MB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tipa all 2:1.3-20 [2,978 kB]\n",
            "Fetched 554 MB in 31s (17.9 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../04-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../05-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../06-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../07-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../08-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../09-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../10-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../11-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../12-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../13-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../14-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../15-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../16-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../17-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../18-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../19-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../20-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../21-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package cm-super-minimal.\n",
            "Preparing to unpack .../22-cm-super-minimal_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super-minimal (0.3.4-11) ...\n",
            "Selecting previously unselected package pfb2t1c2pfb.\n",
            "Preparing to unpack .../23-pfb2t1c2pfb_0.3-11_amd64.deb ...\n",
            "Unpacking pfb2t1c2pfb (0.3-11) ...\n",
            "Selecting previously unselected package cm-super.\n",
            "Preparing to unpack .../24-cm-super_0.3.4-11_all.deb ...\n",
            "Unpacking cm-super (0.3.4-11) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../25-ghostscript_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking ghostscript (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package dvipng.\n",
            "Preparing to unpack .../26-dvipng_1.15-1_amd64.deb ...\n",
            "Unpacking dvipng (1.15-1) ...\n",
            "Selecting previously unselected package fonts-adf-accanthis.\n",
            "Preparing to unpack .../27-fonts-adf-accanthis_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-accanthis (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-adf-berenis.\n",
            "Preparing to unpack .../28-fonts-adf-berenis_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-berenis (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-adf-gillius.\n",
            "Preparing to unpack .../29-fonts-adf-gillius_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-gillius (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-adf-universalis.\n",
            "Preparing to unpack .../30-fonts-adf-universalis_0.20110505-1_all.deb ...\n",
            "Unpacking fonts-adf-universalis (0.20110505-1) ...\n",
            "Selecting previously unselected package fonts-cabin.\n",
            "Preparing to unpack .../31-fonts-cabin_1.5-2_all.deb ...\n",
            "Unpacking fonts-cabin (1.5-2) ...\n",
            "Selecting previously unselected package fonts-comfortaa.\n",
            "Preparing to unpack .../32-fonts-comfortaa_3.001-2_all.deb ...\n",
            "Unpacking fonts-comfortaa (3.001-2) ...\n",
            "Selecting previously unselected package fonts-croscore.\n",
            "Preparing to unpack .../33-fonts-croscore_20171026-2_all.deb ...\n",
            "Unpacking fonts-croscore (20171026-2) ...\n",
            "Selecting previously unselected package fonts-crosextra-caladea.\n",
            "Preparing to unpack .../34-fonts-crosextra-caladea_20130214-2_all.deb ...\n",
            "Unpacking fonts-crosextra-caladea (20130214-2) ...\n",
            "Selecting previously unselected package fonts-crosextra-carlito.\n",
            "Preparing to unpack .../35-fonts-crosextra-carlito_20130920-1_all.deb ...\n",
            "Unpacking fonts-crosextra-carlito (20130920-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../36-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../37-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package fonts-ebgaramond.\n",
            "Preparing to unpack .../38-fonts-ebgaramond_0.016-1_all.deb ...\n",
            "Unpacking fonts-ebgaramond (0.016-1) ...\n",
            "Selecting previously unselected package fonts-ebgaramond-extra.\n",
            "Preparing to unpack .../39-fonts-ebgaramond-extra_0.016-1_all.deb ...\n",
            "Unpacking fonts-ebgaramond-extra (0.016-1) ...\n",
            "Selecting previously unselected package fonts-font-awesome.\n",
            "Preparing to unpack .../40-fonts-font-awesome_4.7.0~dfsg-3_all.deb ...\n",
            "Unpacking fonts-font-awesome (4.7.0~dfsg-3) ...\n",
            "Selecting previously unselected package fonts-freefont-otf.\n",
            "Preparing to unpack .../41-fonts-freefont-otf_20120503-7_all.deb ...\n",
            "Unpacking fonts-freefont-otf (20120503-7) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../42-fonts-freefont-ttf_20120503-7_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-7) ...\n",
            "Selecting previously unselected package fonts-gfs-artemisia.\n",
            "Preparing to unpack .../43-fonts-gfs-artemisia_1.1-5_all.deb ...\n",
            "Unpacking fonts-gfs-artemisia (1.1-5) ...\n",
            "Selecting previously unselected package fonts-gfs-complutum.\n",
            "Preparing to unpack .../44-fonts-gfs-complutum_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-complutum (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-didot.\n",
            "Preparing to unpack .../45-fonts-gfs-didot_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-didot (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-neohellenic.\n",
            "Preparing to unpack .../46-fonts-gfs-neohellenic_1.1-6_all.deb ...\n",
            "Unpacking fonts-gfs-neohellenic (1.1-6) ...\n",
            "Selecting previously unselected package fonts-gfs-olga.\n",
            "Preparing to unpack .../47-fonts-gfs-olga_1.1-5_all.deb ...\n",
            "Unpacking fonts-gfs-olga (1.1-5) ...\n",
            "Selecting previously unselected package fonts-gfs-solomos.\n",
            "Preparing to unpack .../48-fonts-gfs-solomos_1.1-5_all.deb ...\n",
            "Unpacking fonts-gfs-solomos (1.1-5) ...\n",
            "Selecting previously unselected package fonts-go.\n",
            "Preparing to unpack .../49-fonts-go_0~20161116-1_all.deb ...\n",
            "Unpacking fonts-go (0~20161116-1) ...\n",
            "Selecting previously unselected package fonts-junicode.\n",
            "Preparing to unpack .../50-fonts-junicode_1.001-2_all.deb ...\n",
            "Unpacking fonts-junicode (1.001-2) ...\n",
            "Selecting previously unselected package fonts-linuxlibertine.\n",
            "Preparing to unpack .../51-fonts-linuxlibertine_5.3.0-4_all.deb ...\n",
            "Unpacking fonts-linuxlibertine (5.3.0-4) ...\n",
            "Selecting previously unselected package fonts-lobster.\n",
            "Preparing to unpack .../52-fonts-lobster_2.0-2_all.deb ...\n",
            "Unpacking fonts-lobster (2.0-2) ...\n",
            "Selecting previously unselected package fonts-lobstertwo.\n",
            "Preparing to unpack .../53-fonts-lobstertwo_2.0-2_all.deb ...\n",
            "Unpacking fonts-lobstertwo (2.0-2) ...\n",
            "Selecting previously unselected package fonts-noto-hinted.\n",
            "Preparing to unpack .../54-fonts-noto-hinted_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-hinted (20171026-2) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../55-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-oflb-asana-math.\n",
            "Preparing to unpack .../56-fonts-oflb-asana-math_000.907-6_all.deb ...\n",
            "Unpacking fonts-oflb-asana-math (000.907-6) ...\n",
            "Selecting previously unselected package fonts-open-sans.\n",
            "Preparing to unpack .../57-fonts-open-sans_1.11-1_all.deb ...\n",
            "Unpacking fonts-open-sans (1.11-1) ...\n",
            "Selecting previously unselected package fonts-roboto-hinted.\n",
            "Preparing to unpack .../58-fonts-roboto-hinted_2%3a0~20160106-2_all.deb ...\n",
            "Unpacking fonts-roboto-hinted (2:0~20160106-2) ...\n",
            "Selecting previously unselected package fonts-sil-gentium.\n",
            "Preparing to unpack .../59-fonts-sil-gentium_20081126%3a1.03-2_all.deb ...\n",
            "Unpacking fonts-sil-gentium (20081126:1.03-2) ...\n",
            "Selecting previously unselected package fonts-sil-gentium-basic.\n",
            "Preparing to unpack .../60-fonts-sil-gentium-basic_1.102-1_all.deb ...\n",
            "Unpacking fonts-sil-gentium-basic (1.102-1) ...\n",
            "Selecting previously unselected package fonts-sil-gentiumplus.\n",
            "Preparing to unpack .../61-fonts-sil-gentiumplus_5.000-2_all.deb ...\n",
            "Unpacking fonts-sil-gentiumplus (5.000-2) ...\n",
            "Selecting previously unselected package fonts-sil-gentiumplus-compact.\n",
            "Preparing to unpack .../62-fonts-sil-gentiumplus-compact_5.000-2_all.deb ...\n",
            "Unpacking fonts-sil-gentiumplus-compact (5.000-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../63-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../64-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../65-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../66-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../67-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../68-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../69-ruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.7) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../70-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../71-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../72-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../73-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../74-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../75-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../76-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../77-libruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../78-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../79-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../80-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-fonts-extra.\n",
            "Preparing to unpack .../81-texlive-fonts-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-fonts-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package fonts-stix.\n",
            "Preparing to unpack .../82-fonts-stix_1.1.1-4_all.deb ...\n",
            "Unpacking fonts-stix (1.1.1-4) ...\n",
            "Selecting previously unselected package texlive-fonts-extra-links.\n",
            "Preparing to unpack .../83-texlive-fonts-extra-links_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-fonts-extra-links (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../84-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../85-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../86-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../87-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../88-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-gfs-neohellenic (1.1-6) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up fonts-stix (1.1.1-4) ...\n",
            "Setting up fonts-comfortaa (3.001-2) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up fonts-linuxlibertine (5.3.0-4) ...\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up fonts-oflb-asana-math (000.907-6) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up fonts-lobster (2.0-2) ...\n",
            "Setting up fonts-gfs-solomos (1.1-5) ...\n",
            "Setting up fonts-adf-accanthis (0.20110505-1) ...\n",
            "Setting up fonts-freefont-otf (20120503-7) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-open-sans (1.11-1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-crosextra-carlito (20130920-1) ...\n",
            "Setting up pfb2t1c2pfb (0.3-11) ...\n",
            "Setting up fonts-ebgaramond-extra (0.016-1) ...\n",
            "Setting up fonts-font-awesome (4.7.0~dfsg-3) ...\n",
            "Setting up fonts-junicode (1.001-2) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up fonts-gfs-complutum (1.1-6) ...\n",
            "Setting up fonts-cabin (1.5-2) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up fonts-sil-gentiumplus-compact (5.000-2) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up fonts-adf-gillius (0.20110505-1) ...\n",
            "Setting up fonts-crosextra-caladea (20130214-2) ...\n",
            "Setting up fonts-noto-hinted (20171026-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up fonts-ebgaramond (0.016-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up fonts-croscore (20171026-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up fonts-adf-berenis (0.20110505-1) ...\n",
            "Setting up fonts-adf-universalis (0.20110505-1) ...\n",
            "Setting up fonts-sil-gentiumplus (5.000-2) ...\n",
            "Setting up fonts-gfs-didot (1.1-6) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up fonts-gfs-artemisia (1.1-5) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Setting up fonts-freefont-ttf (20120503-7) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up fonts-go (0~20161116-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up fonts-sil-gentium (20081126:1.03-2) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up fonts-sil-gentium-basic (1.102-1) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-gfs-olga (1.1-5) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up fonts-roboto-hinted (2:0~20160106-2) ...\n",
            "Setting up fonts-lobstertwo (2.0-2) ...\n",
            "Setting up ghostscript (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "Setting up texlive-fonts-extra-links (2017.20180305-2) ...\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-fonts-extra (2017.20180305-2) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up dvipng (1.15-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up cm-super-minimal (0.3.4-11) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up cm-super (0.3.4-11) ...\n",
            "Creating fonts. This may take some time... done.\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.7) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89lKvjja8ieu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "2af53c53-471d-4458-8b86-12e50b935245"
      },
      "source": [
        "  ######################################################################\n",
        "    ############################# Plotting ###############################\n",
        "    ######################################################################    \n",
        "    \n",
        "fig, ax = newfig(1.5, 1)\n",
        "ax.axis('off')\n",
        "    \n",
        "    ######## Row 2: Pressure #######################\n",
        "    ########      Predicted p(t,x,y)     ########### \n",
        "gs = gridspec.GridSpec(1, 2)\n",
        "gs.update(top=0.8, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n",
        "ax = plt.subplot(gs[:, 0])\n",
        "h = ax.imshow(Exact_sol, interpolation='nearest', cmap='jet', \n",
        "                  extent=[lb_sol[0], ub_sol[0], lb_sol[1], ub_sol[1]],\n",
        "                  origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "ax.set_title('Exact Dynamics', fontsize = 10)\n",
        "line = np.linspace(lb_sol[1], ub_sol[1], 2)[:,None]\n",
        "ax.plot(t_idn[index]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ########     Exact p(t,x,y)     ########### \n",
        "ax = plt.subplot(gs[:, 1])\n",
        "h = ax.imshow(U_pred, interpolation='nearest', cmap='jet', \n",
        "                 extent=[lb_sol[0], ub_sol[0], lb_sol[1], ub_sol[1]],origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "ax.set_title('Learned Dynamics', fontsize = 10)\n",
        "line = np.linspace(lb_sol[1], ub_sol[1], 2)[:,None]\n",
        "ax.plot(t_idn[index]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "savefig('/content/figures/Burgers_Sine')\n",
        "# savefig('/content/figures/Burgers_Exp')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 582.814x360.199 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}