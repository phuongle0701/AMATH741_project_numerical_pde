{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit (conda)",
      "metadata": {
        "interpreter": {
          "hash": "51c008122401afe5535fd6c3a84bd5dd8b8bd079df3baf2c4d203d1ed6e4d4a5"
        }
      }
    },
    "colab": {
      "name": "kdV_Simulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWFV7xXykthN"
      },
      "source": [
        "# kdV Equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAEDmqxBkthQ",
        "outputId": "38e8822f-3d00-4850-b611-5af2ad5bdb62"
      },
      "source": [
        "!pip install pyDOE"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyDOE) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgZ0Td7ZkthR",
        "outputId": "f40c228f-b67e-4637-fdd8-ae803874b1b9"
      },
      "source": [
        "%tensorflow_version 1.x ### to run on Google Colab: \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x ### to run on Google Colab:`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NNAbFw9kthR",
        "outputId": "df34fa3b-caaf-4ea2-d601-b6408c5cd46b"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOIRwz2AmXrR"
      },
      "source": [
        "###############################################################################\n",
        "############################## Helper Functions ###############################\n",
        "###############################################################################\n",
        "\n",
        "def initialize_NN(layers):\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers) \n",
        "    for l in range(0,num_layers-1):\n",
        "        W = xavier_init(size=[layers[l], layers[l+1]])\n",
        "        b = tf.Variable(tf.zeros([1,layers[l+1]]), dtype=tf.float32)\n",
        "        weights.append(W)\n",
        "        biases.append(b)        \n",
        "    return weights, biases\n",
        "    \n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    out_dim = size[1]        \n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "def neural_net(X, weights, biases):\n",
        "    num_layers = len(weights) + 1\n",
        "    H = X\n",
        "    for l in range(0,num_layers-2):\n",
        "        W = weights[l]\n",
        "        b = biases[l]\n",
        "        H = tf.sin(tf.add(tf.matmul(H, W), b))\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    Y = tf.add(tf.matmul(H, W), b)\n",
        "    return Y\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "################################ DeepHPM Class ################################\n",
        "###############################################################################\n",
        "\n",
        "class DeepHPM:\n",
        "    def __init__(self, t, x, u,\n",
        "                       x0, u0, tb, X_f,\n",
        "                       u_layers, pde_layers,\n",
        "                       layers,\n",
        "                       lb_idn, ub_idn,\n",
        "                       lb_sol, ub_sol):\n",
        "        \n",
        "        # Domain Boundary\n",
        "        self.lb_idn = lb_idn\n",
        "        self.ub_idn = ub_idn\n",
        "        \n",
        "        self.lb_sol = lb_sol\n",
        "        self.ub_sol = ub_sol\n",
        "        \n",
        "        # Init for Identification\n",
        "        self.idn_init(t, x, u, u_layers, pde_layers)\n",
        "        \n",
        "        # Init for Solution\n",
        "        self.sol_init(x0, u0, tb, X_f, layers)\n",
        "            \n",
        "        # tf session\n",
        "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                                     log_device_placement=True))\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "        \n",
        "    ###########################################################################\n",
        "    ############################# Identifier ##################################\n",
        "    ###########################################################################\n",
        "        \n",
        "    def idn_init(self, t, x, u, u_layers, pde_layers):\n",
        "        # Training Data for Identification\n",
        "        self.t = t\n",
        "        self.x = x\n",
        "        self.u = u\n",
        "        \n",
        "        # Layers for Identification\n",
        "        self.u_layers = u_layers\n",
        "        self.pde_layers = pde_layers\n",
        "        \n",
        "        # Initialize NNs for Identification\n",
        "        self.u_weights, self.u_biases = initialize_NN(u_layers)\n",
        "        self.pde_weights, self.pde_biases = initialize_NN(pde_layers)\n",
        "        \n",
        "        # tf placeholders for Identification\n",
        "        self.t_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.terms_tf = tf.placeholder(tf.float32, shape=[None, pde_layers[0]])\n",
        "        \n",
        "        # tf graphs for Identification\n",
        "        self.idn_u_pred = self.idn_net_u(self.t_tf, self.x_tf)\n",
        "        self.pde_pred = self.net_pde(self.terms_tf)\n",
        "        self.idn_f_pred = self.idn_net_f(self.t_tf, self.x_tf)\n",
        "        \n",
        "        # loss for Identification\n",
        "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) ## Regularization\n",
        "        reg_constant = 0.001  # Choose an appropriate one.\n",
        "        self.idn_u_loss = tf.reduce_sum(tf.square(self.idn_u_pred - self.u_tf)) + reg_constant*sum(reg_losses)\n",
        "        self.idn_f_loss = tf.reduce_sum(tf.square(self.idn_f_pred)) + reg_constant*sum(reg_losses)\n",
        "                        \n",
        "        # Optimizer for Identification\n",
        "        self.idn_u_optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.idn_u_loss,\n",
        "                               var_list = self.u_weights + self.u_biases,\n",
        "                               method = 'L-BFGS-B',\n",
        "                               options = {'maxiter': 50000,\n",
        "                                          'maxfun': 50000,\n",
        "                                          'maxcor': 50,\n",
        "                                          'maxls': 50,\n",
        "                                          'ftol': 1.0*np.finfo(float).eps})\n",
        "    \n",
        "        self.idn_f_optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.idn_f_loss,\n",
        "                               var_list = self.pde_weights + self.pde_biases,\n",
        "                               method = 'L-BFGS-B',\n",
        "                               options = {'maxiter': 50000,\n",
        "                                          'maxfun': 50000,\n",
        "                                          'maxcor': 50,\n",
        "                                          'maxls': 50,\n",
        "                                          'ftol': 1.0*np.finfo(float).eps})\n",
        "    \n",
        "        self.idn_u_optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.idn_u_train_op_Adam = self.idn_u_optimizer_Adam.minimize(self.idn_u_loss, \n",
        "                                   var_list = self.u_weights + self.u_biases)\n",
        "    \n",
        "        self.idn_f_optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.idn_f_train_op_Adam = self.idn_f_optimizer_Adam.minimize(self.idn_f_loss, \n",
        "                                   var_list = self.pde_weights + self.pde_biases)\n",
        "    \n",
        "    def idn_net_u(self, t, x):\n",
        "        X = tf.concat([t,x],1)\n",
        "        H = 2.0*(X - self.lb_idn)/(self.ub_idn - self.lb_idn) - 1.0\n",
        "        u = neural_net(H, self.u_weights, self.u_biases)\n",
        "        return u\n",
        "    \n",
        "    def net_pde(self, terms):\n",
        "        pde = neural_net(terms, self.pde_weights, self.pde_biases)\n",
        "        return pde\n",
        "    \n",
        "    def idn_net_f(self, t, x):\n",
        "        u = self.idn_net_u(t, x)\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        \n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        u_xxx = tf.gradients(u_xx, x)[0]\n",
        "        \n",
        "        terms = tf.concat([u,u_x,u_xx,u_xxx],1)\n",
        "        \n",
        "        f = u_t - self.net_pde(terms)\n",
        "        \n",
        "        return f\n",
        "\n",
        "    def idn_u_train(self, N_iter):\n",
        "        tf_dict = {self.t_tf: self.t, self.x_tf: self.x, self.u_tf: self.u}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(N_iter):\n",
        "            \n",
        "            self.sess.run(self.idn_u_train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.idn_u_loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.idn_u_optimizer.minimize(self.sess,\n",
        "                                      feed_dict = tf_dict,\n",
        "                                      fetches = [self.idn_u_loss],\n",
        "                                      loss_callback = self.callback)\n",
        "        \n",
        "    def idn_f_train(self, N_iter):\n",
        "        tf_dict = {self.t_tf: self.t, self.x_tf: self.x}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(N_iter):\n",
        "            \n",
        "            self.sess.run(self.idn_f_train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.idn_f_loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "        \n",
        "        self.idn_f_optimizer.minimize(self.sess,\n",
        "                                      feed_dict = tf_dict,\n",
        "                                      fetches = [self.idn_f_loss],\n",
        "                                      loss_callback = self.callback)\n",
        "\n",
        "    def idn_predict(self, t_star, x_star):\n",
        "        \n",
        "        tf_dict = {self.t_tf: t_star, self.x_tf: x_star}\n",
        "        \n",
        "        u_star = self.sess.run(self.idn_u_pred, tf_dict)\n",
        "        f_star = self.sess.run(self.idn_f_pred, tf_dict)\n",
        "        \n",
        "        return u_star, f_star\n",
        "    \n",
        "    def predict_pde(self, terms_star):\n",
        "        \n",
        "        tf_dict = {self.terms_tf: terms_star}\n",
        "        \n",
        "        pde_star = self.sess.run(self.pde_pred, tf_dict)\n",
        "        \n",
        "        return pde_star\n",
        "    \n",
        "    ###########################################################################\n",
        "    ############################### Solver ####################################\n",
        "    ###########################################################################\n",
        "    \n",
        "    def sol_init(self, x0, u0, tb, X_f, layers):\n",
        "        # Training Data for Solution\n",
        "        X0 = np.concatenate((0*x0, x0), 1) # (0, x0)\n",
        "        X_lb = np.concatenate((tb, 0*tb + self.lb_sol[1]), 1) # (tb, lb[1])\n",
        "        X_ub = np.concatenate((tb, 0*tb + self.ub_sol[1]), 1) # (tb, ub[1])\n",
        "        \n",
        "        self.X_f = X_f # Collocation Points\n",
        "        self.t0 = X0[:,0:1] # Initial Data (time)\n",
        "        self.x0 = X0[:,1:2] # Initial Data (space)\n",
        "        self.t_lb = X_lb[:,0:1] # Boundary Data (time) -- lower boundary\n",
        "        self.x_lb = X_lb[:,1:2] # Boundary Data (space) -- lower boundary\n",
        "        self.t_ub = X_ub[:,0:1] # Boundary Data (time) -- upper boundary\n",
        "        self.x_ub = X_ub[:,1:2] # Boundary Data (space) -- upper boundary\n",
        "        self.t_f = X_f[:,0:1] # Collocation Points (time)\n",
        "        self.x_f = X_f[:,1:2] # Collocation Points (space)\n",
        "        self.u0 = u0 # Boundary Data\n",
        "        \n",
        "        # Layers for Solution\n",
        "        self.layers = layers\n",
        "        \n",
        "        # Initialize NNs for Solution\n",
        "        self.weights, self.biases = initialize_NN(layers)\n",
        "        \n",
        "        # tf placeholders for Solution\n",
        "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        \n",
        "        # tf graphs for Solution\n",
        "        self.u0_pred, _, _  = self.sol_net_u(self.t0_tf, self.x0_tf)\n",
        "        self.u_lb_pred, self.u_x_lb_pred, self.u_xx_lb_pred = self.sol_net_u(self.t_lb_tf, self.x_lb_tf)\n",
        "        self.u_ub_pred, self.u_x_ub_pred, self.u_xx_ub_pred = self.sol_net_u(self.t_ub_tf, self.x_ub_tf)\n",
        "        self.sol_f_pred = self.sol_net_f(self.t_f_tf, self.x_f_tf)\n",
        "        \n",
        "        # loss for Solution\n",
        "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) ## REGULARIZATION L1\n",
        "        reg_constant = 0.001  # Choose an appropriate one.\n",
        "        self.sol_loss = tf.reduce_sum(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.u_xx_lb_pred - self.u_xx_ub_pred)) + \\\n",
        "                        tf.reduce_sum(tf.square(self.sol_f_pred)) + reg_constant*sum(reg_losses)\n",
        "        \n",
        "        # Optimizer for Solution\n",
        "        self.sol_optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.sol_loss,\n",
        "                             var_list = self.weights + self.biases,\n",
        "                             method = 'L-BFGS-B',\n",
        "                             options = {'maxiter': 50000,\n",
        "                                        'maxfun': 50000,\n",
        "                                        'maxcor': 50,\n",
        "                                        'maxls': 50,\n",
        "                                        'ftol': 1.0*np.finfo(float).eps})\n",
        "    \n",
        "        self.sol_optimizer_Adam = tf.train.AdamOptimizer()\n",
        "        self.sol_train_op_Adam = self.sol_optimizer_Adam.minimize(self.sol_loss,\n",
        "                                 var_list = self.weights + self.biases)\n",
        "    \n",
        "    def sol_net_u(self, t, x):\n",
        "        X = tf.concat([t,x],1)\n",
        "        H = 2.0*(X - self.lb_sol)/(self.ub_sol - self.lb_sol) - 1.0\n",
        "        u = neural_net(H, self.weights, self.biases)\n",
        "        u_x = tf.gradients(u, x)[0]\n",
        "        u_xx = tf.gradients(u_x, x)[0]\n",
        "        return u, u_x, u_xx\n",
        "    \n",
        "    def sol_net_f(self, t, x):\n",
        "        u, u_x, u_xx = self.sol_net_u(t,x)\n",
        "        \n",
        "        u_t = tf.gradients(u, t)[0]\n",
        "        \n",
        "        u_xxx = tf.gradients(u_xx, x)[0]\n",
        "        \n",
        "        terms = tf.concat([u,u_x,u_xx,u_xxx],1)\n",
        "        \n",
        "        f = u_t - self.net_pde(terms)\n",
        "        \n",
        "        return f\n",
        "    \n",
        "    def callback(self, loss):\n",
        "        print('Loss: %e' % (loss))\n",
        "        \n",
        "    def sol_train(self, N_iter):        \n",
        "        tf_dict = {self.t0_tf: self.t0, self.x0_tf: self.x0,\n",
        "                   self.u0_tf: self.u0,\n",
        "                   self.t_lb_tf: self.t_lb, self.x_lb_tf: self.x_lb,\n",
        "                   self.t_ub_tf: self.t_ub, self.x_ub_tf: self.x_ub,\n",
        "                   self.t_f_tf: self.t_f, self.x_f_tf: self.x_f}\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for it in range(N_iter):\n",
        "            \n",
        "            self.sess.run(self.sol_train_op_Adam, tf_dict)\n",
        "            \n",
        "            # Print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                loss_value = self.sess.run(self.sol_loss, tf_dict)\n",
        "                print('It: %d, Loss: %.3e, Time: %.2f' % \n",
        "                      (it, loss_value, elapsed))\n",
        "                start_time = time.time()\n",
        "                \n",
        "        self.sol_optimizer.minimize(self.sess, \n",
        "                                    feed_dict = tf_dict,         \n",
        "                                    fetches = [self.sol_loss], \n",
        "                                    loss_callback = self.callback)\n",
        "    \n",
        "    def sol_predict(self, t_star, x_star):\n",
        "        \n",
        "        u_star = self.sess.run(self.u0_pred, {self.t0_tf: t_star, self.x0_tf: x_star})  \n",
        "        f_star = self.sess.run(self.sol_f_pred, {self.t_f_tf: t_star, self.x_f_tf: x_star})\n",
        "               \n",
        "        return u_star, f_star  "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD4oJO6EkthU"
      },
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1tvYzPzkthV"
      },
      "source": [
        "###############################################################################\n",
        "################################ Main Function ################################\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "    # Doman bounds\n",
        "lb_idn = np.array([0.0, -8.0])\n",
        "ub_idn = np.array([5.0, 8.0])\n",
        "lb_sol = np.array([0.0, -8.0])\n",
        "ub_sol = np.array([5.0, 8.0])\n",
        "    \n",
        "    ### Load Data ###\n",
        "    \n",
        "#data_idn = scipy.io.loadmat('/content/Data/kdV_sine.mat') \n",
        "data_idn = scipy.io.loadmat('/content/Data/kdV_cosine.mat')   \n",
        "\n",
        "t_idn = data_idn['t'].flatten()[:,None]\n",
        "x_idn = data_idn['x'].flatten()[:,None]\n",
        "Exact_idn = np.real(data_idn['usol'])    \n",
        "T_idn, X_idn = np.meshgrid(t_idn,x_idn)\n",
        "    \n",
        "keep = 4/5\n",
        "index = int(keep*t_idn.shape[0])\n",
        "T_idn = T_idn[:,0:index]\n",
        "X_idn = X_idn[:,0:index]\n",
        "Exact_idn = Exact_idn[:,0:index]\n",
        "    \n",
        "t_idn_star = T_idn.flatten()[:,None]\n",
        "x_idn_star = X_idn.flatten()[:,None]\n",
        "X_idn_star = np.hstack((t_idn_star, x_idn_star))\n",
        "u_idn_star = Exact_idn.flatten()[:,None]\n",
        "    \n",
        "    #\n",
        "    \n",
        "#data_sol = scipy.io.loadmat('/content/Data/kdV_sine.mat')\n",
        "data_sol = scipy.io.loadmat('/content/Data/kdV_cosine.mat')\n",
        "\n",
        "t_sol = data_sol['t'].flatten()[:,None]\n",
        "x_sol = data_sol['x'].flatten()[:,None]\n",
        "Exact_sol = np.real(data_sol['usol'])\n",
        "    \n",
        "T_sol, X_sol = np.meshgrid(t_sol,x_sol)\n",
        "    \n",
        "t_sol_star = T_sol.flatten()[:,None]\n",
        "x_sol_star = X_sol.flatten()[:,None]\n",
        "X_sol_star = np.hstack((t_sol_star, x_sol_star))\n",
        "u_sol_star = Exact_sol.flatten()[:,None]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yos7rU0umyBo",
        "outputId": "8df73d6e-ec26-4d40-aad8-d5f0cf9b2437"
      },
      "source": [
        "   ### Training Data ###\n",
        "    \n",
        "    # For identification\n",
        "N_train = 10000\n",
        "    \n",
        "idx = np.random.choice(t_idn_star.shape[0], N_train, replace=False)    \n",
        "t_train = t_idn_star[idx,:]\n",
        "x_train = x_idn_star[idx,:]\n",
        "u_train = u_idn_star[idx,:]\n",
        "    \n",
        "noise = 0.00 # adjust this for different purposes. \n",
        "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "    \n",
        "    # For solution\n",
        "N0 = Exact_sol.shape[0]\n",
        "N_b = Exact_sol.shape[1]\n",
        "N_f = 20000\n",
        "        \n",
        "idx_x = np.random.choice(x_sol.shape[0], N0, replace=False)\n",
        "x0_train = x_sol[idx_x,:]\n",
        "u0_train = Exact_sol[idx_x,0:1]\n",
        "    \n",
        "idx_t = np.random.choice(t_sol.shape[0], N_b, replace=False)\n",
        "tb_train = t_sol[idx_t,:]\n",
        "    \n",
        "X_f_train = lb_sol + (ub_sol-lb_sol)*lhs(2, N_f)\n",
        "        \n",
        "    # Layers\n",
        "u_layers = [2, 50, 50, 50, 50, 1]\n",
        "pde_layers = [4, 100, 100, 1]    \n",
        "layers = [2, 50, 50, 50, 50, 1]\n",
        "    \n",
        "    # Model\n",
        "model = DeepHPM(t_train, x_train, u_train,x0_train, u0_train, tb_train, X_f_train,u_layers, pde_layers,\n",
        "                    layers,lb_idn, ub_idn,\n",
        "                    lb_sol,ub_sol)\n",
        "    # Train the identifier\n",
        "model.idn_u_train(N_iter=0)\n",
        "model.idn_f_train(N_iter=0)\n",
        "u_pred_identifier, f_pred_identifier = model.idn_predict(t_idn_star, x_idn_star)\n",
        "error_u_identifier = np.linalg.norm(u_idn_star-u_pred_identifier,2)/np.linalg.norm(u_idn_star,2)\n",
        "print('Error u: %e' % (error_u_identifier))\n",
        "    \n",
        "    ### Solution ###\n",
        "    \n",
        "    # Train the solver\n",
        "model.sol_train(N_iter=0)        \n",
        "u_pred, f_pred = model.sol_predict(t_sol_star, x_sol_star)\n",
        "    \n",
        "u_pred_idn, f_pred_idn = model.sol_predict(t_idn_star, x_idn_star)\n",
        "    \n",
        "error_u = np.linalg.norm(u_sol_star-u_pred,2)/np.linalg.norm(u_sol_star,2)\n",
        "error_u_idn = np.linalg.norm(u_idn_star-u_pred_idn,2)/np.linalg.norm(u_idn_star,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error u (idn): %e' % (error_u_idn))\n",
        "U_pred = griddata(X_sol_star, u_pred.flatten(), (T_sol, X_sol), method='cubic')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss: 1.780142e-01\n",
            "Loss: 1.779714e-01\n",
            "Loss: 1.779356e-01\n",
            "Loss: 1.778114e-01\n",
            "Loss: 1.777387e-01\n",
            "Loss: 1.776541e-01\n",
            "Loss: 1.775860e-01\n",
            "Loss: 1.775250e-01\n",
            "Loss: 1.774778e-01\n",
            "Loss: 1.774421e-01\n",
            "Loss: 1.773985e-01\n",
            "Loss: 1.773528e-01\n",
            "Loss: 1.772850e-01\n",
            "Loss: 1.772018e-01\n",
            "Loss: 1.771209e-01\n",
            "Loss: 1.770461e-01\n",
            "Loss: 1.769670e-01\n",
            "Loss: 1.768439e-01\n",
            "Loss: 1.767753e-01\n",
            "Loss: 1.767521e-01\n",
            "Loss: 1.767003e-01\n",
            "Loss: 1.766644e-01\n",
            "Loss: 1.766331e-01\n",
            "Loss: 1.766103e-01\n",
            "Loss: 1.765843e-01\n",
            "Loss: 1.765525e-01\n",
            "Loss: 1.766225e-01\n",
            "Loss: 1.765299e-01\n",
            "Loss: 1.764684e-01\n",
            "Loss: 1.763512e-01\n",
            "Loss: 1.766935e-01\n",
            "Loss: 1.763095e-01\n",
            "Loss: 1.761978e-01\n",
            "Loss: 1.760936e-01\n",
            "Loss: 1.759845e-01\n",
            "Loss: 1.758280e-01\n",
            "Loss: 1.756742e-01\n",
            "Loss: 1.757963e-01\n",
            "Loss: 1.756144e-01\n",
            "Loss: 1.755496e-01\n",
            "Loss: 1.755033e-01\n",
            "Loss: 1.754727e-01\n",
            "Loss: 1.754372e-01\n",
            "Loss: 1.753540e-01\n",
            "Loss: 1.752573e-01\n",
            "Loss: 1.751686e-01\n",
            "Loss: 1.750441e-01\n",
            "Loss: 1.749441e-01\n",
            "Loss: 1.748424e-01\n",
            "Loss: 1.747293e-01\n",
            "Loss: 1.745841e-01\n",
            "Loss: 1.744558e-01\n",
            "Loss: 1.743919e-01\n",
            "Loss: 1.743352e-01\n",
            "Loss: 1.742640e-01\n",
            "Loss: 1.741260e-01\n",
            "Loss: 1.739660e-01\n",
            "Loss: 1.738400e-01\n",
            "Loss: 1.738459e-01\n",
            "Loss: 1.737600e-01\n",
            "Loss: 1.736080e-01\n",
            "Loss: 1.734138e-01\n",
            "Loss: 1.735142e-01\n",
            "Loss: 1.733177e-01\n",
            "Loss: 1.733881e-01\n",
            "Loss: 1.732413e-01\n",
            "Loss: 1.731221e-01\n",
            "Loss: 1.730153e-01\n",
            "Loss: 1.728436e-01\n",
            "Loss: 1.726442e-01\n",
            "Loss: 1.725127e-01\n",
            "Loss: 1.723584e-01\n",
            "Loss: 1.723460e-01\n",
            "Loss: 1.722629e-01\n",
            "Loss: 1.722081e-01\n",
            "Loss: 1.721535e-01\n",
            "Loss: 1.720647e-01\n",
            "Loss: 1.719736e-01\n",
            "Loss: 1.718934e-01\n",
            "Loss: 1.718035e-01\n",
            "Loss: 1.716295e-01\n",
            "Loss: 1.714331e-01\n",
            "Loss: 1.711948e-01\n",
            "Loss: 1.710937e-01\n",
            "Loss: 1.709690e-01\n",
            "Loss: 1.708607e-01\n",
            "Loss: 1.707372e-01\n",
            "Loss: 1.705713e-01\n",
            "Loss: 1.704672e-01\n",
            "Loss: 1.703556e-01\n",
            "Loss: 1.702788e-01\n",
            "Loss: 1.702238e-01\n",
            "Loss: 1.701595e-01\n",
            "Loss: 1.700745e-01\n",
            "Loss: 1.699718e-01\n",
            "Loss: 1.699012e-01\n",
            "Loss: 1.698257e-01\n",
            "Loss: 1.697542e-01\n",
            "Loss: 1.696451e-01\n",
            "Loss: 1.695395e-01\n",
            "Loss: 1.694598e-01\n",
            "Loss: 1.693740e-01\n",
            "Loss: 1.692843e-01\n",
            "Loss: 1.691828e-01\n",
            "Loss: 1.690679e-01\n",
            "Loss: 1.689723e-01\n",
            "Loss: 1.689068e-01\n",
            "Loss: 1.689198e-01\n",
            "Loss: 1.688276e-01\n",
            "Loss: 1.687599e-01\n",
            "Loss: 1.687018e-01\n",
            "Loss: 1.686363e-01\n",
            "Loss: 1.685717e-01\n",
            "Loss: 1.684902e-01\n",
            "Loss: 1.683483e-01\n",
            "Loss: 1.681833e-01\n",
            "Loss: 1.680122e-01\n",
            "Loss: 1.678533e-01\n",
            "Loss: 1.677147e-01\n",
            "Loss: 1.675520e-01\n",
            "Loss: 1.673563e-01\n",
            "Loss: 1.671651e-01\n",
            "Loss: 1.670050e-01\n",
            "Loss: 1.668882e-01\n",
            "Loss: 1.667858e-01\n",
            "Loss: 1.665475e-01\n",
            "Loss: 1.664679e-01\n",
            "Loss: 1.660860e-01\n",
            "Loss: 1.659267e-01\n",
            "Loss: 1.657969e-01\n",
            "Loss: 1.657062e-01\n",
            "Loss: 1.656223e-01\n",
            "Loss: 1.654768e-01\n",
            "Loss: 1.653389e-01\n",
            "Loss: 1.652347e-01\n",
            "Loss: 1.650411e-01\n",
            "Loss: 1.647959e-01\n",
            "Loss: 1.646077e-01\n",
            "Loss: 1.646750e-01\n",
            "Loss: 1.644421e-01\n",
            "Loss: 1.641338e-01\n",
            "Loss: 1.639104e-01\n",
            "Loss: 1.637179e-01\n",
            "Loss: 1.636016e-01\n",
            "Loss: 1.637655e-01\n",
            "Loss: 1.635315e-01\n",
            "Loss: 1.634335e-01\n",
            "Loss: 1.633958e-01\n",
            "Loss: 1.633520e-01\n",
            "Loss: 1.632877e-01\n",
            "Loss: 1.632165e-01\n",
            "Loss: 1.631167e-01\n",
            "Loss: 1.629402e-01\n",
            "Loss: 1.627365e-01\n",
            "Loss: 1.624679e-01\n",
            "Loss: 1.622029e-01\n",
            "Loss: 1.622337e-01\n",
            "Loss: 1.621269e-01\n",
            "Loss: 1.620376e-01\n",
            "Loss: 1.619738e-01\n",
            "Loss: 1.618930e-01\n",
            "Loss: 1.617887e-01\n",
            "Loss: 1.616049e-01\n",
            "Loss: 1.614853e-01\n",
            "Loss: 1.612502e-01\n",
            "Loss: 1.610855e-01\n",
            "Loss: 1.609306e-01\n",
            "Loss: 1.607866e-01\n",
            "Loss: 1.606714e-01\n",
            "Loss: 1.605270e-01\n",
            "Loss: 1.603623e-01\n",
            "Loss: 1.602241e-01\n",
            "Loss: 1.601206e-01\n",
            "Loss: 1.600186e-01\n",
            "Loss: 1.598785e-01\n",
            "Loss: 1.596310e-01\n",
            "Loss: 1.594465e-01\n",
            "Loss: 1.592611e-01\n",
            "Loss: 1.591019e-01\n",
            "Loss: 1.589972e-01\n",
            "Loss: 1.589157e-01\n",
            "Loss: 1.588085e-01\n",
            "Loss: 1.586845e-01\n",
            "Loss: 1.585376e-01\n",
            "Loss: 1.584197e-01\n",
            "Loss: 1.583106e-01\n",
            "Loss: 1.581938e-01\n",
            "Loss: 1.579579e-01\n",
            "Loss: 1.577684e-01\n",
            "Loss: 1.576054e-01\n",
            "Loss: 1.575048e-01\n",
            "Loss: 1.574505e-01\n",
            "Loss: 1.574145e-01\n",
            "Loss: 1.573802e-01\n",
            "Loss: 1.573502e-01\n",
            "Loss: 1.572794e-01\n",
            "Loss: 1.571825e-01\n",
            "Loss: 1.574040e-01\n",
            "Loss: 1.571361e-01\n",
            "Loss: 1.570652e-01\n",
            "Loss: 1.570275e-01\n",
            "Loss: 1.569809e-01\n",
            "Loss: 1.569164e-01\n",
            "Loss: 1.568504e-01\n",
            "Loss: 1.568179e-01\n",
            "Loss: 1.567373e-01\n",
            "Loss: 1.566675e-01\n",
            "Loss: 1.565706e-01\n",
            "Loss: 1.564059e-01\n",
            "Loss: 1.563111e-01\n",
            "Loss: 1.561748e-01\n",
            "Loss: 1.561223e-01\n",
            "Loss: 1.560784e-01\n",
            "Loss: 1.560091e-01\n",
            "Loss: 1.559005e-01\n",
            "Loss: 1.557970e-01\n",
            "Loss: 1.556311e-01\n",
            "Loss: 1.554748e-01\n",
            "Loss: 1.553517e-01\n",
            "Loss: 1.553272e-01\n",
            "Loss: 1.550520e-01\n",
            "Loss: 1.549282e-01\n",
            "Loss: 1.548173e-01\n",
            "Loss: 1.547598e-01\n",
            "Loss: 1.547269e-01\n",
            "Loss: 1.546928e-01\n",
            "Loss: 1.546402e-01\n",
            "Loss: 1.545935e-01\n",
            "Loss: 1.545184e-01\n",
            "Loss: 1.544459e-01\n",
            "Loss: 1.543615e-01\n",
            "Loss: 1.542848e-01\n",
            "Loss: 1.542164e-01\n",
            "Loss: 1.541069e-01\n",
            "Loss: 1.539101e-01\n",
            "Loss: 1.536954e-01\n",
            "Loss: 1.534950e-01\n",
            "Loss: 1.532816e-01\n",
            "Loss: 1.531753e-01\n",
            "Loss: 1.530880e-01\n",
            "Loss: 1.529632e-01\n",
            "Loss: 1.531953e-01\n",
            "Loss: 1.529188e-01\n",
            "Loss: 1.528465e-01\n",
            "Loss: 1.527022e-01\n",
            "Loss: 1.526269e-01\n",
            "Loss: 1.525471e-01\n",
            "Loss: 1.524856e-01\n",
            "Loss: 1.524242e-01\n",
            "Loss: 1.523547e-01\n",
            "Loss: 1.522956e-01\n",
            "Loss: 1.523243e-01\n",
            "Loss: 1.522681e-01\n",
            "Loss: 1.522187e-01\n",
            "Loss: 1.521255e-01\n",
            "Loss: 1.520513e-01\n",
            "Loss: 1.518968e-01\n",
            "Loss: 1.517455e-01\n",
            "Loss: 1.517201e-01\n",
            "Loss: 1.515866e-01\n",
            "Loss: 1.515492e-01\n",
            "Loss: 1.514931e-01\n",
            "Loss: 1.514234e-01\n",
            "Loss: 1.516465e-01\n",
            "Loss: 1.513972e-01\n",
            "Loss: 1.513202e-01\n",
            "Loss: 1.512747e-01\n",
            "Loss: 1.512260e-01\n",
            "Loss: 1.511626e-01\n",
            "Loss: 1.511232e-01\n",
            "Loss: 1.510372e-01\n",
            "Loss: 1.509847e-01\n",
            "Loss: 1.509231e-01\n",
            "Loss: 1.508799e-01\n",
            "Loss: 1.508011e-01\n",
            "Loss: 1.507569e-01\n",
            "Loss: 1.506481e-01\n",
            "Loss: 1.505765e-01\n",
            "Loss: 1.505009e-01\n",
            "Loss: 1.504233e-01\n",
            "Loss: 1.503567e-01\n",
            "Loss: 1.503122e-01\n",
            "Loss: 1.502473e-01\n",
            "Loss: 1.501811e-01\n",
            "Loss: 1.500943e-01\n",
            "Loss: 1.500702e-01\n",
            "Loss: 1.500398e-01\n",
            "Loss: 1.500236e-01\n",
            "Loss: 1.499954e-01\n",
            "Loss: 1.499394e-01\n",
            "Loss: 1.498660e-01\n",
            "Loss: 1.497764e-01\n",
            "Loss: 1.496992e-01\n",
            "Loss: 1.495973e-01\n",
            "Loss: 1.495424e-01\n",
            "Loss: 1.494782e-01\n",
            "Loss: 1.493728e-01\n",
            "Loss: 1.492907e-01\n",
            "Loss: 1.491712e-01\n",
            "Loss: 1.491109e-01\n",
            "Loss: 1.490582e-01\n",
            "Loss: 1.490021e-01\n",
            "Loss: 1.489402e-01\n",
            "Loss: 1.488726e-01\n",
            "Loss: 1.487152e-01\n",
            "Loss: 1.486005e-01\n",
            "Loss: 1.484593e-01\n",
            "Loss: 1.485024e-01\n",
            "Loss: 1.484092e-01\n",
            "Loss: 1.483085e-01\n",
            "Loss: 1.481571e-01\n",
            "Loss: 1.479588e-01\n",
            "Loss: 1.477013e-01\n",
            "Loss: 1.476841e-01\n",
            "Loss: 1.475596e-01\n",
            "Loss: 1.475352e-01\n",
            "Loss: 1.475088e-01\n",
            "Loss: 1.474539e-01\n",
            "Loss: 1.473929e-01\n",
            "Loss: 1.472782e-01\n",
            "Loss: 1.476892e-01\n",
            "Loss: 1.472225e-01\n",
            "Loss: 1.471302e-01\n",
            "Loss: 1.470199e-01\n",
            "Loss: 1.469180e-01\n",
            "Loss: 1.467587e-01\n",
            "Loss: 1.466441e-01\n",
            "Loss: 1.465653e-01\n",
            "Loss: 1.464789e-01\n",
            "Loss: 1.463786e-01\n",
            "Loss: 1.462784e-01\n",
            "Loss: 1.460449e-01\n",
            "Loss: 1.458638e-01\n",
            "Loss: 1.455426e-01\n",
            "Loss: 1.455585e-01\n",
            "Loss: 1.454262e-01\n",
            "Loss: 1.452445e-01\n",
            "Loss: 1.451200e-01\n",
            "Loss: 1.450656e-01\n",
            "Loss: 1.450162e-01\n",
            "Loss: 1.449789e-01\n",
            "Loss: 1.449189e-01\n",
            "Loss: 1.448809e-01\n",
            "Loss: 1.448148e-01\n",
            "Loss: 1.447378e-01\n",
            "Loss: 1.446824e-01\n",
            "Loss: 1.446372e-01\n",
            "Loss: 1.445912e-01\n",
            "Loss: 1.445465e-01\n",
            "Loss: 1.444961e-01\n",
            "Loss: 1.444546e-01\n",
            "Loss: 1.444132e-01\n",
            "Loss: 1.443386e-01\n",
            "Loss: 1.442651e-01\n",
            "Loss: 1.441904e-01\n",
            "Loss: 1.440644e-01\n",
            "Loss: 1.439867e-01\n",
            "Loss: 1.438957e-01\n",
            "Loss: 1.438478e-01\n",
            "Loss: 1.438087e-01\n",
            "Loss: 1.437484e-01\n",
            "Loss: 1.436829e-01\n",
            "Loss: 1.436073e-01\n",
            "Loss: 1.435063e-01\n",
            "Loss: 1.439805e-01\n",
            "Loss: 1.434540e-01\n",
            "Loss: 1.433566e-01\n",
            "Loss: 1.431706e-01\n",
            "Loss: 1.430540e-01\n",
            "Loss: 1.429445e-01\n",
            "Loss: 1.428213e-01\n",
            "Loss: 1.426780e-01\n",
            "Loss: 1.425373e-01\n",
            "Loss: 1.423940e-01\n",
            "Loss: 1.422538e-01\n",
            "Loss: 1.421470e-01\n",
            "Loss: 1.419939e-01\n",
            "Loss: 1.418554e-01\n",
            "Loss: 1.416182e-01\n",
            "Loss: 1.414834e-01\n",
            "Loss: 1.413928e-01\n",
            "Loss: 1.413658e-01\n",
            "Loss: 1.412797e-01\n",
            "Loss: 1.412543e-01\n",
            "Loss: 1.412100e-01\n",
            "Loss: 1.411741e-01\n",
            "Loss: 1.410251e-01\n",
            "Loss: 1.410333e-01\n",
            "Loss: 1.409063e-01\n",
            "Loss: 1.407564e-01\n",
            "Loss: 1.406250e-01\n",
            "Loss: 1.404891e-01\n",
            "Loss: 1.402668e-01\n",
            "Loss: 1.400521e-01\n",
            "Loss: 1.399065e-01\n",
            "Loss: 1.397716e-01\n",
            "Loss: 1.396809e-01\n",
            "Loss: 1.395984e-01\n",
            "Loss: 1.394103e-01\n",
            "Loss: 1.392390e-01\n",
            "Loss: 1.419322e-01\n",
            "Loss: 1.391532e-01\n",
            "Loss: 1.390197e-01\n",
            "Loss: 1.388794e-01\n",
            "Loss: 1.387659e-01\n",
            "Loss: 1.385883e-01\n",
            "Loss: 1.383179e-01\n",
            "Loss: 1.380777e-01\n",
            "Loss: 1.380921e-01\n",
            "Loss: 1.379603e-01\n",
            "Loss: 1.377883e-01\n",
            "Loss: 1.375954e-01\n",
            "Loss: 1.374508e-01\n",
            "Loss: 1.372243e-01\n",
            "Loss: 1.369545e-01\n",
            "Loss: 1.364551e-01\n",
            "Loss: 1.359534e-01\n",
            "Loss: 1.354259e-01\n",
            "Loss: 1.350563e-01\n",
            "Loss: 1.348903e-01\n",
            "Loss: 1.346353e-01\n",
            "Loss: 1.344719e-01\n",
            "Loss: 1.343277e-01\n",
            "Loss: 1.341588e-01\n",
            "Loss: 1.339240e-01\n",
            "Loss: 1.336972e-01\n",
            "Loss: 1.335079e-01\n",
            "Loss: 1.332808e-01\n",
            "Loss: 1.330763e-01\n",
            "Loss: 1.331113e-01\n",
            "Loss: 1.329874e-01\n",
            "Loss: 1.327988e-01\n",
            "Loss: 1.326190e-01\n",
            "Loss: 1.324485e-01\n",
            "Loss: 1.322154e-01\n",
            "Loss: 1.318556e-01\n",
            "Loss: 1.315832e-01\n",
            "Loss: 1.314367e-01\n",
            "Loss: 1.312809e-01\n",
            "Loss: 1.310702e-01\n",
            "Loss: 1.307594e-01\n",
            "Loss: 1.304154e-01\n",
            "Loss: 1.297825e-01\n",
            "Loss: 1.295363e-01\n",
            "Loss: 1.288421e-01\n",
            "Loss: 1.286253e-01\n",
            "Loss: 1.284763e-01\n",
            "Loss: 1.283648e-01\n",
            "Loss: 1.282617e-01\n",
            "Loss: 1.281826e-01\n",
            "Loss: 1.280900e-01\n",
            "Loss: 1.279432e-01\n",
            "Loss: 1.281837e-01\n",
            "Loss: 1.278245e-01\n",
            "Loss: 1.275948e-01\n",
            "Loss: 1.273668e-01\n",
            "Loss: 1.271396e-01\n",
            "Loss: 1.268190e-01\n",
            "Loss: 1.272301e-01\n",
            "Loss: 1.266705e-01\n",
            "Loss: 1.262763e-01\n",
            "Loss: 1.258824e-01\n",
            "Loss: 1.256586e-01\n",
            "Loss: 1.254946e-01\n",
            "Loss: 1.253957e-01\n",
            "Loss: 1.253156e-01\n",
            "Loss: 1.252296e-01\n",
            "Loss: 1.250956e-01\n",
            "Loss: 1.250050e-01\n",
            "Loss: 1.247302e-01\n",
            "Loss: 1.245998e-01\n",
            "Loss: 1.245299e-01\n",
            "Loss: 1.244425e-01\n",
            "Loss: 1.242735e-01\n",
            "Loss: 1.240155e-01\n",
            "Loss: 1.237467e-01\n",
            "Loss: 1.234729e-01\n",
            "Loss: 1.231339e-01\n",
            "Loss: 1.229362e-01\n",
            "Loss: 1.228327e-01\n",
            "Loss: 1.226676e-01\n",
            "Loss: 1.225782e-01\n",
            "Loss: 1.224672e-01\n",
            "Loss: 1.223637e-01\n",
            "Loss: 1.222049e-01\n",
            "Loss: 1.220192e-01\n",
            "Loss: 1.222366e-01\n",
            "Loss: 1.219142e-01\n",
            "Loss: 1.217690e-01\n",
            "Loss: 1.215277e-01\n",
            "Loss: 1.212001e-01\n",
            "Loss: 1.208581e-01\n",
            "Loss: 1.202966e-01\n",
            "Loss: 1.193694e-01\n",
            "Loss: 1.189188e-01\n",
            "Loss: 1.186792e-01\n",
            "Loss: 1.184490e-01\n",
            "Loss: 1.182703e-01\n",
            "Loss: 1.179798e-01\n",
            "Loss: 1.178088e-01\n",
            "Loss: 1.176949e-01\n",
            "Loss: 1.175675e-01\n",
            "Loss: 1.174190e-01\n",
            "Loss: 1.172486e-01\n",
            "Loss: 1.172551e-01\n",
            "Loss: 1.171307e-01\n",
            "Loss: 1.169711e-01\n",
            "Loss: 1.168170e-01\n",
            "Loss: 1.166010e-01\n",
            "Loss: 1.163870e-01\n",
            "Loss: 1.179758e-01\n",
            "Loss: 1.163112e-01\n",
            "Loss: 1.160702e-01\n",
            "Loss: 1.159119e-01\n",
            "Loss: 1.157811e-01\n",
            "Loss: 1.155889e-01\n",
            "Loss: 1.154786e-01\n",
            "Loss: 1.153626e-01\n",
            "Loss: 1.152225e-01\n",
            "Loss: 1.151628e-01\n",
            "Loss: 1.149792e-01\n",
            "Loss: 1.149215e-01\n",
            "Loss: 1.148516e-01\n",
            "Loss: 1.147630e-01\n",
            "Loss: 1.146318e-01\n",
            "Loss: 1.145320e-01\n",
            "Loss: 1.143814e-01\n",
            "Loss: 1.142873e-01\n",
            "Loss: 1.142038e-01\n",
            "Loss: 1.141076e-01\n",
            "Loss: 1.139981e-01\n",
            "Loss: 1.138767e-01\n",
            "Loss: 1.137943e-01\n",
            "Loss: 1.137005e-01\n",
            "Loss: 1.136140e-01\n",
            "Loss: 1.135480e-01\n",
            "Loss: 1.134475e-01\n",
            "Loss: 1.132909e-01\n",
            "Loss: 1.130531e-01\n",
            "Loss: 1.127767e-01\n",
            "Loss: 1.124378e-01\n",
            "Loss: 1.131885e-01\n",
            "Loss: 1.123322e-01\n",
            "Loss: 1.121299e-01\n",
            "Loss: 1.119033e-01\n",
            "Loss: 1.117305e-01\n",
            "Loss: 1.115107e-01\n",
            "Loss: 1.114866e-01\n",
            "Loss: 1.113720e-01\n",
            "Loss: 1.112517e-01\n",
            "Loss: 1.111020e-01\n",
            "Loss: 1.108164e-01\n",
            "Loss: 1.107775e-01\n",
            "Loss: 1.105531e-01\n",
            "Loss: 1.102699e-01\n",
            "Loss: 1.101194e-01\n",
            "Loss: 1.099782e-01\n",
            "Loss: 1.096966e-01\n",
            "Loss: 1.096548e-01\n",
            "Loss: 1.094649e-01\n",
            "Loss: 1.091094e-01\n",
            "Loss: 1.086306e-01\n",
            "Loss: 1.082652e-01\n",
            "Loss: 1.078120e-01\n",
            "Loss: 1.087166e-01\n",
            "Loss: 1.076434e-01\n",
            "Loss: 1.072008e-01\n",
            "Loss: 1.068614e-01\n",
            "Loss: 1.066222e-01\n",
            "Loss: 1.064055e-01\n",
            "Loss: 1.060931e-01\n",
            "Loss: 1.059444e-01\n",
            "Loss: 1.056923e-01\n",
            "Loss: 1.056017e-01\n",
            "Loss: 1.055177e-01\n",
            "Loss: 1.054063e-01\n",
            "Loss: 1.051316e-01\n",
            "Loss: 1.048365e-01\n",
            "Loss: 1.045571e-01\n",
            "Loss: 1.043809e-01\n",
            "Loss: 1.042997e-01\n",
            "Loss: 1.041711e-01\n",
            "Loss: 1.038838e-01\n",
            "Loss: 1.044742e-01\n",
            "Loss: 1.037824e-01\n",
            "Loss: 1.034339e-01\n",
            "Loss: 1.031661e-01\n",
            "Loss: 1.027899e-01\n",
            "Loss: 1.024885e-01\n",
            "Loss: 1.018833e-01\n",
            "Loss: 1.016483e-01\n",
            "Loss: 1.012869e-01\n",
            "Loss: 1.010970e-01\n",
            "Loss: 1.009208e-01\n",
            "Loss: 1.006086e-01\n",
            "Loss: 1.004068e-01\n",
            "Loss: 1.001745e-01\n",
            "Loss: 9.991325e-02\n",
            "Loss: 9.961252e-02\n",
            "Loss: 9.929430e-02\n",
            "Loss: 9.912185e-02\n",
            "Loss: 9.891210e-02\n",
            "Loss: 9.858385e-02\n",
            "Loss: 9.844612e-02\n",
            "Loss: 9.823159e-02\n",
            "Loss: 9.812676e-02\n",
            "Loss: 9.800479e-02\n",
            "Loss: 9.783041e-02\n",
            "Loss: 9.764846e-02\n",
            "Loss: 9.749644e-02\n",
            "Loss: 9.738383e-02\n",
            "Loss: 9.724817e-02\n",
            "Loss: 9.702796e-02\n",
            "Loss: 9.681860e-02\n",
            "Loss: 9.654893e-02\n",
            "Loss: 9.638293e-02\n",
            "Loss: 9.612614e-02\n",
            "Loss: 9.591663e-02\n",
            "Loss: 9.560312e-02\n",
            "Loss: 9.540813e-02\n",
            "Loss: 9.508656e-02\n",
            "Loss: 9.496892e-02\n",
            "Loss: 9.488342e-02\n",
            "Loss: 9.478000e-02\n",
            "Loss: 9.463219e-02\n",
            "Loss: 9.446044e-02\n",
            "Loss: 9.423035e-02\n",
            "Loss: 9.384920e-02\n",
            "Loss: 9.603518e-02\n",
            "Loss: 9.355497e-02\n",
            "Loss: 9.317594e-02\n",
            "Loss: 9.230740e-02\n",
            "Loss: 9.195884e-02\n",
            "Loss: 9.150393e-02\n",
            "Loss: 9.128025e-02\n",
            "Loss: 9.108022e-02\n",
            "Loss: 9.091404e-02\n",
            "Loss: 9.077515e-02\n",
            "Loss: 9.063300e-02\n",
            "Loss: 9.043954e-02\n",
            "Loss: 9.038128e-02\n",
            "Loss: 9.019130e-02\n",
            "Loss: 9.010442e-02\n",
            "Loss: 8.997948e-02\n",
            "Loss: 8.979404e-02\n",
            "Loss: 8.956923e-02\n",
            "Loss: 8.946936e-02\n",
            "Loss: 8.933192e-02\n",
            "Loss: 8.927462e-02\n",
            "Loss: 8.915429e-02\n",
            "Loss: 8.899447e-02\n",
            "Loss: 8.880263e-02\n",
            "Loss: 8.846365e-02\n",
            "Loss: 8.822370e-02\n",
            "Loss: 8.808804e-02\n",
            "Loss: 8.793500e-02\n",
            "Loss: 8.767013e-02\n",
            "Loss: 8.750795e-02\n",
            "Loss: 8.730838e-02\n",
            "Loss: 8.718461e-02\n",
            "Loss: 8.712955e-02\n",
            "Loss: 8.705910e-02\n",
            "Loss: 8.696819e-02\n",
            "Loss: 8.695193e-02\n",
            "Loss: 8.687842e-02\n",
            "Loss: 8.676217e-02\n",
            "Loss: 8.664097e-02\n",
            "Loss: 8.648499e-02\n",
            "Loss: 8.625254e-02\n",
            "Loss: 8.602844e-02\n",
            "Loss: 8.582340e-02\n",
            "Loss: 8.560002e-02\n",
            "Loss: 8.542815e-02\n",
            "Loss: 8.516145e-02\n",
            "Loss: 8.487646e-02\n",
            "Loss: 8.464170e-02\n",
            "Loss: 8.432604e-02\n",
            "Loss: 8.404929e-02\n",
            "Loss: 8.380835e-02\n",
            "Loss: 8.358447e-02\n",
            "Loss: 8.347248e-02\n",
            "Loss: 8.329387e-02\n",
            "Loss: 8.316399e-02\n",
            "Loss: 8.300406e-02\n",
            "Loss: 8.283787e-02\n",
            "Loss: 8.267894e-02\n",
            "Loss: 8.247872e-02\n",
            "Loss: 8.244428e-02\n",
            "Loss: 8.231030e-02\n",
            "Loss: 8.212110e-02\n",
            "Loss: 8.190449e-02\n",
            "Loss: 8.177115e-02\n",
            "Loss: 8.155884e-02\n",
            "Loss: 8.138978e-02\n",
            "Loss: 8.128350e-02\n",
            "Loss: 8.112726e-02\n",
            "Loss: 8.098812e-02\n",
            "Loss: 8.083096e-02\n",
            "Loss: 8.060178e-02\n",
            "Loss: 8.037163e-02\n",
            "Loss: 8.002743e-02\n",
            "Loss: 7.975519e-02\n",
            "Loss: 7.946794e-02\n",
            "Loss: 7.907651e-02\n",
            "Loss: 7.861920e-02\n",
            "Loss: 7.809864e-02\n",
            "Loss: 7.769048e-02\n",
            "Loss: 7.748522e-02\n",
            "Loss: 7.732937e-02\n",
            "Loss: 7.714562e-02\n",
            "Loss: 7.696453e-02\n",
            "Loss: 7.687460e-02\n",
            "Loss: 7.680362e-02\n",
            "Loss: 7.672334e-02\n",
            "Loss: 7.665518e-02\n",
            "Loss: 7.659709e-02\n",
            "Loss: 7.654683e-02\n",
            "Loss: 7.649456e-02\n",
            "Loss: 7.642969e-02\n",
            "Loss: 7.629021e-02\n",
            "Loss: 7.615719e-02\n",
            "Loss: 7.600860e-02\n",
            "Loss: 7.584073e-02\n",
            "Loss: 7.570858e-02\n",
            "Loss: 7.551195e-02\n",
            "Loss: 7.531466e-02\n",
            "Loss: 7.513146e-02\n",
            "Loss: 7.497428e-02\n",
            "Loss: 7.488383e-02\n",
            "Loss: 7.481781e-02\n",
            "Loss: 7.459886e-02\n",
            "Loss: 7.451455e-02\n",
            "Loss: 7.446443e-02\n",
            "Loss: 7.442892e-02\n",
            "Loss: 7.451615e-02\n",
            "Loss: 7.437631e-02\n",
            "Loss: 7.425839e-02\n",
            "Loss: 7.403894e-02\n",
            "Loss: 7.388774e-02\n",
            "Loss: 7.364950e-02\n",
            "Loss: 7.405184e-02\n",
            "Loss: 7.351080e-02\n",
            "Loss: 7.328671e-02\n",
            "Loss: 7.301339e-02\n",
            "Loss: 7.289330e-02\n",
            "Loss: 7.275894e-02\n",
            "Loss: 7.264432e-02\n",
            "Loss: 7.248998e-02\n",
            "Loss: 7.232071e-02\n",
            "Loss: 7.203701e-02\n",
            "Loss: 7.172021e-02\n",
            "Loss: 7.195675e-02\n",
            "Loss: 7.149221e-02\n",
            "Loss: 7.123679e-02\n",
            "Loss: 7.075168e-02\n",
            "Loss: 7.045007e-02\n",
            "Loss: 7.006963e-02\n",
            "Loss: 6.970233e-02\n",
            "Loss: 6.954956e-02\n",
            "Loss: 6.942829e-02\n",
            "Loss: 6.922167e-02\n",
            "Loss: 6.912927e-02\n",
            "Loss: 6.914310e-02\n",
            "Loss: 6.904323e-02\n",
            "Loss: 6.894894e-02\n",
            "Loss: 6.890899e-02\n",
            "Loss: 6.885973e-02\n",
            "Loss: 6.879898e-02\n",
            "Loss: 6.872596e-02\n",
            "Loss: 6.865062e-02\n",
            "Loss: 6.854072e-02\n",
            "Loss: 6.846417e-02\n",
            "Loss: 6.826982e-02\n",
            "Loss: 6.817583e-02\n",
            "Loss: 6.807408e-02\n",
            "Loss: 6.799588e-02\n",
            "Loss: 6.791267e-02\n",
            "Loss: 6.781603e-02\n",
            "Loss: 6.775409e-02\n",
            "Loss: 6.762651e-02\n",
            "Loss: 6.753810e-02\n",
            "Loss: 6.741061e-02\n",
            "Loss: 6.730701e-02\n",
            "Loss: 6.716195e-02\n",
            "Loss: 6.704904e-02\n",
            "Loss: 6.711750e-02\n",
            "Loss: 6.697109e-02\n",
            "Loss: 6.686784e-02\n",
            "Loss: 6.672190e-02\n",
            "Loss: 6.654565e-02\n",
            "Loss: 6.637491e-02\n",
            "Loss: 6.620671e-02\n",
            "Loss: 6.610560e-02\n",
            "Loss: 6.599148e-02\n",
            "Loss: 6.591009e-02\n",
            "Loss: 6.576583e-02\n",
            "Loss: 6.654532e-02\n",
            "Loss: 6.573266e-02\n",
            "Loss: 6.561337e-02\n",
            "Loss: 6.550389e-02\n",
            "Loss: 6.541789e-02\n",
            "Loss: 6.534874e-02\n",
            "Loss: 6.523321e-02\n",
            "Loss: 6.511847e-02\n",
            "Loss: 6.501499e-02\n",
            "Loss: 6.491305e-02\n",
            "Loss: 6.484474e-02\n",
            "Loss: 6.480969e-02\n",
            "Loss: 6.467140e-02\n",
            "Loss: 6.453777e-02\n",
            "Loss: 6.436004e-02\n",
            "Loss: 6.419868e-02\n",
            "Loss: 6.455956e-02\n",
            "Loss: 6.412278e-02\n",
            "Loss: 6.391578e-02\n",
            "Loss: 6.380954e-02\n",
            "Loss: 6.370762e-02\n",
            "Loss: 6.361489e-02\n",
            "Loss: 6.351255e-02\n",
            "Loss: 6.335463e-02\n",
            "Loss: 6.324945e-02\n",
            "Loss: 6.299099e-02\n",
            "Loss: 6.293192e-02\n",
            "Loss: 6.280584e-02\n",
            "Loss: 6.266817e-02\n",
            "Loss: 6.248463e-02\n",
            "Loss: 6.211336e-02\n",
            "Loss: 6.228045e-02\n",
            "Loss: 6.190256e-02\n",
            "Loss: 6.141397e-02\n",
            "Loss: 6.104861e-02\n",
            "Loss: 6.088474e-02\n",
            "Loss: 6.078332e-02\n",
            "Loss: 6.062906e-02\n",
            "Loss: 6.045751e-02\n",
            "Loss: 6.010592e-02\n",
            "Loss: 5.977431e-02\n",
            "Loss: 5.947410e-02\n",
            "Loss: 5.920151e-02\n",
            "Loss: 5.904864e-02\n",
            "Loss: 5.873161e-02\n",
            "Loss: 5.879954e-02\n",
            "Loss: 5.856068e-02\n",
            "Loss: 5.828063e-02\n",
            "Loss: 5.802963e-02\n",
            "Loss: 5.791507e-02\n",
            "Loss: 5.784135e-02\n",
            "Loss: 5.777124e-02\n",
            "Loss: 5.768247e-02\n",
            "Loss: 5.754684e-02\n",
            "Loss: 5.731067e-02\n",
            "Loss: 5.702257e-02\n",
            "Loss: 5.674836e-02\n",
            "Loss: 5.661607e-02\n",
            "Loss: 5.651007e-02\n",
            "Loss: 5.645321e-02\n",
            "Loss: 5.631249e-02\n",
            "Loss: 5.618896e-02\n",
            "Loss: 5.604755e-02\n",
            "Loss: 5.592708e-02\n",
            "Loss: 5.575594e-02\n",
            "Loss: 5.559081e-02\n",
            "Loss: 5.548376e-02\n",
            "Loss: 5.532569e-02\n",
            "Loss: 5.520470e-02\n",
            "Loss: 5.502945e-02\n",
            "Loss: 5.478504e-02\n",
            "Loss: 5.462107e-02\n",
            "Loss: 5.440800e-02\n",
            "Loss: 5.425282e-02\n",
            "Loss: 5.409293e-02\n",
            "Loss: 5.393145e-02\n",
            "Loss: 5.429715e-02\n",
            "Loss: 5.385483e-02\n",
            "Loss: 5.368500e-02\n",
            "Loss: 5.357520e-02\n",
            "Loss: 5.350095e-02\n",
            "Loss: 5.344969e-02\n",
            "Loss: 5.340008e-02\n",
            "Loss: 5.331879e-02\n",
            "Loss: 5.318832e-02\n",
            "Loss: 5.338781e-02\n",
            "Loss: 5.309433e-02\n",
            "Loss: 5.289470e-02\n",
            "Loss: 5.269635e-02\n",
            "Loss: 5.255607e-02\n",
            "Loss: 5.247393e-02\n",
            "Loss: 5.236385e-02\n",
            "Loss: 5.225507e-02\n",
            "Loss: 5.214576e-02\n",
            "Loss: 5.200808e-02\n",
            "Loss: 5.214713e-02\n",
            "Loss: 5.195693e-02\n",
            "Loss: 5.188701e-02\n",
            "Loss: 5.182604e-02\n",
            "Loss: 5.175125e-02\n",
            "Loss: 5.160943e-02\n",
            "Loss: 5.153498e-02\n",
            "Loss: 5.136056e-02\n",
            "Loss: 5.127983e-02\n",
            "Loss: 5.116090e-02\n",
            "Loss: 5.109490e-02\n",
            "Loss: 5.095861e-02\n",
            "Loss: 5.091869e-02\n",
            "Loss: 5.075795e-02\n",
            "Loss: 5.069310e-02\n",
            "Loss: 5.063561e-02\n",
            "Loss: 5.056700e-02\n",
            "Loss: 5.051873e-02\n",
            "Loss: 5.045410e-02\n",
            "Loss: 5.040926e-02\n",
            "Loss: 5.035309e-02\n",
            "Loss: 5.030760e-02\n",
            "Loss: 5.024305e-02\n",
            "Loss: 5.015906e-02\n",
            "Loss: 5.010675e-02\n",
            "Loss: 5.008510e-02\n",
            "Loss: 5.000936e-02\n",
            "Loss: 4.997760e-02\n",
            "Loss: 4.989727e-02\n",
            "Loss: 4.984613e-02\n",
            "Loss: 4.980412e-02\n",
            "Loss: 4.978903e-02\n",
            "Loss: 4.976327e-02\n",
            "Loss: 4.973258e-02\n",
            "Loss: 4.967844e-02\n",
            "Loss: 4.961330e-02\n",
            "Loss: 4.969458e-02\n",
            "Loss: 4.958728e-02\n",
            "Loss: 4.953457e-02\n",
            "Loss: 4.949179e-02\n",
            "Loss: 4.941909e-02\n",
            "Loss: 4.939285e-02\n",
            "Loss: 4.929930e-02\n",
            "Loss: 4.924716e-02\n",
            "Loss: 4.917993e-02\n",
            "Loss: 4.911137e-02\n",
            "Loss: 4.910277e-02\n",
            "Loss: 4.904282e-02\n",
            "Loss: 4.902883e-02\n",
            "Loss: 4.901025e-02\n",
            "Loss: 4.898627e-02\n",
            "Loss: 4.893113e-02\n",
            "Loss: 4.884410e-02\n",
            "Loss: 4.906893e-02\n",
            "Loss: 4.878417e-02\n",
            "Loss: 4.867753e-02\n",
            "Loss: 4.856992e-02\n",
            "Loss: 4.851005e-02\n",
            "Loss: 4.843755e-02\n",
            "Loss: 4.836316e-02\n",
            "Loss: 4.828174e-02\n",
            "Loss: 4.823031e-02\n",
            "Loss: 4.816967e-02\n",
            "Loss: 4.811783e-02\n",
            "Loss: 4.807007e-02\n",
            "Loss: 4.801971e-02\n",
            "Loss: 4.794778e-02\n",
            "Loss: 4.788727e-02\n",
            "Loss: 4.784326e-02\n",
            "Loss: 4.779352e-02\n",
            "Loss: 4.773476e-02\n",
            "Loss: 4.768139e-02\n",
            "Loss: 4.764569e-02\n",
            "Loss: 4.760155e-02\n",
            "Loss: 4.754657e-02\n",
            "Loss: 4.746924e-02\n",
            "Loss: 4.769285e-02\n",
            "Loss: 4.742112e-02\n",
            "Loss: 4.736464e-02\n",
            "Loss: 4.728771e-02\n",
            "Loss: 4.723342e-02\n",
            "Loss: 4.713669e-02\n",
            "Loss: 4.707032e-02\n",
            "Loss: 4.700340e-02\n",
            "Loss: 4.695586e-02\n",
            "Loss: 4.690395e-02\n",
            "Loss: 4.683547e-02\n",
            "Loss: 4.674107e-02\n",
            "Loss: 4.664914e-02\n",
            "Loss: 4.653979e-02\n",
            "Loss: 4.646467e-02\n",
            "Loss: 4.640226e-02\n",
            "Loss: 4.634274e-02\n",
            "Loss: 4.628778e-02\n",
            "Loss: 4.618268e-02\n",
            "Loss: 4.613911e-02\n",
            "Loss: 4.606392e-02\n",
            "Loss: 4.603107e-02\n",
            "Loss: 4.597051e-02\n",
            "Loss: 4.592610e-02\n",
            "Loss: 4.587560e-02\n",
            "Loss: 4.580605e-02\n",
            "Loss: 4.568980e-02\n",
            "Loss: 4.566155e-02\n",
            "Loss: 4.552110e-02\n",
            "Loss: 4.546624e-02\n",
            "Loss: 4.539882e-02\n",
            "Loss: 4.532531e-02\n",
            "Loss: 4.527355e-02\n",
            "Loss: 4.523440e-02\n",
            "Loss: 4.517901e-02\n",
            "Loss: 4.510102e-02\n",
            "Loss: 4.510806e-02\n",
            "Loss: 4.503836e-02\n",
            "Loss: 4.493199e-02\n",
            "Loss: 4.481392e-02\n",
            "Loss: 4.474199e-02\n",
            "Loss: 4.463813e-02\n",
            "Loss: 4.455275e-02\n",
            "Loss: 4.447003e-02\n",
            "Loss: 4.433650e-02\n",
            "Loss: 4.423178e-02\n",
            "Loss: 4.404464e-02\n",
            "Loss: 4.389613e-02\n",
            "Loss: 4.377256e-02\n",
            "Loss: 4.365752e-02\n",
            "Loss: 4.351220e-02\n",
            "Loss: 4.344111e-02\n",
            "Loss: 4.334384e-02\n",
            "Loss: 4.329191e-02\n",
            "Loss: 4.324517e-02\n",
            "Loss: 4.318223e-02\n",
            "Loss: 4.366042e-02\n",
            "Loss: 4.315214e-02\n",
            "Loss: 4.307475e-02\n",
            "Loss: 4.295479e-02\n",
            "Loss: 4.283507e-02\n",
            "Loss: 4.273448e-02\n",
            "Loss: 4.268409e-02\n",
            "Loss: 4.257912e-02\n",
            "Loss: 4.249052e-02\n",
            "Loss: 4.237533e-02\n",
            "Loss: 4.226312e-02\n",
            "Loss: 4.215192e-02\n",
            "Loss: 4.205865e-02\n",
            "Loss: 4.196511e-02\n",
            "Loss: 4.188345e-02\n",
            "Loss: 4.179817e-02\n",
            "Loss: 4.173709e-02\n",
            "Loss: 4.167727e-02\n",
            "Loss: 4.164897e-02\n",
            "Loss: 4.154740e-02\n",
            "Loss: 4.152592e-02\n",
            "Loss: 4.136258e-02\n",
            "Loss: 4.131052e-02\n",
            "Loss: 4.121517e-02\n",
            "Loss: 4.114526e-02\n",
            "Loss: 4.104655e-02\n",
            "Loss: 4.102278e-02\n",
            "Loss: 4.086286e-02\n",
            "Loss: 4.075319e-02\n",
            "Loss: 4.062954e-02\n",
            "Loss: 4.052097e-02\n",
            "Loss: 4.041037e-02\n",
            "Loss: 4.031172e-02\n",
            "Loss: 4.023926e-02\n",
            "Loss: 4.019031e-02\n",
            "Loss: 4.013486e-02\n",
            "Loss: 4.007923e-02\n",
            "Loss: 4.003377e-02\n",
            "Loss: 3.997221e-02\n",
            "Loss: 3.990441e-02\n",
            "Loss: 3.977550e-02\n",
            "Loss: 3.972553e-02\n",
            "Loss: 3.962303e-02\n",
            "Loss: 3.955570e-02\n",
            "Loss: 3.947657e-02\n",
            "Loss: 3.939903e-02\n",
            "Loss: 3.931416e-02\n",
            "Loss: 3.927070e-02\n",
            "Loss: 3.924374e-02\n",
            "Loss: 3.919813e-02\n",
            "Loss: 3.913029e-02\n",
            "Loss: 3.897202e-02\n",
            "Loss: 3.882380e-02\n",
            "Loss: 3.992490e-02\n",
            "Loss: 3.880004e-02\n",
            "Loss: 3.871627e-02\n",
            "Loss: 3.868000e-02\n",
            "Loss: 3.865729e-02\n",
            "Loss: 3.863461e-02\n",
            "Loss: 3.857695e-02\n",
            "Loss: 3.849298e-02\n",
            "Loss: 3.839603e-02\n",
            "Loss: 3.827929e-02\n",
            "Loss: 3.820312e-02\n",
            "Loss: 3.813009e-02\n",
            "Loss: 3.804140e-02\n",
            "Loss: 3.795968e-02\n",
            "Loss: 3.810556e-02\n",
            "Loss: 3.793567e-02\n",
            "Loss: 3.790142e-02\n",
            "Loss: 3.786460e-02\n",
            "Loss: 3.779731e-02\n",
            "Loss: 3.773510e-02\n",
            "Loss: 3.767410e-02\n",
            "Loss: 3.762717e-02\n",
            "Loss: 3.755063e-02\n",
            "Loss: 3.750513e-02\n",
            "Loss: 3.737669e-02\n",
            "Loss: 3.733893e-02\n",
            "Loss: 3.729328e-02\n",
            "Loss: 3.724923e-02\n",
            "Loss: 3.720788e-02\n",
            "Loss: 3.714308e-02\n",
            "Loss: 3.718939e-02\n",
            "Loss: 3.710404e-02\n",
            "Loss: 3.705019e-02\n",
            "Loss: 3.700282e-02\n",
            "Loss: 3.697868e-02\n",
            "Loss: 3.694801e-02\n",
            "Loss: 3.687023e-02\n",
            "Loss: 3.680911e-02\n",
            "Loss: 3.673513e-02\n",
            "Loss: 3.669877e-02\n",
            "Loss: 3.665993e-02\n",
            "Loss: 3.661907e-02\n",
            "Loss: 3.655401e-02\n",
            "Loss: 3.647943e-02\n",
            "Loss: 3.640206e-02\n",
            "Loss: 3.634616e-02\n",
            "Loss: 3.627246e-02\n",
            "Loss: 3.622586e-02\n",
            "Loss: 3.617744e-02\n",
            "Loss: 3.613198e-02\n",
            "Loss: 3.608560e-02\n",
            "Loss: 3.603205e-02\n",
            "Loss: 3.594484e-02\n",
            "Loss: 3.593300e-02\n",
            "Loss: 3.587533e-02\n",
            "Loss: 3.579931e-02\n",
            "Loss: 3.579528e-02\n",
            "Loss: 3.575871e-02\n",
            "Loss: 3.570848e-02\n",
            "Loss: 3.567600e-02\n",
            "Loss: 3.562612e-02\n",
            "Loss: 3.558575e-02\n",
            "Loss: 3.551073e-02\n",
            "Loss: 3.541373e-02\n",
            "Loss: 3.558658e-02\n",
            "Loss: 3.533696e-02\n",
            "Loss: 3.519348e-02\n",
            "Loss: 3.504910e-02\n",
            "Loss: 3.491380e-02\n",
            "Loss: 3.479409e-02\n",
            "Loss: 3.471725e-02\n",
            "Loss: 3.458782e-02\n",
            "Loss: 3.454469e-02\n",
            "Loss: 3.451402e-02\n",
            "Loss: 3.446332e-02\n",
            "Loss: 3.444971e-02\n",
            "Loss: 3.438790e-02\n",
            "Loss: 3.436765e-02\n",
            "Loss: 3.434813e-02\n",
            "Loss: 3.433605e-02\n",
            "Loss: 3.429762e-02\n",
            "Loss: 3.425290e-02\n",
            "Loss: 3.419559e-02\n",
            "Loss: 3.412706e-02\n",
            "Loss: 3.405908e-02\n",
            "Loss: 3.400610e-02\n",
            "Loss: 3.395345e-02\n",
            "Loss: 3.391073e-02\n",
            "Loss: 3.384930e-02\n",
            "Loss: 3.396228e-02\n",
            "Loss: 3.380938e-02\n",
            "Loss: 3.374439e-02\n",
            "Loss: 3.369427e-02\n",
            "Loss: 3.364014e-02\n",
            "Loss: 3.356364e-02\n",
            "Loss: 3.348779e-02\n",
            "Loss: 3.340024e-02\n",
            "Loss: 3.329945e-02\n",
            "Loss: 3.319313e-02\n",
            "Loss: 3.307223e-02\n",
            "Loss: 3.302388e-02\n",
            "Loss: 3.299004e-02\n",
            "Loss: 3.295056e-02\n",
            "Loss: 3.292562e-02\n",
            "Loss: 3.288016e-02\n",
            "Loss: 3.285466e-02\n",
            "Loss: 3.281241e-02\n",
            "Loss: 3.278049e-02\n",
            "Loss: 3.275494e-02\n",
            "Loss: 3.271828e-02\n",
            "Loss: 3.268735e-02\n",
            "Loss: 3.266167e-02\n",
            "Loss: 3.261961e-02\n",
            "Loss: 3.256193e-02\n",
            "Loss: 3.248759e-02\n",
            "Loss: 3.244927e-02\n",
            "Loss: 3.239276e-02\n",
            "Loss: 3.232957e-02\n",
            "Loss: 3.227542e-02\n",
            "Loss: 3.223452e-02\n",
            "Loss: 3.220728e-02\n",
            "Loss: 3.218027e-02\n",
            "Loss: 3.213894e-02\n",
            "Loss: 3.211115e-02\n",
            "Loss: 3.211215e-02\n",
            "Loss: 3.208878e-02\n",
            "Loss: 3.207313e-02\n",
            "Loss: 3.205075e-02\n",
            "Loss: 3.200901e-02\n",
            "Loss: 3.196021e-02\n",
            "Loss: 3.189969e-02\n",
            "Loss: 3.183115e-02\n",
            "Loss: 3.179017e-02\n",
            "Loss: 3.176425e-02\n",
            "Loss: 3.175416e-02\n",
            "Loss: 3.173319e-02\n",
            "Loss: 3.169590e-02\n",
            "Loss: 3.165134e-02\n",
            "Loss: 3.161142e-02\n",
            "Loss: 3.152873e-02\n",
            "Loss: 3.148509e-02\n",
            "Loss: 3.144813e-02\n",
            "Loss: 3.140897e-02\n",
            "Loss: 3.136332e-02\n",
            "Loss: 3.131188e-02\n",
            "Loss: 3.136396e-02\n",
            "Loss: 3.128390e-02\n",
            "Loss: 3.123601e-02\n",
            "Loss: 3.119203e-02\n",
            "Loss: 3.113787e-02\n",
            "Loss: 3.107730e-02\n",
            "Loss: 3.103587e-02\n",
            "Loss: 3.098527e-02\n",
            "Loss: 3.094233e-02\n",
            "Loss: 3.085648e-02\n",
            "Loss: 3.076897e-02\n",
            "Loss: 3.069060e-02\n",
            "Loss: 3.063570e-02\n",
            "Loss: 3.059364e-02\n",
            "Loss: 3.055882e-02\n",
            "Loss: 3.051798e-02\n",
            "Loss: 3.047621e-02\n",
            "Loss: 3.042645e-02\n",
            "Loss: 3.038963e-02\n",
            "Loss: 3.034332e-02\n",
            "Loss: 3.028299e-02\n",
            "Loss: 3.023962e-02\n",
            "Loss: 3.021089e-02\n",
            "Loss: 3.016403e-02\n",
            "Loss: 3.013791e-02\n",
            "Loss: 3.010485e-02\n",
            "Loss: 3.006600e-02\n",
            "Loss: 3.000572e-02\n",
            "Loss: 2.995710e-02\n",
            "Loss: 2.989149e-02\n",
            "Loss: 2.979848e-02\n",
            "Loss: 2.965856e-02\n",
            "Loss: 2.956879e-02\n",
            "Loss: 2.956299e-02\n",
            "Loss: 2.945160e-02\n",
            "Loss: 2.939585e-02\n",
            "Loss: 2.931602e-02\n",
            "Loss: 2.914196e-02\n",
            "Loss: 2.911270e-02\n",
            "Loss: 2.900765e-02\n",
            "Loss: 2.895633e-02\n",
            "Loss: 2.888681e-02\n",
            "Loss: 2.883030e-02\n",
            "Loss: 2.877750e-02\n",
            "Loss: 2.872199e-02\n",
            "Loss: 2.868226e-02\n",
            "Loss: 2.869248e-02\n",
            "Loss: 2.865358e-02\n",
            "Loss: 2.860183e-02\n",
            "Loss: 2.853413e-02\n",
            "Loss: 2.848643e-02\n",
            "Loss: 2.845385e-02\n",
            "Loss: 2.839892e-02\n",
            "Loss: 2.837078e-02\n",
            "Loss: 2.834130e-02\n",
            "Loss: 2.836601e-02\n",
            "Loss: 2.832370e-02\n",
            "Loss: 2.829100e-02\n",
            "Loss: 2.825371e-02\n",
            "Loss: 2.822293e-02\n",
            "Loss: 2.818467e-02\n",
            "Loss: 2.811468e-02\n",
            "Loss: 2.803452e-02\n",
            "Loss: 2.796298e-02\n",
            "Loss: 2.792064e-02\n",
            "Loss: 2.789852e-02\n",
            "Loss: 2.786004e-02\n",
            "Loss: 2.782841e-02\n",
            "Loss: 2.777855e-02\n",
            "Loss: 2.771744e-02\n",
            "Loss: 2.766797e-02\n",
            "Loss: 2.761866e-02\n",
            "Loss: 2.756687e-02\n",
            "Loss: 2.749582e-02\n",
            "Loss: 2.741748e-02\n",
            "Loss: 2.735078e-02\n",
            "Loss: 2.730089e-02\n",
            "Loss: 2.727261e-02\n",
            "Loss: 2.723849e-02\n",
            "Loss: 2.718828e-02\n",
            "Loss: 2.716606e-02\n",
            "Loss: 2.713047e-02\n",
            "Loss: 2.707613e-02\n",
            "Loss: 2.702854e-02\n",
            "Loss: 2.697188e-02\n",
            "Loss: 2.700799e-02\n",
            "Loss: 2.693452e-02\n",
            "Loss: 2.686429e-02\n",
            "Loss: 2.677583e-02\n",
            "Loss: 2.710129e-02\n",
            "Loss: 2.674927e-02\n",
            "Loss: 2.669789e-02\n",
            "Loss: 2.663199e-02\n",
            "Loss: 2.659196e-02\n",
            "Loss: 2.656294e-02\n",
            "Loss: 2.654285e-02\n",
            "Loss: 2.652223e-02\n",
            "Loss: 2.648639e-02\n",
            "Loss: 2.645807e-02\n",
            "Loss: 2.641609e-02\n",
            "Loss: 2.636309e-02\n",
            "Loss: 2.630760e-02\n",
            "Loss: 2.631496e-02\n",
            "Loss: 2.627904e-02\n",
            "Loss: 2.622108e-02\n",
            "Loss: 2.617810e-02\n",
            "Loss: 2.615297e-02\n",
            "Loss: 2.612835e-02\n",
            "Loss: 2.610720e-02\n",
            "Loss: 2.607843e-02\n",
            "Loss: 2.604049e-02\n",
            "Loss: 2.604035e-02\n",
            "Loss: 2.602062e-02\n",
            "Loss: 2.598963e-02\n",
            "Loss: 2.596914e-02\n",
            "Loss: 2.594314e-02\n",
            "Loss: 2.592214e-02\n",
            "Loss: 2.587961e-02\n",
            "Loss: 2.586876e-02\n",
            "Loss: 2.583954e-02\n",
            "Loss: 2.582724e-02\n",
            "Loss: 2.581179e-02\n",
            "Loss: 2.579437e-02\n",
            "Loss: 2.577986e-02\n",
            "Loss: 2.576324e-02\n",
            "Loss: 2.575126e-02\n",
            "Loss: 2.574063e-02\n",
            "Loss: 2.572840e-02\n",
            "Loss: 2.571498e-02\n",
            "Loss: 2.570212e-02\n",
            "Loss: 2.568929e-02\n",
            "Loss: 2.567991e-02\n",
            "Loss: 2.566860e-02\n",
            "Loss: 2.565877e-02\n",
            "Loss: 2.564577e-02\n",
            "Loss: 2.566281e-02\n",
            "Loss: 2.564088e-02\n",
            "Loss: 2.563442e-02\n",
            "Loss: 2.562847e-02\n",
            "Loss: 2.562298e-02\n",
            "Loss: 2.561149e-02\n",
            "Loss: 2.559191e-02\n",
            "Loss: 2.559205e-02\n",
            "Loss: 2.557802e-02\n",
            "Loss: 2.555245e-02\n",
            "Loss: 2.553454e-02\n",
            "Loss: 2.553188e-02\n",
            "Loss: 2.550910e-02\n",
            "Loss: 2.549899e-02\n",
            "Loss: 2.547917e-02\n",
            "Loss: 2.546054e-02\n",
            "Loss: 2.544572e-02\n",
            "Loss: 2.541380e-02\n",
            "Loss: 2.538059e-02\n",
            "Loss: 2.536317e-02\n",
            "Loss: 2.534652e-02\n",
            "Loss: 2.533117e-02\n",
            "Loss: 2.531835e-02\n",
            "Loss: 2.528934e-02\n",
            "Loss: 2.526814e-02\n",
            "Loss: 2.525575e-02\n",
            "Loss: 2.524460e-02\n",
            "Loss: 2.522358e-02\n",
            "Loss: 2.520198e-02\n",
            "Loss: 2.518209e-02\n",
            "Loss: 2.516721e-02\n",
            "Loss: 2.515958e-02\n",
            "Loss: 2.514422e-02\n",
            "Loss: 2.512670e-02\n",
            "Loss: 2.512993e-02\n",
            "Loss: 2.511346e-02\n",
            "Loss: 2.509041e-02\n",
            "Loss: 2.507546e-02\n",
            "Loss: 2.504670e-02\n",
            "Loss: 2.502017e-02\n",
            "Loss: 2.499003e-02\n",
            "Loss: 2.491354e-02\n",
            "Loss: 2.485980e-02\n",
            "Loss: 2.484474e-02\n",
            "Loss: 2.477847e-02\n",
            "Loss: 2.476470e-02\n",
            "Loss: 2.474566e-02\n",
            "Loss: 2.471587e-02\n",
            "Loss: 2.470070e-02\n",
            "Loss: 2.461910e-02\n",
            "Loss: 2.457042e-02\n",
            "Loss: 2.449091e-02\n",
            "Loss: 2.445518e-02\n",
            "Loss: 2.440496e-02\n",
            "Loss: 2.437076e-02\n",
            "Loss: 2.433995e-02\n",
            "Loss: 2.438389e-02\n",
            "Loss: 2.432511e-02\n",
            "Loss: 2.429468e-02\n",
            "Loss: 2.426533e-02\n",
            "Loss: 2.421011e-02\n",
            "Loss: 2.415785e-02\n",
            "Loss: 2.408645e-02\n",
            "Loss: 2.402452e-02\n",
            "Loss: 2.399159e-02\n",
            "Loss: 2.397216e-02\n",
            "Loss: 2.393931e-02\n",
            "Loss: 2.389630e-02\n",
            "Loss: 2.382730e-02\n",
            "Loss: 2.374712e-02\n",
            "Loss: 2.369487e-02\n",
            "Loss: 2.363053e-02\n",
            "Loss: 2.357952e-02\n",
            "Loss: 2.352538e-02\n",
            "Loss: 2.345211e-02\n",
            "Loss: 2.344316e-02\n",
            "Loss: 2.340637e-02\n",
            "Loss: 2.333608e-02\n",
            "Loss: 2.331501e-02\n",
            "Loss: 2.329096e-02\n",
            "Loss: 2.328007e-02\n",
            "Loss: 2.326011e-02\n",
            "Loss: 2.325397e-02\n",
            "Loss: 2.320637e-02\n",
            "Loss: 2.318135e-02\n",
            "Loss: 2.315548e-02\n",
            "Loss: 2.312087e-02\n",
            "Loss: 2.311784e-02\n",
            "Loss: 2.308881e-02\n",
            "Loss: 2.303556e-02\n",
            "Loss: 2.300902e-02\n",
            "Loss: 2.298540e-02\n",
            "Loss: 2.295127e-02\n",
            "Loss: 2.288992e-02\n",
            "Loss: 2.314484e-02\n",
            "Loss: 2.287020e-02\n",
            "Loss: 2.281479e-02\n",
            "Loss: 2.276319e-02\n",
            "Loss: 2.270311e-02\n",
            "Loss: 2.265766e-02\n",
            "Loss: 2.259755e-02\n",
            "Loss: 2.253853e-02\n",
            "Loss: 2.248267e-02\n",
            "Loss: 2.246017e-02\n",
            "Loss: 2.244472e-02\n",
            "Loss: 2.242523e-02\n",
            "Loss: 2.239269e-02\n",
            "Loss: 2.236450e-02\n",
            "Loss: 2.235088e-02\n",
            "Loss: 2.231710e-02\n",
            "Loss: 2.230329e-02\n",
            "Loss: 2.227742e-02\n",
            "Loss: 2.225555e-02\n",
            "Loss: 2.224197e-02\n",
            "Loss: 2.222160e-02\n",
            "Loss: 2.220480e-02\n",
            "Loss: 2.217557e-02\n",
            "Loss: 2.213117e-02\n",
            "Loss: 2.209169e-02\n",
            "Loss: 2.204929e-02\n",
            "Loss: 2.202384e-02\n",
            "Loss: 2.200069e-02\n",
            "Loss: 2.197293e-02\n",
            "Loss: 2.194631e-02\n",
            "Loss: 2.191510e-02\n",
            "Loss: 2.190111e-02\n",
            "Loss: 2.186842e-02\n",
            "Loss: 2.184249e-02\n",
            "Loss: 2.181637e-02\n",
            "Loss: 2.179852e-02\n",
            "Loss: 2.178791e-02\n",
            "Loss: 2.176960e-02\n",
            "Loss: 2.175337e-02\n",
            "Loss: 2.174235e-02\n",
            "Loss: 2.172508e-02\n",
            "Loss: 2.169924e-02\n",
            "Loss: 2.173946e-02\n",
            "Loss: 2.168956e-02\n",
            "Loss: 2.167534e-02\n",
            "Loss: 2.166628e-02\n",
            "Loss: 2.165337e-02\n",
            "Loss: 2.163027e-02\n",
            "Loss: 2.160274e-02\n",
            "Loss: 2.158488e-02\n",
            "Loss: 2.156211e-02\n",
            "Loss: 2.154904e-02\n",
            "Loss: 2.152902e-02\n",
            "Loss: 2.149621e-02\n",
            "Loss: 2.152153e-02\n",
            "Loss: 2.147887e-02\n",
            "Loss: 2.144235e-02\n",
            "Loss: 2.141260e-02\n",
            "Loss: 2.137953e-02\n",
            "Loss: 2.134269e-02\n",
            "Loss: 2.131128e-02\n",
            "Loss: 2.126895e-02\n",
            "Loss: 2.123522e-02\n",
            "Loss: 2.120047e-02\n",
            "Loss: 2.116514e-02\n",
            "Loss: 2.111971e-02\n",
            "Loss: 2.107162e-02\n",
            "Loss: 2.104615e-02\n",
            "Loss: 2.102914e-02\n",
            "Loss: 2.101104e-02\n",
            "Loss: 2.100233e-02\n",
            "Loss: 2.099554e-02\n",
            "Loss: 2.098813e-02\n",
            "Loss: 2.097876e-02\n",
            "Loss: 2.096982e-02\n",
            "Loss: 2.095868e-02\n",
            "Loss: 2.094735e-02\n",
            "Loss: 2.092684e-02\n",
            "Loss: 2.090077e-02\n",
            "Loss: 2.087705e-02\n",
            "Loss: 2.085136e-02\n",
            "Loss: 2.083586e-02\n",
            "Loss: 2.083270e-02\n",
            "Loss: 2.081534e-02\n",
            "Loss: 2.080160e-02\n",
            "Loss: 2.078716e-02\n",
            "Loss: 2.077524e-02\n",
            "Loss: 2.076042e-02\n",
            "Loss: 2.074616e-02\n",
            "Loss: 2.073397e-02\n",
            "Loss: 2.071961e-02\n",
            "Loss: 2.071407e-02\n",
            "Loss: 2.070658e-02\n",
            "Loss: 2.069779e-02\n",
            "Loss: 2.068600e-02\n",
            "Loss: 2.067735e-02\n",
            "Loss: 2.066432e-02\n",
            "Loss: 2.065659e-02\n",
            "Loss: 2.064003e-02\n",
            "Loss: 2.062834e-02\n",
            "Loss: 2.061515e-02\n",
            "Loss: 2.059691e-02\n",
            "Loss: 2.058642e-02\n",
            "Loss: 2.057219e-02\n",
            "Loss: 2.056245e-02\n",
            "Loss: 2.055250e-02\n",
            "Loss: 2.052535e-02\n",
            "Loss: 2.049515e-02\n",
            "Loss: 2.048909e-02\n",
            "Loss: 2.046678e-02\n",
            "Loss: 2.045785e-02\n",
            "Loss: 2.044522e-02\n",
            "Loss: 2.043238e-02\n",
            "Loss: 2.041751e-02\n",
            "Loss: 2.040566e-02\n",
            "Loss: 2.039571e-02\n",
            "Loss: 2.037925e-02\n",
            "Loss: 2.035701e-02\n",
            "Loss: 2.034257e-02\n",
            "Loss: 2.030379e-02\n",
            "Loss: 2.028827e-02\n",
            "Loss: 2.027419e-02\n",
            "Loss: 2.026467e-02\n",
            "Loss: 2.025032e-02\n",
            "Loss: 2.021752e-02\n",
            "Loss: 2.019959e-02\n",
            "Loss: 2.018605e-02\n",
            "Loss: 2.017047e-02\n",
            "Loss: 2.015516e-02\n",
            "Loss: 2.014553e-02\n",
            "Loss: 2.012785e-02\n",
            "Loss: 2.009601e-02\n",
            "Loss: 2.004518e-02\n",
            "Loss: 1.999909e-02\n",
            "Loss: 1.995030e-02\n",
            "Loss: 1.992078e-02\n",
            "Loss: 1.990567e-02\n",
            "Loss: 1.989030e-02\n",
            "Loss: 1.988191e-02\n",
            "Loss: 1.986160e-02\n",
            "Loss: 1.983931e-02\n",
            "Loss: 1.982006e-02\n",
            "Loss: 1.979717e-02\n",
            "Loss: 1.977022e-02\n",
            "Loss: 1.972929e-02\n",
            "Loss: 1.970704e-02\n",
            "Loss: 1.969399e-02\n",
            "Loss: 1.969730e-02\n",
            "Loss: 1.968438e-02\n",
            "Loss: 1.967550e-02\n",
            "Loss: 1.966341e-02\n",
            "Loss: 1.965175e-02\n",
            "Loss: 1.963788e-02\n",
            "Loss: 1.962247e-02\n",
            "Loss: 1.959787e-02\n",
            "Loss: 1.957852e-02\n",
            "Loss: 1.955447e-02\n",
            "Loss: 1.953849e-02\n",
            "Loss: 1.949964e-02\n",
            "Loss: 1.946083e-02\n",
            "Loss: 1.939697e-02\n",
            "Loss: 1.936070e-02\n",
            "Loss: 1.931768e-02\n",
            "Loss: 1.929447e-02\n",
            "Loss: 1.926709e-02\n",
            "Loss: 1.920954e-02\n",
            "Loss: 1.911957e-02\n",
            "Loss: 1.903545e-02\n",
            "Loss: 1.898349e-02\n",
            "Loss: 1.894884e-02\n",
            "Loss: 1.892842e-02\n",
            "Loss: 1.889534e-02\n",
            "Loss: 1.886173e-02\n",
            "Loss: 1.885965e-02\n",
            "Loss: 1.883955e-02\n",
            "Loss: 1.881314e-02\n",
            "Loss: 1.878671e-02\n",
            "Loss: 1.875087e-02\n",
            "Loss: 1.871555e-02\n",
            "Loss: 1.867872e-02\n",
            "Loss: 1.865061e-02\n",
            "Loss: 1.869687e-02\n",
            "Loss: 1.863031e-02\n",
            "Loss: 1.859786e-02\n",
            "Loss: 1.855678e-02\n",
            "Loss: 1.852383e-02\n",
            "Loss: 1.846577e-02\n",
            "Loss: 1.840308e-02\n",
            "Loss: 1.831792e-02\n",
            "Loss: 1.823461e-02\n",
            "Loss: 1.817794e-02\n",
            "Loss: 1.814812e-02\n",
            "Loss: 1.811950e-02\n",
            "Loss: 1.808700e-02\n",
            "Loss: 1.809430e-02\n",
            "Loss: 1.806944e-02\n",
            "Loss: 1.804608e-02\n",
            "Loss: 1.802323e-02\n",
            "Loss: 1.799990e-02\n",
            "Loss: 1.796742e-02\n",
            "Loss: 1.793502e-02\n",
            "Loss: 1.791650e-02\n",
            "Loss: 1.789759e-02\n",
            "Loss: 1.787302e-02\n",
            "Loss: 1.783732e-02\n",
            "Loss: 1.781394e-02\n",
            "Loss: 1.779322e-02\n",
            "Loss: 1.777678e-02\n",
            "Loss: 1.775755e-02\n",
            "Loss: 1.772952e-02\n",
            "Loss: 1.770863e-02\n",
            "Loss: 1.769132e-02\n",
            "Loss: 1.768160e-02\n",
            "Loss: 1.767518e-02\n",
            "Loss: 1.766476e-02\n",
            "Loss: 1.764888e-02\n",
            "Loss: 1.762909e-02\n",
            "Loss: 1.760732e-02\n",
            "Loss: 1.758241e-02\n",
            "Loss: 1.756064e-02\n",
            "Loss: 1.753332e-02\n",
            "Loss: 1.751156e-02\n",
            "Loss: 1.749560e-02\n",
            "Loss: 1.748625e-02\n",
            "Loss: 1.747125e-02\n",
            "Loss: 1.743910e-02\n",
            "Loss: 1.740081e-02\n",
            "Loss: 1.738034e-02\n",
            "Loss: 1.734906e-02\n",
            "Loss: 1.733894e-02\n",
            "Loss: 1.732871e-02\n",
            "Loss: 1.731857e-02\n",
            "Loss: 1.730818e-02\n",
            "Loss: 1.729810e-02\n",
            "Loss: 1.728867e-02\n",
            "Loss: 1.734591e-02\n",
            "Loss: 1.728252e-02\n",
            "Loss: 1.727305e-02\n",
            "Loss: 1.726237e-02\n",
            "Loss: 1.724431e-02\n",
            "Loss: 1.722148e-02\n",
            "Loss: 1.720151e-02\n",
            "Loss: 1.716297e-02\n",
            "Loss: 1.712747e-02\n",
            "Loss: 1.711119e-02\n",
            "Loss: 1.710299e-02\n",
            "Loss: 1.709664e-02\n",
            "Loss: 1.709299e-02\n",
            "Loss: 1.708901e-02\n",
            "Loss: 1.707794e-02\n",
            "Loss: 1.708692e-02\n",
            "Loss: 1.707214e-02\n",
            "Loss: 1.706057e-02\n",
            "Loss: 1.704541e-02\n",
            "Loss: 1.703176e-02\n",
            "Loss: 1.700531e-02\n",
            "Loss: 1.699096e-02\n",
            "Loss: 1.694657e-02\n",
            "Loss: 1.689927e-02\n",
            "Loss: 1.686312e-02\n",
            "Loss: 1.682757e-02\n",
            "Loss: 1.681664e-02\n",
            "Loss: 1.679704e-02\n",
            "Loss: 1.678862e-02\n",
            "Loss: 1.677745e-02\n",
            "Loss: 1.677092e-02\n",
            "Loss: 1.676119e-02\n",
            "Loss: 1.675468e-02\n",
            "Loss: 1.674253e-02\n",
            "Loss: 1.673156e-02\n",
            "Loss: 1.671344e-02\n",
            "Loss: 1.669467e-02\n",
            "Loss: 1.667413e-02\n",
            "Loss: 1.665885e-02\n",
            "Loss: 1.663880e-02\n",
            "Loss: 1.662523e-02\n",
            "Loss: 1.660780e-02\n",
            "Loss: 1.659536e-02\n",
            "Loss: 1.658420e-02\n",
            "Loss: 1.656723e-02\n",
            "Loss: 1.653875e-02\n",
            "Loss: 1.650807e-02\n",
            "Loss: 1.651207e-02\n",
            "Loss: 1.646217e-02\n",
            "Loss: 1.643161e-02\n",
            "Loss: 1.641047e-02\n",
            "Loss: 1.641232e-02\n",
            "Loss: 1.640562e-02\n",
            "Loss: 1.640134e-02\n",
            "Loss: 1.639335e-02\n",
            "Loss: 1.638491e-02\n",
            "Loss: 1.640883e-02\n",
            "Loss: 1.637995e-02\n",
            "Loss: 1.636831e-02\n",
            "Loss: 1.635554e-02\n",
            "Loss: 1.633402e-02\n",
            "Loss: 1.630438e-02\n",
            "Loss: 1.626197e-02\n",
            "Loss: 1.625048e-02\n",
            "Loss: 1.619780e-02\n",
            "Loss: 1.617893e-02\n",
            "Loss: 1.616054e-02\n",
            "Loss: 1.614138e-02\n",
            "Loss: 1.611906e-02\n",
            "Loss: 1.608937e-02\n",
            "Loss: 1.607484e-02\n",
            "Loss: 1.604483e-02\n",
            "Loss: 1.602157e-02\n",
            "Loss: 1.599832e-02\n",
            "Loss: 1.597664e-02\n",
            "Loss: 1.594843e-02\n",
            "Loss: 1.592864e-02\n",
            "Loss: 1.590377e-02\n",
            "Loss: 1.587463e-02\n",
            "Loss: 1.584380e-02\n",
            "Loss: 1.582024e-02\n",
            "Loss: 1.580121e-02\n",
            "Loss: 1.578335e-02\n",
            "Loss: 1.576634e-02\n",
            "Loss: 1.574648e-02\n",
            "Loss: 1.572725e-02\n",
            "Loss: 1.571360e-02\n",
            "Loss: 1.569073e-02\n",
            "Loss: 1.567092e-02\n",
            "Loss: 1.565252e-02\n",
            "Loss: 1.562237e-02\n",
            "Loss: 1.557871e-02\n",
            "Loss: 1.555373e-02\n",
            "Loss: 1.550360e-02\n",
            "Loss: 1.547655e-02\n",
            "Loss: 1.545577e-02\n",
            "Loss: 1.542721e-02\n",
            "Loss: 1.540482e-02\n",
            "Loss: 1.538279e-02\n",
            "Loss: 1.535982e-02\n",
            "Loss: 1.532714e-02\n",
            "Loss: 1.530870e-02\n",
            "Loss: 1.527405e-02\n",
            "Loss: 1.525279e-02\n",
            "Loss: 1.521017e-02\n",
            "Loss: 1.517092e-02\n",
            "Loss: 1.515072e-02\n",
            "Loss: 1.512308e-02\n",
            "Loss: 1.508350e-02\n",
            "Loss: 1.517283e-02\n",
            "Loss: 1.506373e-02\n",
            "Loss: 1.503807e-02\n",
            "Loss: 1.501966e-02\n",
            "Loss: 1.500806e-02\n",
            "Loss: 1.498646e-02\n",
            "Loss: 1.500452e-02\n",
            "Loss: 1.496293e-02\n",
            "Loss: 1.492154e-02\n",
            "Loss: 1.487136e-02\n",
            "Loss: 1.483348e-02\n",
            "Loss: 1.480437e-02\n",
            "Loss: 1.479052e-02\n",
            "Loss: 1.478131e-02\n",
            "Loss: 1.477117e-02\n",
            "Loss: 1.475858e-02\n",
            "Loss: 1.474050e-02\n",
            "Loss: 1.473031e-02\n",
            "Loss: 1.472190e-02\n",
            "Loss: 1.471338e-02\n",
            "Loss: 1.470812e-02\n",
            "Loss: 1.469818e-02\n",
            "Loss: 1.469136e-02\n",
            "Loss: 1.468890e-02\n",
            "Loss: 1.467993e-02\n",
            "Loss: 1.467642e-02\n",
            "Loss: 1.467106e-02\n",
            "Loss: 1.466652e-02\n",
            "Loss: 1.465501e-02\n",
            "Loss: 1.465070e-02\n",
            "Loss: 1.463405e-02\n",
            "Loss: 1.462201e-02\n",
            "Loss: 1.461506e-02\n",
            "Loss: 1.460803e-02\n",
            "Loss: 1.459813e-02\n",
            "Loss: 1.458902e-02\n",
            "Loss: 1.456994e-02\n",
            "Loss: 1.455830e-02\n",
            "Loss: 1.454380e-02\n",
            "Loss: 1.453686e-02\n",
            "Loss: 1.453146e-02\n",
            "Loss: 1.452531e-02\n",
            "Loss: 1.451589e-02\n",
            "Loss: 1.450527e-02\n",
            "Loss: 1.451032e-02\n",
            "Loss: 1.449946e-02\n",
            "Loss: 1.449209e-02\n",
            "Loss: 1.448572e-02\n",
            "Loss: 1.447854e-02\n",
            "Loss: 1.446764e-02\n",
            "Loss: 1.445594e-02\n",
            "Loss: 1.444516e-02\n",
            "Loss: 1.443558e-02\n",
            "Loss: 1.442299e-02\n",
            "Loss: 1.441326e-02\n",
            "Loss: 1.439644e-02\n",
            "Loss: 1.439468e-02\n",
            "Loss: 1.438061e-02\n",
            "Loss: 1.437530e-02\n",
            "Loss: 1.436557e-02\n",
            "Loss: 1.435631e-02\n",
            "Loss: 1.434478e-02\n",
            "Loss: 1.433521e-02\n",
            "Loss: 1.432217e-02\n",
            "Loss: 1.430914e-02\n",
            "Loss: 1.429326e-02\n",
            "Loss: 1.428219e-02\n",
            "Loss: 1.427750e-02\n",
            "Loss: 1.427493e-02\n",
            "Loss: 1.427187e-02\n",
            "Loss: 1.426510e-02\n",
            "Loss: 1.425852e-02\n",
            "Loss: 1.425198e-02\n",
            "Loss: 1.424270e-02\n",
            "Loss: 1.424670e-02\n",
            "Loss: 1.423887e-02\n",
            "Loss: 1.423512e-02\n",
            "Loss: 1.422863e-02\n",
            "Loss: 1.422476e-02\n",
            "Loss: 1.421691e-02\n",
            "Loss: 1.420935e-02\n",
            "Loss: 1.420386e-02\n",
            "Loss: 1.419720e-02\n",
            "Loss: 1.419293e-02\n",
            "Loss: 1.419072e-02\n",
            "Loss: 1.418587e-02\n",
            "Loss: 1.418408e-02\n",
            "Loss: 1.417939e-02\n",
            "Loss: 1.417575e-02\n",
            "Loss: 1.417246e-02\n",
            "Loss: 1.416863e-02\n",
            "Loss: 1.416216e-02\n",
            "Loss: 1.415800e-02\n",
            "Loss: 1.414930e-02\n",
            "Loss: 1.415224e-02\n",
            "Loss: 1.414594e-02\n",
            "Loss: 1.414219e-02\n",
            "Loss: 1.413881e-02\n",
            "Loss: 1.413673e-02\n",
            "Loss: 1.413470e-02\n",
            "Loss: 1.413148e-02\n",
            "Loss: 1.412872e-02\n",
            "Loss: 1.412636e-02\n",
            "Loss: 1.411963e-02\n",
            "Loss: 1.411413e-02\n",
            "Loss: 1.410536e-02\n",
            "Loss: 1.409747e-02\n",
            "Loss: 1.409151e-02\n",
            "Loss: 1.408601e-02\n",
            "Loss: 1.408269e-02\n",
            "Loss: 1.407559e-02\n",
            "Loss: 1.407012e-02\n",
            "Loss: 1.408854e-02\n",
            "Loss: 1.406777e-02\n",
            "Loss: 1.406158e-02\n",
            "Loss: 1.405638e-02\n",
            "Loss: 1.404950e-02\n",
            "Loss: 1.404009e-02\n",
            "Loss: 1.402815e-02\n",
            "Loss: 1.401933e-02\n",
            "Loss: 1.401247e-02\n",
            "Loss: 1.400589e-02\n",
            "Loss: 1.399897e-02\n",
            "Loss: 1.399368e-02\n",
            "Loss: 1.399852e-02\n",
            "Loss: 1.399022e-02\n",
            "Loss: 1.398565e-02\n",
            "Loss: 1.397968e-02\n",
            "Loss: 1.397670e-02\n",
            "Loss: 1.397462e-02\n",
            "Loss: 1.397493e-02\n",
            "Loss: 1.397288e-02\n",
            "Loss: 1.397102e-02\n",
            "Loss: 1.396723e-02\n",
            "Loss: 1.396429e-02\n",
            "Loss: 1.395974e-02\n",
            "Loss: 1.395430e-02\n",
            "Loss: 1.394764e-02\n",
            "Loss: 1.394330e-02\n",
            "Loss: 1.393568e-02\n",
            "Loss: 1.392558e-02\n",
            "Loss: 1.391700e-02\n",
            "Loss: 1.390794e-02\n",
            "Loss: 1.390514e-02\n",
            "Loss: 1.390021e-02\n",
            "Loss: 1.389088e-02\n",
            "Loss: 1.388276e-02\n",
            "Loss: 1.386850e-02\n",
            "Loss: 1.385191e-02\n",
            "Loss: 1.384257e-02\n",
            "Loss: 1.383393e-02\n",
            "Loss: 1.383575e-02\n",
            "Loss: 1.383071e-02\n",
            "Loss: 1.382583e-02\n",
            "Loss: 1.382192e-02\n",
            "Loss: 1.381666e-02\n",
            "Loss: 1.381125e-02\n",
            "Loss: 1.380645e-02\n",
            "Loss: 1.380452e-02\n",
            "Loss: 1.380068e-02\n",
            "Loss: 1.379809e-02\n",
            "Loss: 1.379337e-02\n",
            "Loss: 1.378657e-02\n",
            "Loss: 1.377694e-02\n",
            "Loss: 1.376648e-02\n",
            "Loss: 1.375932e-02\n",
            "Loss: 1.375487e-02\n",
            "Loss: 1.374963e-02\n",
            "Loss: 1.374114e-02\n",
            "Loss: 1.373218e-02\n",
            "Loss: 1.372065e-02\n",
            "Loss: 1.371653e-02\n",
            "Loss: 1.370850e-02\n",
            "Loss: 1.370462e-02\n",
            "Loss: 1.369919e-02\n",
            "Loss: 1.369163e-02\n",
            "Loss: 1.367901e-02\n",
            "Loss: 1.369412e-02\n",
            "Loss: 1.367214e-02\n",
            "Loss: 1.366147e-02\n",
            "Loss: 1.365186e-02\n",
            "Loss: 1.364425e-02\n",
            "Loss: 1.365530e-02\n",
            "Loss: 1.364030e-02\n",
            "Loss: 1.363474e-02\n",
            "Loss: 1.362668e-02\n",
            "Loss: 1.362193e-02\n",
            "Loss: 1.361651e-02\n",
            "Loss: 1.360965e-02\n",
            "Loss: 1.360580e-02\n",
            "Loss: 1.359941e-02\n",
            "Loss: 1.359490e-02\n",
            "Loss: 1.358593e-02\n",
            "Loss: 1.361249e-02\n",
            "Loss: 1.358384e-02\n",
            "Loss: 1.357784e-02\n",
            "Loss: 1.357146e-02\n",
            "Loss: 1.356351e-02\n",
            "Loss: 1.356179e-02\n",
            "Loss: 1.355633e-02\n",
            "Loss: 1.355317e-02\n",
            "Loss: 1.354867e-02\n",
            "Loss: 1.354274e-02\n",
            "Loss: 1.353515e-02\n",
            "Loss: 1.352550e-02\n",
            "Loss: 1.352664e-02\n",
            "Loss: 1.352033e-02\n",
            "Loss: 1.351252e-02\n",
            "Loss: 1.350709e-02\n",
            "Loss: 1.350093e-02\n",
            "Loss: 1.349163e-02\n",
            "Loss: 1.348363e-02\n",
            "Loss: 1.348142e-02\n",
            "Loss: 1.347176e-02\n",
            "Loss: 1.346676e-02\n",
            "Loss: 1.346007e-02\n",
            "Loss: 1.345406e-02\n",
            "Loss: 1.344521e-02\n",
            "Loss: 1.344352e-02\n",
            "Loss: 1.342959e-02\n",
            "Loss: 1.342063e-02\n",
            "Loss: 1.341127e-02\n",
            "Loss: 1.339463e-02\n",
            "Loss: 1.344171e-02\n",
            "Loss: 1.338896e-02\n",
            "Loss: 1.338044e-02\n",
            "Loss: 1.336916e-02\n",
            "Loss: 1.335813e-02\n",
            "Loss: 1.335465e-02\n",
            "Loss: 1.334197e-02\n",
            "Loss: 1.333830e-02\n",
            "Loss: 1.333105e-02\n",
            "Loss: 1.332345e-02\n",
            "Loss: 1.331200e-02\n",
            "Loss: 1.333964e-02\n",
            "Loss: 1.330424e-02\n",
            "Loss: 1.328778e-02\n",
            "Loss: 1.327653e-02\n",
            "Loss: 1.326950e-02\n",
            "Loss: 1.326158e-02\n",
            "Loss: 1.324640e-02\n",
            "Loss: 1.323477e-02\n",
            "Loss: 1.322043e-02\n",
            "Loss: 1.321428e-02\n",
            "Loss: 1.321112e-02\n",
            "Loss: 1.320700e-02\n",
            "Loss: 1.320442e-02\n",
            "Loss: 1.320130e-02\n",
            "Loss: 1.319603e-02\n",
            "Loss: 1.321843e-02\n",
            "Loss: 1.319232e-02\n",
            "Loss: 1.318478e-02\n",
            "Loss: 1.317344e-02\n",
            "Loss: 1.316632e-02\n",
            "Loss: 1.315572e-02\n",
            "Loss: 1.314814e-02\n",
            "Loss: 1.314104e-02\n",
            "Loss: 1.313098e-02\n",
            "Loss: 1.311866e-02\n",
            "Loss: 1.310943e-02\n",
            "Loss: 1.310174e-02\n",
            "Loss: 1.309109e-02\n",
            "Loss: 1.308254e-02\n",
            "Loss: 1.307305e-02\n",
            "Loss: 1.306597e-02\n",
            "Loss: 1.305909e-02\n",
            "Loss: 1.304691e-02\n",
            "Loss: 1.303909e-02\n",
            "Loss: 1.302613e-02\n",
            "Loss: 1.301809e-02\n",
            "Loss: 1.300907e-02\n",
            "Loss: 1.300060e-02\n",
            "Loss: 1.298774e-02\n",
            "Loss: 1.297470e-02\n",
            "Loss: 1.296526e-02\n",
            "Loss: 1.295807e-02\n",
            "Loss: 1.294062e-02\n",
            "Loss: 1.293905e-02\n",
            "Loss: 1.293167e-02\n",
            "Loss: 1.291800e-02\n",
            "Loss: 1.291055e-02\n",
            "Loss: 1.290451e-02\n",
            "Loss: 1.289861e-02\n",
            "Loss: 1.288757e-02\n",
            "Loss: 1.298292e-02\n",
            "Loss: 1.288298e-02\n",
            "Loss: 1.287710e-02\n",
            "Loss: 1.286450e-02\n",
            "Loss: 1.285342e-02\n",
            "Loss: 1.283944e-02\n",
            "Loss: 1.282409e-02\n",
            "Loss: 1.280873e-02\n",
            "Loss: 1.279407e-02\n",
            "Loss: 1.277909e-02\n",
            "Loss: 1.276283e-02\n",
            "Loss: 1.274847e-02\n",
            "Loss: 1.274820e-02\n",
            "Loss: 1.274036e-02\n",
            "Loss: 1.272895e-02\n",
            "Loss: 1.270970e-02\n",
            "Loss: 1.270207e-02\n",
            "Loss: 1.269672e-02\n",
            "Loss: 1.269205e-02\n",
            "Loss: 1.268809e-02\n",
            "Loss: 1.268191e-02\n",
            "Loss: 1.267700e-02\n",
            "Loss: 1.266989e-02\n",
            "Loss: 1.266181e-02\n",
            "Loss: 1.265496e-02\n",
            "Loss: 1.264160e-02\n",
            "Loss: 1.263052e-02\n",
            "Loss: 1.261210e-02\n",
            "Loss: 1.258939e-02\n",
            "Loss: 1.257476e-02\n",
            "Loss: 1.256778e-02\n",
            "Loss: 1.254833e-02\n",
            "Loss: 1.254530e-02\n",
            "Loss: 1.254237e-02\n",
            "Loss: 1.253781e-02\n",
            "Loss: 1.252386e-02\n",
            "Loss: 1.251273e-02\n",
            "Loss: 1.250304e-02\n",
            "Loss: 1.249477e-02\n",
            "Loss: 1.249130e-02\n",
            "Loss: 1.248720e-02\n",
            "Loss: 1.248349e-02\n",
            "Loss: 1.247552e-02\n",
            "Loss: 1.246677e-02\n",
            "Loss: 1.245553e-02\n",
            "Loss: 1.244918e-02\n",
            "Loss: 1.243930e-02\n",
            "Loss: 1.243416e-02\n",
            "Loss: 1.242772e-02\n",
            "Loss: 1.246352e-02\n",
            "Loss: 1.242582e-02\n",
            "Loss: 1.242299e-02\n",
            "Loss: 1.241507e-02\n",
            "Loss: 1.240577e-02\n",
            "Loss: 1.239531e-02\n",
            "Loss: 1.239133e-02\n",
            "Loss: 1.238820e-02\n",
            "Loss: 1.238447e-02\n",
            "Loss: 1.238008e-02\n",
            "Loss: 1.237526e-02\n",
            "Loss: 1.237149e-02\n",
            "Loss: 1.236702e-02\n",
            "Loss: 1.236234e-02\n",
            "Loss: 1.235883e-02\n",
            "Loss: 1.235399e-02\n",
            "Loss: 1.234656e-02\n",
            "Loss: 1.235750e-02\n",
            "Loss: 1.234497e-02\n",
            "Loss: 1.234068e-02\n",
            "Loss: 1.233882e-02\n",
            "Loss: 1.233570e-02\n",
            "Loss: 1.233255e-02\n",
            "Loss: 1.233144e-02\n",
            "Loss: 1.232773e-02\n",
            "Loss: 1.232643e-02\n",
            "Loss: 1.232487e-02\n",
            "Loss: 1.232203e-02\n",
            "Loss: 1.231568e-02\n",
            "Loss: 1.231859e-02\n",
            "Loss: 1.231308e-02\n",
            "Loss: 1.230851e-02\n",
            "Loss: 1.230499e-02\n",
            "Loss: 1.229961e-02\n",
            "Loss: 1.229609e-02\n",
            "Loss: 1.228849e-02\n",
            "Loss: 1.228217e-02\n",
            "Loss: 1.227665e-02\n",
            "Loss: 1.227319e-02\n",
            "Loss: 1.226796e-02\n",
            "Loss: 1.226231e-02\n",
            "Loss: 1.225690e-02\n",
            "Loss: 1.225115e-02\n",
            "Loss: 1.224477e-02\n",
            "Loss: 1.223624e-02\n",
            "Loss: 1.223195e-02\n",
            "Loss: 1.222947e-02\n",
            "Loss: 1.222609e-02\n",
            "Loss: 1.222275e-02\n",
            "Loss: 1.221868e-02\n",
            "Loss: 1.221122e-02\n",
            "Loss: 1.220105e-02\n",
            "Loss: 1.220735e-02\n",
            "Loss: 1.219661e-02\n",
            "Loss: 1.218954e-02\n",
            "Loss: 1.218465e-02\n",
            "Loss: 1.218625e-02\n",
            "Loss: 1.218267e-02\n",
            "Loss: 1.217928e-02\n",
            "Loss: 1.217671e-02\n",
            "Loss: 1.217239e-02\n",
            "Loss: 1.216863e-02\n",
            "Loss: 1.216484e-02\n",
            "Loss: 1.215895e-02\n",
            "Loss: 1.214967e-02\n",
            "Loss: 1.214755e-02\n",
            "Loss: 1.213064e-02\n",
            "Loss: 1.212516e-02\n",
            "Loss: 1.211679e-02\n",
            "Loss: 1.211500e-02\n",
            "Loss: 1.210221e-02\n",
            "Loss: 1.209414e-02\n",
            "Loss: 1.208074e-02\n",
            "Loss: 1.212676e-02\n",
            "Loss: 1.207634e-02\n",
            "Loss: 1.206262e-02\n",
            "Loss: 1.204250e-02\n",
            "Loss: 1.202210e-02\n",
            "Loss: 1.200345e-02\n",
            "Loss: 1.198976e-02\n",
            "Loss: 1.197987e-02\n",
            "Loss: 1.197181e-02\n",
            "Loss: 1.195816e-02\n",
            "Loss: 1.194633e-02\n",
            "Loss: 1.192672e-02\n",
            "Loss: 1.190917e-02\n",
            "Loss: 1.189916e-02\n",
            "Loss: 1.188659e-02\n",
            "Loss: 1.187329e-02\n",
            "Loss: 1.185005e-02\n",
            "Loss: 1.183497e-02\n",
            "Loss: 1.182279e-02\n",
            "Loss: 1.181621e-02\n",
            "Loss: 1.180801e-02\n",
            "Loss: 1.180149e-02\n",
            "Loss: 1.179142e-02\n",
            "Loss: 1.177776e-02\n",
            "Loss: 1.176271e-02\n",
            "Loss: 1.175646e-02\n",
            "Loss: 1.174750e-02\n",
            "Loss: 1.174305e-02\n",
            "Loss: 1.173599e-02\n",
            "Loss: 1.173441e-02\n",
            "Loss: 1.172264e-02\n",
            "Loss: 1.171955e-02\n",
            "Loss: 1.171490e-02\n",
            "Loss: 1.171072e-02\n",
            "Loss: 1.169973e-02\n",
            "Loss: 1.169399e-02\n",
            "Loss: 1.168593e-02\n",
            "Loss: 1.168217e-02\n",
            "Loss: 1.167568e-02\n",
            "Loss: 1.166957e-02\n",
            "Loss: 1.166297e-02\n",
            "Loss: 1.165715e-02\n",
            "Loss: 1.165364e-02\n",
            "Loss: 1.164903e-02\n",
            "Loss: 1.164577e-02\n",
            "Loss: 1.164200e-02\n",
            "Loss: 1.163877e-02\n",
            "Loss: 1.163391e-02\n",
            "Loss: 1.162923e-02\n",
            "Loss: 1.161934e-02\n",
            "Loss: 1.161344e-02\n",
            "Loss: 1.160921e-02\n",
            "Loss: 1.160610e-02\n",
            "Loss: 1.160239e-02\n",
            "Loss: 1.159878e-02\n",
            "Loss: 1.159609e-02\n",
            "Loss: 1.159306e-02\n",
            "Loss: 1.158909e-02\n",
            "Loss: 1.158250e-02\n",
            "Loss: 1.159867e-02\n",
            "Loss: 1.158012e-02\n",
            "Loss: 1.157593e-02\n",
            "Loss: 1.157194e-02\n",
            "Loss: 1.156729e-02\n",
            "Loss: 1.156163e-02\n",
            "Loss: 1.155833e-02\n",
            "Loss: 1.155413e-02\n",
            "Loss: 1.155243e-02\n",
            "Loss: 1.154972e-02\n",
            "Loss: 1.154461e-02\n",
            "Loss: 1.154002e-02\n",
            "Loss: 1.153538e-02\n",
            "Loss: 1.153225e-02\n",
            "Loss: 1.152916e-02\n",
            "Loss: 1.152663e-02\n",
            "Loss: 1.152303e-02\n",
            "Loss: 1.151856e-02\n",
            "Loss: 1.151071e-02\n",
            "Loss: 1.150483e-02\n",
            "Loss: 1.150358e-02\n",
            "Loss: 1.149661e-02\n",
            "Loss: 1.149406e-02\n",
            "Loss: 1.149033e-02\n",
            "Loss: 1.148593e-02\n",
            "Loss: 1.155684e-02\n",
            "Loss: 1.148386e-02\n",
            "Loss: 1.147817e-02\n",
            "Loss: 1.146672e-02\n",
            "Loss: 1.145587e-02\n",
            "Loss: 1.144745e-02\n",
            "Loss: 1.144180e-02\n",
            "Loss: 1.143391e-02\n",
            "Loss: 1.142194e-02\n",
            "Loss: 1.141249e-02\n",
            "Loss: 1.140942e-02\n",
            "Loss: 1.139944e-02\n",
            "Loss: 1.139621e-02\n",
            "Loss: 1.139117e-02\n",
            "Loss: 1.138507e-02\n",
            "Loss: 1.137878e-02\n",
            "Loss: 1.137163e-02\n",
            "Loss: 1.136390e-02\n",
            "Loss: 1.135453e-02\n",
            "Loss: 1.134998e-02\n",
            "Loss: 1.134772e-02\n",
            "Loss: 1.134170e-02\n",
            "Loss: 1.133862e-02\n",
            "Loss: 1.133537e-02\n",
            "Loss: 1.133214e-02\n",
            "Loss: 1.133436e-02\n",
            "Loss: 1.132902e-02\n",
            "Loss: 1.132412e-02\n",
            "Loss: 1.131493e-02\n",
            "Loss: 1.131136e-02\n",
            "Loss: 1.131002e-02\n",
            "Loss: 1.130400e-02\n",
            "Loss: 1.130120e-02\n",
            "Loss: 1.129680e-02\n",
            "Loss: 1.129272e-02\n",
            "Loss: 1.129217e-02\n",
            "Loss: 1.128952e-02\n",
            "Loss: 1.128827e-02\n",
            "Loss: 1.128582e-02\n",
            "Loss: 1.128263e-02\n",
            "Loss: 1.127834e-02\n",
            "Loss: 1.127733e-02\n",
            "Loss: 1.127263e-02\n",
            "Loss: 1.127040e-02\n",
            "Loss: 1.126562e-02\n",
            "Loss: 1.126055e-02\n",
            "Loss: 1.125662e-02\n",
            "Loss: 1.125321e-02\n",
            "Loss: 1.125087e-02\n",
            "Loss: 1.124926e-02\n",
            "Loss: 1.124403e-02\n",
            "Loss: 1.125539e-02\n",
            "Loss: 1.124206e-02\n",
            "Loss: 1.123630e-02\n",
            "Loss: 1.122962e-02\n",
            "Loss: 1.122321e-02\n",
            "Loss: 1.121759e-02\n",
            "Loss: 1.121175e-02\n",
            "Loss: 1.120554e-02\n",
            "Loss: 1.120189e-02\n",
            "Loss: 1.119438e-02\n",
            "Loss: 1.118544e-02\n",
            "Loss: 1.117493e-02\n",
            "Loss: 1.116600e-02\n",
            "Loss: 1.116017e-02\n",
            "Loss: 1.115160e-02\n",
            "Loss: 1.115372e-02\n",
            "Loss: 1.114780e-02\n",
            "Loss: 1.114476e-02\n",
            "Loss: 1.114007e-02\n",
            "Loss: 1.113737e-02\n",
            "Loss: 1.112719e-02\n",
            "Loss: 1.114302e-02\n",
            "Loss: 1.112373e-02\n",
            "Loss: 1.111571e-02\n",
            "Loss: 1.110572e-02\n",
            "Loss: 1.110006e-02\n",
            "Loss: 1.109183e-02\n",
            "Loss: 1.111312e-02\n",
            "Loss: 1.108844e-02\n",
            "Loss: 1.108008e-02\n",
            "Loss: 1.107150e-02\n",
            "Loss: 1.111330e-02\n",
            "Loss: 1.106657e-02\n",
            "Loss: 1.106017e-02\n",
            "Loss: 1.105589e-02\n",
            "Loss: 1.105094e-02\n",
            "Loss: 1.104492e-02\n",
            "Loss: 1.103641e-02\n",
            "Loss: 1.109131e-02\n",
            "Loss: 1.103339e-02\n",
            "Loss: 1.102278e-02\n",
            "Loss: 1.101059e-02\n",
            "Loss: 1.100802e-02\n",
            "Loss: 1.099598e-02\n",
            "Loss: 1.099242e-02\n",
            "Loss: 1.098356e-02\n",
            "Loss: 1.097562e-02\n",
            "Loss: 1.096359e-02\n",
            "Loss: 1.096082e-02\n",
            "Loss: 1.095024e-02\n",
            "Loss: 1.094781e-02\n",
            "Loss: 1.094505e-02\n",
            "Loss: 1.094222e-02\n",
            "Loss: 1.093730e-02\n",
            "Loss: 1.093332e-02\n",
            "Loss: 1.092450e-02\n",
            "Loss: 1.092102e-02\n",
            "Loss: 1.091828e-02\n",
            "Loss: 1.091659e-02\n",
            "Loss: 1.091272e-02\n",
            "Loss: 1.090634e-02\n",
            "Loss: 1.090255e-02\n",
            "Loss: 1.089664e-02\n",
            "Loss: 1.089156e-02\n",
            "Loss: 1.088885e-02\n",
            "Loss: 1.088265e-02\n",
            "Loss: 1.088278e-02\n",
            "Loss: 1.087985e-02\n",
            "Loss: 1.087674e-02\n",
            "Loss: 1.087464e-02\n",
            "Loss: 1.087230e-02\n",
            "Loss: 1.086791e-02\n",
            "Loss: 1.086272e-02\n",
            "Loss: 1.086855e-02\n",
            "Loss: 1.085984e-02\n",
            "Loss: 1.085548e-02\n",
            "Loss: 1.085253e-02\n",
            "Loss: 1.085046e-02\n",
            "Loss: 1.084752e-02\n",
            "Loss: 1.084646e-02\n",
            "Loss: 1.084306e-02\n",
            "Loss: 1.084150e-02\n",
            "Loss: 1.083919e-02\n",
            "Loss: 1.083665e-02\n",
            "Loss: 1.083814e-02\n",
            "Loss: 1.083543e-02\n",
            "Loss: 1.083298e-02\n",
            "Loss: 1.083127e-02\n",
            "Loss: 1.082839e-02\n",
            "Loss: 1.082634e-02\n",
            "Loss: 1.082353e-02\n",
            "Loss: 1.082218e-02\n",
            "Loss: 1.082104e-02\n",
            "Loss: 1.082000e-02\n",
            "Loss: 1.081781e-02\n",
            "Loss: 1.081858e-02\n",
            "Loss: 1.081584e-02\n",
            "Loss: 1.081314e-02\n",
            "Loss: 1.080854e-02\n",
            "Loss: 1.080527e-02\n",
            "Loss: 1.080787e-02\n",
            "Loss: 1.080280e-02\n",
            "Loss: 1.080002e-02\n",
            "Loss: 1.079846e-02\n",
            "Loss: 1.079770e-02\n",
            "Loss: 1.079609e-02\n",
            "Loss: 1.079293e-02\n",
            "Loss: 1.081968e-02\n",
            "Loss: 1.079153e-02\n",
            "Loss: 1.078821e-02\n",
            "Loss: 1.078581e-02\n",
            "Loss: 1.078362e-02\n",
            "Loss: 1.078167e-02\n",
            "Loss: 1.077862e-02\n",
            "Loss: 1.077566e-02\n",
            "Loss: 1.077353e-02\n",
            "Loss: 1.077377e-02\n",
            "Loss: 1.077252e-02\n",
            "Loss: 1.077102e-02\n",
            "Loss: 1.076845e-02\n",
            "Loss: 1.076594e-02\n",
            "Loss: 1.076488e-02\n",
            "Loss: 1.076258e-02\n",
            "Loss: 1.076177e-02\n",
            "Loss: 1.076011e-02\n",
            "Loss: 1.075756e-02\n",
            "Loss: 1.075514e-02\n",
            "Loss: 1.075186e-02\n",
            "Loss: 1.074782e-02\n",
            "Loss: 1.074433e-02\n",
            "Loss: 1.074300e-02\n",
            "Loss: 1.074035e-02\n",
            "Loss: 1.073846e-02\n",
            "Loss: 1.073598e-02\n",
            "Loss: 1.073289e-02\n",
            "Loss: 1.073176e-02\n",
            "Loss: 1.072837e-02\n",
            "Loss: 1.072637e-02\n",
            "Loss: 1.072438e-02\n",
            "Loss: 1.072702e-02\n",
            "Loss: 1.072292e-02\n",
            "Loss: 1.072138e-02\n",
            "Loss: 1.071826e-02\n",
            "Loss: 1.071651e-02\n",
            "Loss: 1.071245e-02\n",
            "Loss: 1.070917e-02\n",
            "Loss: 1.070864e-02\n",
            "Loss: 1.070432e-02\n",
            "Loss: 1.070280e-02\n",
            "Loss: 1.070005e-02\n",
            "Loss: 1.069646e-02\n",
            "Loss: 1.072989e-02\n",
            "Loss: 1.069467e-02\n",
            "Loss: 1.068989e-02\n",
            "Loss: 1.068397e-02\n",
            "Loss: 1.067716e-02\n",
            "Loss: 1.066998e-02\n",
            "Loss: 1.066881e-02\n",
            "Loss: 1.066261e-02\n",
            "Loss: 1.066015e-02\n",
            "Loss: 1.065648e-02\n",
            "Loss: 1.065213e-02\n",
            "Loss: 1.064777e-02\n",
            "Loss: 1.064167e-02\n",
            "Loss: 1.063886e-02\n",
            "Loss: 1.063662e-02\n",
            "Loss: 1.063510e-02\n",
            "Loss: 1.063198e-02\n",
            "Loss: 1.062855e-02\n",
            "Loss: 1.062547e-02\n",
            "Loss: 1.062138e-02\n",
            "Loss: 1.062307e-02\n",
            "Loss: 1.061852e-02\n",
            "Loss: 1.061315e-02\n",
            "Loss: 1.060983e-02\n",
            "Loss: 1.060731e-02\n",
            "Loss: 1.060408e-02\n",
            "Loss: 1.066731e-02\n",
            "Loss: 1.060341e-02\n",
            "Loss: 1.059924e-02\n",
            "Loss: 1.059572e-02\n",
            "Loss: 1.059266e-02\n",
            "Loss: 1.059107e-02\n",
            "Loss: 1.058872e-02\n",
            "Loss: 1.058611e-02\n",
            "Loss: 1.058369e-02\n",
            "Loss: 1.058011e-02\n",
            "Loss: 1.057275e-02\n",
            "Loss: 1.058616e-02\n",
            "Loss: 1.057048e-02\n",
            "Loss: 1.056524e-02\n",
            "Loss: 1.056266e-02\n",
            "Loss: 1.055940e-02\n",
            "Loss: 1.055630e-02\n",
            "Loss: 1.055023e-02\n",
            "Loss: 1.054697e-02\n",
            "Loss: 1.054334e-02\n",
            "Loss: 1.054061e-02\n",
            "Loss: 1.053790e-02\n",
            "Loss: 1.053471e-02\n",
            "Loss: 1.053080e-02\n",
            "Loss: 1.052754e-02\n",
            "Loss: 1.052418e-02\n",
            "Loss: 1.052911e-02\n",
            "Loss: 1.052202e-02\n",
            "Loss: 1.051922e-02\n",
            "Loss: 1.051713e-02\n",
            "Loss: 1.051619e-02\n",
            "Loss: 1.051447e-02\n",
            "Loss: 1.051262e-02\n",
            "Loss: 1.051509e-02\n",
            "Loss: 1.051174e-02\n",
            "Loss: 1.050975e-02\n",
            "Loss: 1.050765e-02\n",
            "Loss: 1.050544e-02\n",
            "Loss: 1.050251e-02\n",
            "Loss: 1.049948e-02\n",
            "Loss: 1.049567e-02\n",
            "Loss: 1.049039e-02\n",
            "Loss: 1.048553e-02\n",
            "Loss: 1.047946e-02\n",
            "Loss: 1.047498e-02\n",
            "Loss: 1.047313e-02\n",
            "Loss: 1.046905e-02\n",
            "Loss: 1.046688e-02\n",
            "Loss: 1.046478e-02\n",
            "Loss: 1.046301e-02\n",
            "Loss: 1.045964e-02\n",
            "Loss: 1.045641e-02\n",
            "Loss: 1.045271e-02\n",
            "Loss: 1.045083e-02\n",
            "Loss: 1.044711e-02\n",
            "Loss: 1.044369e-02\n",
            "Loss: 1.044147e-02\n",
            "Loss: 1.043812e-02\n",
            "Loss: 1.045923e-02\n",
            "Loss: 1.043713e-02\n",
            "Loss: 1.043477e-02\n",
            "Loss: 1.043305e-02\n",
            "Loss: 1.043079e-02\n",
            "Loss: 1.042764e-02\n",
            "Loss: 1.045027e-02\n",
            "Loss: 1.042547e-02\n",
            "Loss: 1.042223e-02\n",
            "Loss: 1.041855e-02\n",
            "Loss: 1.041507e-02\n",
            "Loss: 1.041372e-02\n",
            "Loss: 1.040861e-02\n",
            "Loss: 1.040708e-02\n",
            "Loss: 1.040563e-02\n",
            "Loss: 1.040397e-02\n",
            "Loss: 1.040459e-02\n",
            "Loss: 1.040263e-02\n",
            "Loss: 1.039963e-02\n",
            "Loss: 1.039658e-02\n",
            "Loss: 1.039256e-02\n",
            "Loss: 1.038973e-02\n",
            "Loss: 1.038724e-02\n",
            "Loss: 1.038554e-02\n",
            "Loss: 1.038349e-02\n",
            "Loss: 1.038455e-02\n",
            "Loss: 1.038235e-02\n",
            "Loss: 1.037966e-02\n",
            "Loss: 1.037476e-02\n",
            "Loss: 1.036819e-02\n",
            "Loss: 1.036096e-02\n",
            "Loss: 1.035814e-02\n",
            "Loss: 1.035116e-02\n",
            "Loss: 1.034891e-02\n",
            "Loss: 1.034650e-02\n",
            "Loss: 1.034623e-02\n",
            "Loss: 1.034337e-02\n",
            "Loss: 1.034285e-02\n",
            "Loss: 1.034204e-02\n",
            "Loss: 1.034116e-02\n",
            "Loss: 1.033951e-02\n",
            "Loss: 1.033572e-02\n",
            "Loss: 1.033232e-02\n",
            "Loss: 1.032540e-02\n",
            "Loss: 1.032138e-02\n",
            "Loss: 1.034078e-02\n",
            "Loss: 1.031870e-02\n",
            "Loss: 1.030935e-02\n",
            "Loss: 1.030435e-02\n",
            "Loss: 1.029923e-02\n",
            "Loss: 1.029644e-02\n",
            "Loss: 1.029344e-02\n",
            "Loss: 1.029071e-02\n",
            "Loss: 1.028747e-02\n",
            "Loss: 1.028432e-02\n",
            "Loss: 1.028121e-02\n",
            "Loss: 1.027834e-02\n",
            "Loss: 1.027657e-02\n",
            "Loss: 1.027484e-02\n",
            "Loss: 1.027314e-02\n",
            "Loss: 1.026981e-02\n",
            "Loss: 1.026709e-02\n",
            "Loss: 1.026484e-02\n",
            "Loss: 1.026305e-02\n",
            "Loss: 1.025979e-02\n",
            "Loss: 1.025645e-02\n",
            "Loss: 1.025479e-02\n",
            "Loss: 1.025163e-02\n",
            "Loss: 1.024986e-02\n",
            "Loss: 1.024838e-02\n",
            "Loss: 1.024646e-02\n",
            "Loss: 1.024387e-02\n",
            "Loss: 1.024151e-02\n",
            "Loss: 1.023989e-02\n",
            "Loss: 1.023688e-02\n",
            "Loss: 1.023581e-02\n",
            "Loss: 1.023239e-02\n",
            "Loss: 1.023102e-02\n",
            "Loss: 1.023221e-02\n",
            "Loss: 1.023045e-02\n",
            "Loss: 1.022929e-02\n",
            "Loss: 1.022809e-02\n",
            "Loss: 1.022648e-02\n",
            "Loss: 1.022524e-02\n",
            "Loss: 1.022327e-02\n",
            "Loss: 1.022241e-02\n",
            "Loss: 1.022000e-02\n",
            "Loss: 1.021867e-02\n",
            "Loss: 1.021608e-02\n",
            "Loss: 1.021303e-02\n",
            "Loss: 1.021218e-02\n",
            "Loss: 1.020969e-02\n",
            "Loss: 1.020869e-02\n",
            "Loss: 1.020743e-02\n",
            "Loss: 1.020524e-02\n",
            "Loss: 1.020263e-02\n",
            "Loss: 1.019987e-02\n",
            "Loss: 1.019638e-02\n",
            "Loss: 1.019301e-02\n",
            "Loss: 1.019071e-02\n",
            "Loss: 1.018868e-02\n",
            "Loss: 1.018540e-02\n",
            "Loss: 1.018363e-02\n",
            "Loss: 1.018135e-02\n",
            "Loss: 1.017956e-02\n",
            "Loss: 1.017760e-02\n",
            "Loss: 1.017653e-02\n",
            "Loss: 1.017456e-02\n",
            "Loss: 1.017166e-02\n",
            "Loss: 1.016813e-02\n",
            "Loss: 1.016406e-02\n",
            "Loss: 1.015835e-02\n",
            "Loss: 1.015290e-02\n",
            "Loss: 1.014891e-02\n",
            "Loss: 1.014577e-02\n",
            "Loss: 1.014296e-02\n",
            "Loss: 1.013991e-02\n",
            "Loss: 1.013654e-02\n",
            "Loss: 1.013355e-02\n",
            "Loss: 1.013045e-02\n",
            "Loss: 1.012769e-02\n",
            "Loss: 1.012561e-02\n",
            "Loss: 1.012350e-02\n",
            "Loss: 1.012185e-02\n",
            "Loss: 1.011923e-02\n",
            "Loss: 1.011899e-02\n",
            "Loss: 1.011708e-02\n",
            "Loss: 1.011277e-02\n",
            "Loss: 1.010638e-02\n",
            "Loss: 1.010137e-02\n",
            "Loss: 1.009397e-02\n",
            "Loss: 1.012586e-02\n",
            "Loss: 1.009236e-02\n",
            "Loss: 1.008629e-02\n",
            "Loss: 1.008307e-02\n",
            "Loss: 1.008001e-02\n",
            "Loss: 1.007279e-02\n",
            "Loss: 1.006654e-02\n",
            "Loss: 1.005948e-02\n",
            "Loss: 1.005378e-02\n",
            "Loss: 1.006600e-02\n",
            "Loss: 1.005200e-02\n",
            "Loss: 1.004875e-02\n",
            "Loss: 1.004618e-02\n",
            "Loss: 1.004120e-02\n",
            "Loss: 1.003540e-02\n",
            "Loss: 1.002713e-02\n",
            "Loss: 1.002175e-02\n",
            "Loss: 1.001621e-02\n",
            "Loss: 1.001054e-02\n",
            "Loss: 1.000883e-02\n",
            "Loss: 1.000013e-02\n",
            "Loss: 9.997297e-03\n",
            "Loss: 9.991778e-03\n",
            "Loss: 9.984738e-03\n",
            "Loss: 9.982247e-03\n",
            "Loss: 9.968691e-03\n",
            "Loss: 9.962194e-03\n",
            "Loss: 9.956533e-03\n",
            "Loss: 9.950622e-03\n",
            "Loss: 9.949508e-03\n",
            "Loss: 9.939753e-03\n",
            "Loss: 9.935882e-03\n",
            "Loss: 9.931302e-03\n",
            "Loss: 9.926959e-03\n",
            "Loss: 9.923267e-03\n",
            "Loss: 9.917328e-03\n",
            "Loss: 9.915425e-03\n",
            "Loss: 9.911889e-03\n",
            "Loss: 9.912805e-03\n",
            "Loss: 9.909615e-03\n",
            "Loss: 9.905144e-03\n",
            "Loss: 9.898815e-03\n",
            "Loss: 9.893325e-03\n",
            "Loss: 9.888779e-03\n",
            "Loss: 9.884994e-03\n",
            "Loss: 9.882735e-03\n",
            "Loss: 9.881341e-03\n",
            "Loss: 9.879624e-03\n",
            "Loss: 9.877940e-03\n",
            "Loss: 9.874722e-03\n",
            "Loss: 9.870494e-03\n",
            "Loss: 9.872548e-03\n",
            "Loss: 9.867140e-03\n",
            "Loss: 9.863072e-03\n",
            "Loss: 9.859836e-03\n",
            "Loss: 9.857400e-03\n",
            "Loss: 9.851907e-03\n",
            "Loss: 9.892218e-03\n",
            "Loss: 9.849664e-03\n",
            "Loss: 9.844411e-03\n",
            "Loss: 9.839592e-03\n",
            "Loss: 9.834595e-03\n",
            "Loss: 9.829396e-03\n",
            "Loss: 9.824379e-03\n",
            "Loss: 9.820853e-03\n",
            "Loss: 9.818736e-03\n",
            "Loss: 9.816680e-03\n",
            "Loss: 9.812514e-03\n",
            "Loss: 9.809369e-03\n",
            "Loss: 9.806237e-03\n",
            "Loss: 9.803002e-03\n",
            "Loss: 9.800161e-03\n",
            "Loss: 9.796688e-03\n",
            "Loss: 9.794282e-03\n",
            "Loss: 9.790232e-03\n",
            "Loss: 9.784584e-03\n",
            "Loss: 9.779449e-03\n",
            "Loss: 9.774154e-03\n",
            "Loss: 9.770961e-03\n",
            "Loss: 9.768445e-03\n",
            "Loss: 9.765322e-03\n",
            "Loss: 9.764984e-03\n",
            "Loss: 9.763394e-03\n",
            "Loss: 9.760752e-03\n",
            "Loss: 9.757870e-03\n",
            "Loss: 9.754871e-03\n",
            "Loss: 9.752508e-03\n",
            "Loss: 9.750888e-03\n",
            "Loss: 9.749459e-03\n",
            "Loss: 9.746767e-03\n",
            "Loss: 9.743351e-03\n",
            "Loss: 9.741814e-03\n",
            "Loss: 9.735701e-03\n",
            "Loss: 9.731619e-03\n",
            "Loss: 9.727363e-03\n",
            "Loss: 9.721810e-03\n",
            "Loss: 9.715470e-03\n",
            "Loss: 9.707321e-03\n",
            "Loss: 9.700956e-03\n",
            "Loss: 9.692312e-03\n",
            "Loss: 9.693081e-03\n",
            "Loss: 9.690518e-03\n",
            "Loss: 9.687740e-03\n",
            "Loss: 9.683958e-03\n",
            "Loss: 9.678720e-03\n",
            "Loss: 9.672105e-03\n",
            "Loss: 9.666171e-03\n",
            "Loss: 9.667341e-03\n",
            "Loss: 9.663327e-03\n",
            "Loss: 9.660039e-03\n",
            "Loss: 9.657763e-03\n",
            "Loss: 9.655250e-03\n",
            "Loss: 9.653074e-03\n",
            "Loss: 9.649209e-03\n",
            "Loss: 9.647728e-03\n",
            "Loss: 9.641245e-03\n",
            "Loss: 9.638982e-03\n",
            "Loss: 9.637399e-03\n",
            "Loss: 9.635963e-03\n",
            "Loss: 9.633350e-03\n",
            "Loss: 9.633210e-03\n",
            "Loss: 9.631850e-03\n",
            "Loss: 9.630383e-03\n",
            "Loss: 9.627073e-03\n",
            "Loss: 9.623969e-03\n",
            "Loss: 9.619958e-03\n",
            "Loss: 9.615388e-03\n",
            "Loss: 9.614657e-03\n",
            "Loss: 9.611822e-03\n",
            "Loss: 9.610578e-03\n",
            "Loss: 9.608907e-03\n",
            "Loss: 9.606914e-03\n",
            "Loss: 9.602792e-03\n",
            "Loss: 9.599698e-03\n",
            "Loss: 9.596322e-03\n",
            "Loss: 9.593301e-03\n",
            "Loss: 9.591196e-03\n",
            "Loss: 9.588916e-03\n",
            "Loss: 9.586181e-03\n",
            "Loss: 9.582642e-03\n",
            "Loss: 9.578882e-03\n",
            "Loss: 9.575409e-03\n",
            "Loss: 9.572566e-03\n",
            "Loss: 9.565341e-03\n",
            "Loss: 9.562676e-03\n",
            "Loss: 9.559385e-03\n",
            "Loss: 9.560402e-03\n",
            "Loss: 9.558735e-03\n",
            "Loss: 9.557349e-03\n",
            "Loss: 9.556112e-03\n",
            "Loss: 9.554961e-03\n",
            "Loss: 9.552523e-03\n",
            "Loss: 9.549519e-03\n",
            "Loss: 9.547154e-03\n",
            "Loss: 9.544380e-03\n",
            "Loss: 9.542011e-03\n",
            "Loss: 9.538401e-03\n",
            "Loss: 9.535126e-03\n",
            "Loss: 9.532481e-03\n",
            "Loss: 9.530259e-03\n",
            "Loss: 9.528756e-03\n",
            "Loss: 9.525512e-03\n",
            "Loss: 9.521312e-03\n",
            "Loss: 9.516489e-03\n",
            "Loss: 9.512078e-03\n",
            "Loss: 9.508776e-03\n",
            "Loss: 9.507577e-03\n",
            "Loss: 9.504983e-03\n",
            "Loss: 9.503011e-03\n",
            "Loss: 9.501385e-03\n",
            "Loss: 9.499073e-03\n",
            "Loss: 9.497638e-03\n",
            "Loss: 9.493852e-03\n",
            "Loss: 9.491579e-03\n",
            "Loss: 9.488111e-03\n",
            "Loss: 9.487346e-03\n",
            "Loss: 9.482088e-03\n",
            "Loss: 9.480308e-03\n",
            "Loss: 9.478379e-03\n",
            "Loss: 9.477125e-03\n",
            "Loss: 9.475779e-03\n",
            "Loss: 9.474345e-03\n",
            "Loss: 9.472617e-03\n",
            "Loss: 9.470611e-03\n",
            "Loss: 9.468753e-03\n",
            "Loss: 9.467179e-03\n",
            "Loss: 9.465909e-03\n",
            "Loss: 9.464101e-03\n",
            "Loss: 9.460887e-03\n",
            "Loss: 9.455674e-03\n",
            "Loss: 9.458335e-03\n",
            "Loss: 9.452810e-03\n",
            "Loss: 9.448372e-03\n",
            "Loss: 9.444891e-03\n",
            "Loss: 9.442305e-03\n",
            "Loss: 9.437100e-03\n",
            "Loss: 9.439888e-03\n",
            "Loss: 9.434534e-03\n",
            "Loss: 9.430859e-03\n",
            "Loss: 9.429064e-03\n",
            "Loss: 9.426549e-03\n",
            "Loss: 9.423472e-03\n",
            "Loss: 9.438951e-03\n",
            "Loss: 9.422210e-03\n",
            "Loss: 9.419652e-03\n",
            "Loss: 9.417510e-03\n",
            "Loss: 9.415165e-03\n",
            "Loss: 9.411171e-03\n",
            "Loss: 9.404025e-03\n",
            "Loss: 9.400275e-03\n",
            "Loss: 9.393047e-03\n",
            "Loss: 9.389102e-03\n",
            "Loss: 9.386031e-03\n",
            "Loss: 9.382511e-03\n",
            "Loss: 9.379211e-03\n",
            "Loss: 9.377540e-03\n",
            "Loss: 9.374682e-03\n",
            "Loss: 9.372419e-03\n",
            "Loss: 9.370138e-03\n",
            "Loss: 9.368324e-03\n",
            "Loss: 9.364457e-03\n",
            "Loss: 9.362541e-03\n",
            "Loss: 9.359645e-03\n",
            "Loss: 9.357885e-03\n",
            "Loss: 9.355311e-03\n",
            "Loss: 9.353160e-03\n",
            "Loss: 9.351561e-03\n",
            "Loss: 9.349657e-03\n",
            "Loss: 9.348331e-03\n",
            "Loss: 9.346594e-03\n",
            "Loss: 9.345214e-03\n",
            "Loss: 9.343034e-03\n",
            "Loss: 9.341026e-03\n",
            "Loss: 9.339634e-03\n",
            "Loss: 9.338393e-03\n",
            "Loss: 9.337126e-03\n",
            "Loss: 9.335346e-03\n",
            "Loss: 9.333143e-03\n",
            "Loss: 9.330330e-03\n",
            "Loss: 9.329449e-03\n",
            "Loss: 9.325345e-03\n",
            "Loss: 9.323466e-03\n",
            "Loss: 9.321319e-03\n",
            "Loss: 9.317759e-03\n",
            "Loss: 9.313462e-03\n",
            "Loss: 9.309649e-03\n",
            "Loss: 9.306096e-03\n",
            "Loss: 9.302054e-03\n",
            "Loss: 9.298915e-03\n",
            "Loss: 9.294411e-03\n",
            "Loss: 9.291293e-03\n",
            "Loss: 9.289667e-03\n",
            "Loss: 9.288214e-03\n",
            "Loss: 9.283736e-03\n",
            "Loss: 9.279490e-03\n",
            "Loss: 9.279173e-03\n",
            "Loss: 9.275967e-03\n",
            "Loss: 9.275617e-03\n",
            "Loss: 9.273002e-03\n",
            "Loss: 9.272255e-03\n",
            "Loss: 9.271225e-03\n",
            "Loss: 9.269215e-03\n",
            "Loss: 9.266892e-03\n",
            "Loss: 9.266915e-03\n",
            "Loss: 9.265445e-03\n",
            "Loss: 9.263407e-03\n",
            "Loss: 9.260733e-03\n",
            "Loss: 9.257468e-03\n",
            "Loss: 9.252461e-03\n",
            "Loss: 9.250009e-03\n",
            "Loss: 9.244758e-03\n",
            "Loss: 9.241264e-03\n",
            "Loss: 9.237235e-03\n",
            "Loss: 9.233218e-03\n",
            "Loss: 9.233100e-03\n",
            "Loss: 9.231217e-03\n",
            "Loss: 9.228501e-03\n",
            "Loss: 9.226842e-03\n",
            "Loss: 9.223072e-03\n",
            "Loss: 9.224515e-03\n",
            "Loss: 9.221604e-03\n",
            "Loss: 9.219765e-03\n",
            "Loss: 9.217846e-03\n",
            "Loss: 9.217056e-03\n",
            "Loss: 9.214911e-03\n",
            "Loss: 9.211829e-03\n",
            "Loss: 9.209152e-03\n",
            "Loss: 9.204272e-03\n",
            "Loss: 9.196882e-03\n",
            "Loss: 9.190019e-03\n",
            "Loss: 9.182220e-03\n",
            "Loss: 9.174763e-03\n",
            "Loss: 9.171559e-03\n",
            "Loss: 9.166490e-03\n",
            "Loss: 9.163359e-03\n",
            "Loss: 9.161836e-03\n",
            "Loss: 9.157738e-03\n",
            "Loss: 9.156939e-03\n",
            "Loss: 9.151367e-03\n",
            "Loss: 9.149649e-03\n",
            "Loss: 9.147626e-03\n",
            "Loss: 9.144442e-03\n",
            "Loss: 9.140393e-03\n",
            "Loss: 9.137099e-03\n",
            "Loss: 9.134151e-03\n",
            "Loss: 9.125482e-03\n",
            "Loss: 9.118178e-03\n",
            "Loss: 9.115458e-03\n",
            "Loss: 9.106540e-03\n",
            "Loss: 9.103289e-03\n",
            "Loss: 9.097995e-03\n",
            "Loss: 9.090825e-03\n",
            "Loss: 9.099228e-03\n",
            "Loss: 9.087427e-03\n",
            "Loss: 9.080091e-03\n",
            "Loss: 9.077439e-03\n",
            "Loss: 9.075346e-03\n",
            "Loss: 9.072650e-03\n",
            "Loss: 9.065858e-03\n",
            "Loss: 9.066619e-03\n",
            "Loss: 9.062191e-03\n",
            "Loss: 9.054121e-03\n",
            "Loss: 9.043340e-03\n",
            "Loss: 9.034624e-03\n",
            "Loss: 9.024784e-03\n",
            "Loss: 9.016444e-03\n",
            "Loss: 9.009887e-03\n",
            "Loss: 9.001663e-03\n",
            "Loss: 8.993386e-03\n",
            "Loss: 8.987433e-03\n",
            "Loss: 8.983130e-03\n",
            "Loss: 8.978801e-03\n",
            "Loss: 8.973903e-03\n",
            "Loss: 8.971515e-03\n",
            "Loss: 8.967128e-03\n",
            "Loss: 8.964066e-03\n",
            "Loss: 8.962093e-03\n",
            "Loss: 8.959988e-03\n",
            "Loss: 8.958340e-03\n",
            "Loss: 8.955459e-03\n",
            "Loss: 8.951795e-03\n",
            "Loss: 8.948558e-03\n",
            "Loss: 8.939970e-03\n",
            "Loss: 8.935448e-03\n",
            "Loss: 8.931707e-03\n",
            "Loss: 8.929260e-03\n",
            "Loss: 8.926609e-03\n",
            "Loss: 8.923981e-03\n",
            "Loss: 8.920988e-03\n",
            "Loss: 8.915662e-03\n",
            "Loss: 8.913243e-03\n",
            "Loss: 8.909481e-03\n",
            "Loss: 8.933552e-03\n",
            "Loss: 8.908317e-03\n",
            "Loss: 8.907135e-03\n",
            "Loss: 8.904383e-03\n",
            "Loss: 8.902139e-03\n",
            "Loss: 8.896688e-03\n",
            "Loss: 8.948995e-03\n",
            "Loss: 8.895522e-03\n",
            "Loss: 8.890458e-03\n",
            "Loss: 8.884290e-03\n",
            "Loss: 8.875998e-03\n",
            "Loss: 8.874195e-03\n",
            "Loss: 8.869556e-03\n",
            "Loss: 8.868013e-03\n",
            "Loss: 8.865006e-03\n",
            "Loss: 8.860133e-03\n",
            "Loss: 8.855425e-03\n",
            "Loss: 8.850924e-03\n",
            "Loss: 8.848485e-03\n",
            "Loss: 8.846796e-03\n",
            "Loss: 8.844696e-03\n",
            "Loss: 8.841848e-03\n",
            "Loss: 8.837512e-03\n",
            "Loss: 8.832263e-03\n",
            "Loss: 8.838585e-03\n",
            "Loss: 8.830392e-03\n",
            "Loss: 8.825816e-03\n",
            "Loss: 8.820820e-03\n",
            "Loss: 8.811928e-03\n",
            "Loss: 8.809058e-03\n",
            "Loss: 8.801412e-03\n",
            "Loss: 8.798325e-03\n",
            "Loss: 8.794092e-03\n",
            "Loss: 8.789551e-03\n",
            "Loss: 8.780852e-03\n",
            "Loss: 8.772816e-03\n",
            "Loss: 8.766017e-03\n",
            "Loss: 8.759709e-03\n",
            "Loss: 8.761522e-03\n",
            "Loss: 8.757777e-03\n",
            "Loss: 8.754004e-03\n",
            "Loss: 8.749269e-03\n",
            "Loss: 8.746973e-03\n",
            "Loss: 8.743221e-03\n",
            "Loss: 8.741286e-03\n",
            "Loss: 8.738086e-03\n",
            "Loss: 8.735871e-03\n",
            "Loss: 8.734205e-03\n",
            "Loss: 8.730541e-03\n",
            "Loss: 8.725748e-03\n",
            "Loss: 8.723686e-03\n",
            "Loss: 8.721020e-03\n",
            "Loss: 8.720005e-03\n",
            "Loss: 8.718973e-03\n",
            "Loss: 8.717551e-03\n",
            "Loss: 8.715247e-03\n",
            "Loss: 8.713286e-03\n",
            "Loss: 8.710910e-03\n",
            "Loss: 8.708850e-03\n",
            "Loss: 8.704145e-03\n",
            "Loss: 8.701337e-03\n",
            "Loss: 8.696991e-03\n",
            "Loss: 8.694422e-03\n",
            "Loss: 8.690100e-03\n",
            "Loss: 8.687736e-03\n",
            "Loss: 8.684673e-03\n",
            "Loss: 8.681397e-03\n",
            "Loss: 8.678075e-03\n",
            "Loss: 8.673945e-03\n",
            "Loss: 8.669290e-03\n",
            "Loss: 8.665414e-03\n",
            "Loss: 8.661089e-03\n",
            "Loss: 8.658003e-03\n",
            "Loss: 8.655026e-03\n",
            "Loss: 8.652787e-03\n",
            "Loss: 8.650907e-03\n",
            "Loss: 8.646542e-03\n",
            "Loss: 8.644261e-03\n",
            "Loss: 8.641128e-03\n",
            "Loss: 8.639707e-03\n",
            "Loss: 8.637934e-03\n",
            "Loss: 8.635415e-03\n",
            "Loss: 8.634105e-03\n",
            "Loss: 8.629369e-03\n",
            "Loss: 8.627580e-03\n",
            "Loss: 8.625261e-03\n",
            "Loss: 8.623878e-03\n",
            "Loss: 8.620159e-03\n",
            "Loss: 8.626496e-03\n",
            "Loss: 8.618733e-03\n",
            "Loss: 8.615613e-03\n",
            "Loss: 8.612944e-03\n",
            "Loss: 8.611060e-03\n",
            "Loss: 8.608161e-03\n",
            "Loss: 8.603234e-03\n",
            "Loss: 8.596171e-03\n",
            "Loss: 8.591373e-03\n",
            "Loss: 8.588138e-03\n",
            "Loss: 8.584728e-03\n",
            "Loss: 8.581670e-03\n",
            "Loss: 8.577433e-03\n",
            "Loss: 8.576231e-03\n",
            "Loss: 8.574308e-03\n",
            "Loss: 8.572010e-03\n",
            "Loss: 8.568316e-03\n",
            "Loss: 8.565702e-03\n",
            "Loss: 8.562904e-03\n",
            "Loss: 8.560988e-03\n",
            "Loss: 8.559711e-03\n",
            "Loss: 8.559257e-03\n",
            "Loss: 8.558813e-03\n",
            "Loss: 8.558376e-03\n",
            "Loss: 8.557839e-03\n",
            "Loss: 8.557206e-03\n",
            "Loss: 8.556312e-03\n",
            "Loss: 8.556077e-03\n",
            "Loss: 8.555064e-03\n",
            "Loss: 8.554233e-03\n",
            "Loss: 8.553296e-03\n",
            "Loss: 8.552033e-03\n",
            "Loss: 8.551582e-03\n",
            "Loss: 8.548755e-03\n",
            "Loss: 8.547036e-03\n",
            "Loss: 8.544854e-03\n",
            "Loss: 8.542964e-03\n",
            "Loss: 8.541078e-03\n",
            "Loss: 8.538369e-03\n",
            "Loss: 8.537157e-03\n",
            "Loss: 8.535419e-03\n",
            "Loss: 8.533569e-03\n",
            "Loss: 8.531259e-03\n",
            "Loss: 8.529376e-03\n",
            "Loss: 8.527610e-03\n",
            "Loss: 8.526367e-03\n",
            "Loss: 8.523510e-03\n",
            "Loss: 8.520618e-03\n",
            "Loss: 8.515838e-03\n",
            "Loss: 8.511329e-03\n",
            "Loss: 8.505378e-03\n",
            "Loss: 8.502284e-03\n",
            "Loss: 8.499800e-03\n",
            "Loss: 8.493401e-03\n",
            "Loss: 8.500442e-03\n",
            "Loss: 8.490075e-03\n",
            "Loss: 8.484762e-03\n",
            "Loss: 8.481856e-03\n",
            "Loss: 8.479134e-03\n",
            "Loss: 8.476011e-03\n",
            "Loss: 8.471584e-03\n",
            "Loss: 8.465867e-03\n",
            "Loss: 8.464511e-03\n",
            "Loss: 8.459476e-03\n",
            "Loss: 8.457556e-03\n",
            "Loss: 8.453229e-03\n",
            "Loss: 8.450628e-03\n",
            "Loss: 8.447617e-03\n",
            "Loss: 8.455085e-03\n",
            "Loss: 8.446227e-03\n",
            "Loss: 8.443177e-03\n",
            "Loss: 8.439197e-03\n",
            "Loss: 8.435920e-03\n",
            "Loss: 8.432217e-03\n",
            "Loss: 8.430954e-03\n",
            "Loss: 8.427621e-03\n",
            "Loss: 8.426135e-03\n",
            "Loss: 8.424491e-03\n",
            "Loss: 8.422956e-03\n",
            "Loss: 8.421453e-03\n",
            "Loss: 8.418952e-03\n",
            "Loss: 8.419409e-03\n",
            "Loss: 8.417109e-03\n",
            "Loss: 8.414742e-03\n",
            "Loss: 8.409807e-03\n",
            "Loss: 8.407212e-03\n",
            "Loss: 8.417629e-03\n",
            "Loss: 8.405680e-03\n",
            "Loss: 8.401579e-03\n",
            "Loss: 8.399297e-03\n",
            "Loss: 8.397286e-03\n",
            "Loss: 8.395491e-03\n",
            "Loss: 8.392645e-03\n",
            "Loss: 8.390057e-03\n",
            "Loss: 8.386184e-03\n",
            "Loss: 8.381945e-03\n",
            "Loss: 8.379717e-03\n",
            "Loss: 8.377623e-03\n",
            "Loss: 8.382172e-03\n",
            "Loss: 8.376312e-03\n",
            "Loss: 8.374375e-03\n",
            "Loss: 8.371964e-03\n",
            "Loss: 8.369489e-03\n",
            "Loss: 8.366575e-03\n",
            "Loss: 8.363109e-03\n",
            "Loss: 8.358931e-03\n",
            "Loss: 8.352594e-03\n",
            "Loss: 8.349122e-03\n",
            "Loss: 8.351492e-03\n",
            "Loss: 8.346912e-03\n",
            "Loss: 8.342890e-03\n",
            "Loss: 8.339227e-03\n",
            "Loss: 8.336568e-03\n",
            "Loss: 8.335122e-03\n",
            "Loss: 8.333432e-03\n",
            "Loss: 8.332079e-03\n",
            "Loss: 8.330204e-03\n",
            "Loss: 8.327394e-03\n",
            "Loss: 8.328948e-03\n",
            "Loss: 8.325826e-03\n",
            "Loss: 8.324536e-03\n",
            "Loss: 8.322844e-03\n",
            "Loss: 8.322056e-03\n",
            "Loss: 8.321268e-03\n",
            "Loss: 8.319730e-03\n",
            "Loss: 8.318733e-03\n",
            "Loss: 8.317019e-03\n",
            "Loss: 8.315968e-03\n",
            "Loss: 8.313551e-03\n",
            "Loss: 8.312725e-03\n",
            "Loss: 8.310782e-03\n",
            "Loss: 8.309710e-03\n",
            "Loss: 8.307494e-03\n",
            "Loss: 8.303344e-03\n",
            "Loss: 8.299710e-03\n",
            "Loss: 8.301173e-03\n",
            "Loss: 8.298006e-03\n",
            "Loss: 8.295908e-03\n",
            "Loss: 8.294910e-03\n",
            "Loss: 8.293729e-03\n",
            "Loss: 8.292267e-03\n",
            "Loss: 8.291371e-03\n",
            "Loss: 8.290473e-03\n",
            "Loss: 8.289168e-03\n",
            "Loss: 8.287550e-03\n",
            "Loss: 8.286634e-03\n",
            "Loss: 8.285769e-03\n",
            "Loss: 8.284523e-03\n",
            "Loss: 8.282907e-03\n",
            "Loss: 8.280510e-03\n",
            "Loss: 8.284219e-03\n",
            "Loss: 8.279579e-03\n",
            "Loss: 8.278235e-03\n",
            "Loss: 8.277521e-03\n",
            "Loss: 8.277158e-03\n",
            "Loss: 8.276414e-03\n",
            "Loss: 8.276185e-03\n",
            "Loss: 8.275052e-03\n",
            "Loss: 8.274405e-03\n",
            "Loss: 8.273714e-03\n",
            "Loss: 8.272782e-03\n",
            "Loss: 8.270616e-03\n",
            "Loss: 8.274160e-03\n",
            "Loss: 8.270015e-03\n",
            "Loss: 8.268310e-03\n",
            "Loss: 8.267179e-03\n",
            "Loss: 8.266121e-03\n",
            "Loss: 8.264897e-03\n",
            "Loss: 8.264250e-03\n",
            "Loss: 8.262358e-03\n",
            "Loss: 8.261247e-03\n",
            "Loss: 8.260198e-03\n",
            "Loss: 8.258279e-03\n",
            "Loss: 8.257143e-03\n",
            "Loss: 8.255037e-03\n",
            "Loss: 8.254149e-03\n",
            "Loss: 8.253123e-03\n",
            "Loss: 8.252501e-03\n",
            "Loss: 8.251373e-03\n",
            "Loss: 8.250244e-03\n",
            "Loss: 8.249460e-03\n",
            "Loss: 8.247636e-03\n",
            "Loss: 8.245341e-03\n",
            "Loss: 8.248040e-03\n",
            "Loss: 8.243830e-03\n",
            "Loss: 8.242529e-03\n",
            "Loss: 8.241624e-03\n",
            "Loss: 8.241114e-03\n",
            "Loss: 8.240323e-03\n",
            "Loss: 8.239075e-03\n",
            "Loss: 8.238169e-03\n",
            "Loss: 8.239076e-03\n",
            "Loss: 8.237384e-03\n",
            "Loss: 8.236091e-03\n",
            "Loss: 8.235482e-03\n",
            "Loss: 8.234443e-03\n",
            "Loss: 8.235460e-03\n",
            "Loss: 8.234093e-03\n",
            "Loss: 8.233383e-03\n",
            "Loss: 8.232692e-03\n",
            "Loss: 8.232230e-03\n",
            "Loss: 8.231498e-03\n",
            "Loss: 8.230312e-03\n",
            "Loss: 8.229045e-03\n",
            "Loss: 8.226518e-03\n",
            "Loss: 8.224688e-03\n",
            "Loss: 8.223534e-03\n",
            "Loss: 8.224853e-03\n",
            "Loss: 8.223355e-03\n",
            "Loss: 8.222911e-03\n",
            "Loss: 8.221556e-03\n",
            "Loss: 8.220705e-03\n",
            "Loss: 8.219160e-03\n",
            "Loss: 8.219006e-03\n",
            "Loss: 8.218320e-03\n",
            "Loss: 8.217273e-03\n",
            "Loss: 8.216790e-03\n",
            "Loss: 8.215966e-03\n",
            "Loss: 8.215227e-03\n",
            "Loss: 8.213332e-03\n",
            "Loss: 8.217560e-03\n",
            "Loss: 8.212501e-03\n",
            "Loss: 8.210045e-03\n",
            "Loss: 8.207734e-03\n",
            "Loss: 8.205835e-03\n",
            "Loss: 8.203889e-03\n",
            "Loss: 8.201180e-03\n",
            "Loss: 8.271268e-03\n",
            "Loss: 8.200618e-03\n",
            "Loss: 8.198429e-03\n",
            "Loss: 8.196382e-03\n",
            "Loss: 8.194325e-03\n",
            "Loss: 8.190688e-03\n",
            "Loss: 8.183433e-03\n",
            "Loss: 8.172855e-03\n",
            "Loss: 8.195844e-03\n",
            "Loss: 8.170522e-03\n",
            "Loss: 8.165738e-03\n",
            "Loss: 8.163669e-03\n",
            "Loss: 8.162534e-03\n",
            "Loss: 8.160878e-03\n",
            "Loss: 8.157738e-03\n",
            "Loss: 8.154592e-03\n",
            "Loss: 8.150190e-03\n",
            "Loss: 8.147891e-03\n",
            "Loss: 8.144418e-03\n",
            "Loss: 8.142844e-03\n",
            "Loss: 8.140927e-03\n",
            "Loss: 8.138509e-03\n",
            "Loss: 8.135535e-03\n",
            "Loss: 8.133294e-03\n",
            "Loss: 8.130135e-03\n",
            "Loss: 8.127856e-03\n",
            "Loss: 8.126117e-03\n",
            "Loss: 8.123104e-03\n",
            "Loss: 8.122873e-03\n",
            "Loss: 8.121409e-03\n",
            "Loss: 8.119121e-03\n",
            "Loss: 8.116351e-03\n",
            "Loss: 8.114021e-03\n",
            "Loss: 8.110986e-03\n",
            "Loss: 8.107025e-03\n",
            "Loss: 8.104039e-03\n",
            "Loss: 8.103158e-03\n",
            "Loss: 8.101222e-03\n",
            "Loss: 8.101519e-03\n",
            "Loss: 8.100417e-03\n",
            "Loss: 8.098763e-03\n",
            "Loss: 8.097284e-03\n",
            "Loss: 8.095927e-03\n",
            "Loss: 8.093216e-03\n",
            "Loss: 8.113778e-03\n",
            "Loss: 8.092183e-03\n",
            "Loss: 8.088272e-03\n",
            "Loss: 8.083278e-03\n",
            "Loss: 8.081073e-03\n",
            "Loss: 8.076730e-03\n",
            "Loss: 8.075001e-03\n",
            "Loss: 8.073030e-03\n",
            "Loss: 8.070620e-03\n",
            "Loss: 8.069382e-03\n",
            "Loss: 8.066890e-03\n",
            "Loss: 8.065314e-03\n",
            "Loss: 8.062878e-03\n",
            "Loss: 8.063827e-03\n",
            "Loss: 8.061331e-03\n",
            "Loss: 8.058250e-03\n",
            "Loss: 8.054959e-03\n",
            "Loss: 8.052027e-03\n",
            "Loss: 8.048508e-03\n",
            "Loss: 8.045366e-03\n",
            "Loss: 8.042378e-03\n",
            "Loss: 8.039922e-03\n",
            "Loss: 8.037460e-03\n",
            "Loss: 8.034620e-03\n",
            "Loss: 8.029990e-03\n",
            "Loss: 8.031246e-03\n",
            "Loss: 8.028224e-03\n",
            "Loss: 8.026065e-03\n",
            "Loss: 8.023915e-03\n",
            "Loss: 8.021573e-03\n",
            "Loss: 8.019833e-03\n",
            "Loss: 8.017320e-03\n",
            "Loss: 8.018006e-03\n",
            "Loss: 8.015350e-03\n",
            "Loss: 8.012225e-03\n",
            "Loss: 8.009995e-03\n",
            "Loss: 8.008916e-03\n",
            "Loss: 8.007560e-03\n",
            "Loss: 8.005697e-03\n",
            "Loss: 8.003634e-03\n",
            "Loss: 8.001595e-03\n",
            "Loss: 7.998030e-03\n",
            "Loss: 8.007605e-03\n",
            "Loss: 7.996898e-03\n",
            "Loss: 7.993985e-03\n",
            "Loss: 7.989645e-03\n",
            "Loss: 7.984907e-03\n",
            "Loss: 7.983069e-03\n",
            "Loss: 7.978633e-03\n",
            "Loss: 7.975949e-03\n",
            "Loss: 7.972997e-03\n",
            "Loss: 7.968571e-03\n",
            "Loss: 7.970911e-03\n",
            "Loss: 7.966341e-03\n",
            "Loss: 7.963187e-03\n",
            "Loss: 7.962171e-03\n",
            "Loss: 7.961035e-03\n",
            "Loss: 7.959068e-03\n",
            "Loss: 7.959776e-03\n",
            "Loss: 7.957798e-03\n",
            "Loss: 7.955752e-03\n",
            "Loss: 7.954463e-03\n",
            "Loss: 7.953067e-03\n",
            "Loss: 7.951668e-03\n",
            "Loss: 7.950468e-03\n",
            "Loss: 7.949332e-03\n",
            "Loss: 7.947423e-03\n",
            "Loss: 7.945684e-03\n",
            "Loss: 7.943859e-03\n",
            "Loss: 7.941408e-03\n",
            "Loss: 7.939660e-03\n",
            "Loss: 7.936709e-03\n",
            "Loss: 7.937565e-03\n",
            "Loss: 7.934932e-03\n",
            "Loss: 7.931612e-03\n",
            "Loss: 7.928314e-03\n",
            "Loss: 7.925496e-03\n",
            "Loss: 7.922620e-03\n",
            "Loss: 7.919910e-03\n",
            "Loss: 7.917539e-03\n",
            "Loss: 7.913811e-03\n",
            "Loss: 7.910233e-03\n",
            "Loss: 7.911723e-03\n",
            "Loss: 7.908372e-03\n",
            "Loss: 7.905519e-03\n",
            "Loss: 7.903269e-03\n",
            "Loss: 7.900716e-03\n",
            "Loss: 7.897063e-03\n",
            "Loss: 7.890153e-03\n",
            "Loss: 7.879678e-03\n",
            "Loss: 7.929105e-03\n",
            "Loss: 7.878324e-03\n",
            "Loss: 7.873477e-03\n",
            "Loss: 7.871259e-03\n",
            "Loss: 7.868953e-03\n",
            "Loss: 7.866004e-03\n",
            "Loss: 7.862510e-03\n",
            "Loss: 7.860979e-03\n",
            "Loss: 7.858866e-03\n",
            "Loss: 7.857776e-03\n",
            "Loss: 7.857115e-03\n",
            "Loss: 7.856022e-03\n",
            "Loss: 7.854522e-03\n",
            "Loss: 7.852762e-03\n",
            "Loss: 7.851705e-03\n",
            "Loss: 7.849065e-03\n",
            "Loss: 7.846548e-03\n",
            "Loss: 7.844593e-03\n",
            "Loss: 7.843393e-03\n",
            "Loss: 7.842330e-03\n",
            "Loss: 7.841486e-03\n",
            "Loss: 7.840498e-03\n",
            "Loss: 7.839778e-03\n",
            "Loss: 7.839009e-03\n",
            "Loss: 7.837746e-03\n",
            "Loss: 7.838326e-03\n",
            "Loss: 7.836910e-03\n",
            "Loss: 7.834757e-03\n",
            "Loss: 7.830968e-03\n",
            "Loss: 7.827485e-03\n",
            "Loss: 7.824928e-03\n",
            "Loss: 7.822411e-03\n",
            "Loss: 7.820494e-03\n",
            "Loss: 7.819159e-03\n",
            "Loss: 7.816958e-03\n",
            "Loss: 7.822434e-03\n",
            "Loss: 7.815808e-03\n",
            "Loss: 7.813818e-03\n",
            "Loss: 7.810219e-03\n",
            "Loss: 7.807687e-03\n",
            "Loss: 7.805349e-03\n",
            "Loss: 7.804566e-03\n",
            "Loss: 7.803559e-03\n",
            "Loss: 7.802925e-03\n",
            "Loss: 7.801852e-03\n",
            "Loss: 7.800186e-03\n",
            "Loss: 7.796899e-03\n",
            "Loss: 7.800190e-03\n",
            "Loss: 7.795052e-03\n",
            "Loss: 7.792078e-03\n",
            "Loss: 7.789396e-03\n",
            "Loss: 7.787893e-03\n",
            "Loss: 7.786034e-03\n",
            "Loss: 7.784856e-03\n",
            "Loss: 7.783446e-03\n",
            "Loss: 7.781564e-03\n",
            "Loss: 7.779639e-03\n",
            "Loss: 7.778008e-03\n",
            "Loss: 7.777069e-03\n",
            "Loss: 7.775763e-03\n",
            "Loss: 7.774708e-03\n",
            "Loss: 7.772732e-03\n",
            "Loss: 7.771238e-03\n",
            "Loss: 7.768819e-03\n",
            "Loss: 7.767677e-03\n",
            "Loss: 7.766226e-03\n",
            "Loss: 7.764509e-03\n",
            "Loss: 7.763895e-03\n",
            "Loss: 7.759994e-03\n",
            "Loss: 7.758955e-03\n",
            "Loss: 7.757874e-03\n",
            "Loss: 7.756954e-03\n",
            "Loss: 7.756012e-03\n",
            "Loss: 7.754848e-03\n",
            "Loss: 7.753254e-03\n",
            "Loss: 7.751424e-03\n",
            "Loss: 7.748974e-03\n",
            "Loss: 7.745876e-03\n",
            "Loss: 7.744002e-03\n",
            "Loss: 7.740854e-03\n",
            "Loss: 7.737997e-03\n",
            "Loss: 7.734861e-03\n",
            "Loss: 7.743947e-03\n",
            "Loss: 7.734088e-03\n",
            "Loss: 7.732744e-03\n",
            "Loss: 7.730128e-03\n",
            "Loss: 7.728750e-03\n",
            "Loss: 7.725200e-03\n",
            "Loss: 7.722995e-03\n",
            "Loss: 7.721005e-03\n",
            "Loss: 7.719292e-03\n",
            "Loss: 7.717740e-03\n",
            "Loss: 7.716238e-03\n",
            "Loss: 7.714995e-03\n",
            "Loss: 7.713452e-03\n",
            "Loss: 7.710661e-03\n",
            "Loss: 7.710138e-03\n",
            "Loss: 7.707410e-03\n",
            "Loss: 7.705932e-03\n",
            "Loss: 7.703683e-03\n",
            "Loss: 7.700625e-03\n",
            "Loss: 7.700302e-03\n",
            "Loss: 7.695492e-03\n",
            "Loss: 7.694352e-03\n",
            "Loss: 7.692891e-03\n",
            "Loss: 7.690845e-03\n",
            "Loss: 7.689012e-03\n",
            "Loss: 7.686743e-03\n",
            "Loss: 7.684982e-03\n",
            "Loss: 7.682906e-03\n",
            "Loss: 7.680945e-03\n",
            "Loss: 7.675964e-03\n",
            "Loss: 7.672690e-03\n",
            "Loss: 7.669887e-03\n",
            "Loss: 7.667021e-03\n",
            "Loss: 7.668852e-03\n",
            "Loss: 7.665471e-03\n",
            "Loss: 7.663117e-03\n",
            "Loss: 7.662278e-03\n",
            "Loss: 7.660698e-03\n",
            "Loss: 7.659304e-03\n",
            "Loss: 7.655937e-03\n",
            "Loss: 7.669269e-03\n",
            "Loss: 7.654916e-03\n",
            "Loss: 7.652131e-03\n",
            "Loss: 7.649470e-03\n",
            "Loss: 7.645744e-03\n",
            "Loss: 7.641816e-03\n",
            "Loss: 7.637966e-03\n",
            "Loss: 7.635895e-03\n",
            "Loss: 7.633896e-03\n",
            "Loss: 7.631883e-03\n",
            "Loss: 7.628783e-03\n",
            "Loss: 7.626062e-03\n",
            "Loss: 7.623813e-03\n",
            "Loss: 7.622672e-03\n",
            "Loss: 7.621558e-03\n",
            "Loss: 7.619729e-03\n",
            "Loss: 7.618632e-03\n",
            "Loss: 7.616573e-03\n",
            "Loss: 7.615297e-03\n",
            "Loss: 7.616282e-03\n",
            "Loss: 7.614654e-03\n",
            "Loss: 7.613793e-03\n",
            "Loss: 7.612379e-03\n",
            "Loss: 7.611088e-03\n",
            "Loss: 7.609711e-03\n",
            "Loss: 7.607562e-03\n",
            "Loss: 7.608773e-03\n",
            "Loss: 7.606192e-03\n",
            "Loss: 7.604120e-03\n",
            "Loss: 7.602585e-03\n",
            "Loss: 7.600995e-03\n",
            "Loss: 7.602904e-03\n",
            "Loss: 7.599501e-03\n",
            "Loss: 7.596927e-03\n",
            "Loss: 7.595074e-03\n",
            "Loss: 7.594238e-03\n",
            "Loss: 7.593389e-03\n",
            "Loss: 7.591674e-03\n",
            "Loss: 7.596676e-03\n",
            "Loss: 7.590869e-03\n",
            "Loss: 7.589243e-03\n",
            "Loss: 7.587738e-03\n",
            "Loss: 7.586524e-03\n",
            "Loss: 7.585551e-03\n",
            "Loss: 7.584683e-03\n",
            "Loss: 7.584069e-03\n",
            "Loss: 7.583629e-03\n",
            "Loss: 7.582323e-03\n",
            "Loss: 7.580061e-03\n",
            "Loss: 7.584637e-03\n",
            "Loss: 7.579004e-03\n",
            "Loss: 7.576385e-03\n",
            "Loss: 7.574844e-03\n",
            "Loss: 7.573701e-03\n",
            "Loss: 7.572544e-03\n",
            "Loss: 7.569402e-03\n",
            "Loss: 7.567588e-03\n",
            "Loss: 7.565585e-03\n",
            "Loss: 7.564310e-03\n",
            "Loss: 7.562866e-03\n",
            "Loss: 7.561457e-03\n",
            "Loss: 7.560196e-03\n",
            "Loss: 7.559487e-03\n",
            "Loss: 7.557829e-03\n",
            "Loss: 7.558509e-03\n",
            "Loss: 7.556722e-03\n",
            "Loss: 7.554378e-03\n",
            "Loss: 7.551353e-03\n",
            "Loss: 7.549075e-03\n",
            "Loss: 7.555291e-03\n",
            "Loss: 7.548669e-03\n",
            "Loss: 7.547585e-03\n",
            "Loss: 7.546450e-03\n",
            "Loss: 7.544774e-03\n",
            "Loss: 7.543475e-03\n",
            "Loss: 7.541792e-03\n",
            "Loss: 7.540247e-03\n",
            "Loss: 7.538866e-03\n",
            "Loss: 7.537514e-03\n",
            "Loss: 7.536009e-03\n",
            "Loss: 7.559792e-03\n",
            "Loss: 7.535689e-03\n",
            "Loss: 7.533382e-03\n",
            "Loss: 7.531736e-03\n",
            "Loss: 7.530327e-03\n",
            "Loss: 7.529272e-03\n",
            "Loss: 7.528013e-03\n",
            "Loss: 7.526387e-03\n",
            "Loss: 7.524923e-03\n",
            "Loss: 7.522823e-03\n",
            "Loss: 7.521644e-03\n",
            "Loss: 7.518860e-03\n",
            "Loss: 7.517981e-03\n",
            "Loss: 7.513775e-03\n",
            "Loss: 7.511752e-03\n",
            "Loss: 7.508266e-03\n",
            "Loss: 7.504882e-03\n",
            "Loss: 7.510595e-03\n",
            "Loss: 7.503342e-03\n",
            "Loss: 7.501149e-03\n",
            "Loss: 7.499855e-03\n",
            "Loss: 7.498479e-03\n",
            "Loss: 7.501880e-03\n",
            "Loss: 7.497895e-03\n",
            "Loss: 7.496719e-03\n",
            "Loss: 7.495630e-03\n",
            "Loss: 7.494327e-03\n",
            "Loss: 7.492959e-03\n",
            "Loss: 7.491133e-03\n",
            "Loss: 7.489949e-03\n",
            "Loss: 7.487776e-03\n",
            "Loss: 7.486498e-03\n",
            "Loss: 7.485713e-03\n",
            "Loss: 7.484191e-03\n",
            "Loss: 7.487599e-03\n",
            "Loss: 7.483629e-03\n",
            "Loss: 7.482299e-03\n",
            "Loss: 7.481343e-03\n",
            "Loss: 7.480854e-03\n",
            "Loss: 7.480043e-03\n",
            "Loss: 7.478941e-03\n",
            "Loss: 7.477569e-03\n",
            "Loss: 7.478587e-03\n",
            "Loss: 7.476215e-03\n",
            "Loss: 7.474810e-03\n",
            "Loss: 7.473728e-03\n",
            "Loss: 7.471679e-03\n",
            "Loss: 7.469394e-03\n",
            "Loss: 7.466468e-03\n",
            "Loss: 7.464157e-03\n",
            "Loss: 7.460622e-03\n",
            "Loss: 7.458609e-03\n",
            "Loss: 7.456636e-03\n",
            "Loss: 7.455382e-03\n",
            "Loss: 7.454918e-03\n",
            "Loss: 7.453650e-03\n",
            "Loss: 7.453094e-03\n",
            "Loss: 7.452243e-03\n",
            "Loss: 7.451199e-03\n",
            "Loss: 7.450110e-03\n",
            "Loss: 7.446255e-03\n",
            "Loss: 7.445125e-03\n",
            "Loss: 7.444004e-03\n",
            "Loss: 7.442228e-03\n",
            "Loss: 7.440928e-03\n",
            "Loss: 7.438930e-03\n",
            "Loss: 7.438509e-03\n",
            "Loss: 7.437674e-03\n",
            "Loss: 7.437135e-03\n",
            "Loss: 7.436316e-03\n",
            "Loss: 7.434885e-03\n",
            "Loss: 7.433261e-03\n",
            "Loss: 7.431739e-03\n",
            "Loss: 7.430542e-03\n",
            "Loss: 7.428844e-03\n",
            "Loss: 7.427501e-03\n",
            "Loss: 7.425454e-03\n",
            "Loss: 7.425633e-03\n",
            "Loss: 7.424616e-03\n",
            "Loss: 7.422969e-03\n",
            "Loss: 7.420892e-03\n",
            "Loss: 7.418186e-03\n",
            "Loss: 7.416204e-03\n",
            "Loss: 7.413758e-03\n",
            "Loss: 7.411576e-03\n",
            "Loss: 7.409689e-03\n",
            "Loss: 7.408209e-03\n",
            "Loss: 7.404300e-03\n",
            "Loss: 7.402114e-03\n",
            "Loss: 7.400357e-03\n",
            "Loss: 7.399494e-03\n",
            "Loss: 7.398151e-03\n",
            "Loss: 7.397221e-03\n",
            "Loss: 7.396295e-03\n",
            "Loss: 7.395283e-03\n",
            "Loss: 7.392553e-03\n",
            "Loss: 7.393660e-03\n",
            "Loss: 7.391403e-03\n",
            "Loss: 7.389662e-03\n",
            "Loss: 7.388390e-03\n",
            "Loss: 7.387356e-03\n",
            "Loss: 7.385764e-03\n",
            "Loss: 7.383811e-03\n",
            "Loss: 7.382805e-03\n",
            "Loss: 7.380983e-03\n",
            "Loss: 7.378836e-03\n",
            "Loss: 7.377533e-03\n",
            "Loss: 7.375875e-03\n",
            "Loss: 7.373670e-03\n",
            "Loss: 7.378970e-03\n",
            "Loss: 7.372549e-03\n",
            "Loss: 7.371194e-03\n",
            "Loss: 7.369918e-03\n",
            "Loss: 7.369345e-03\n",
            "Loss: 7.368425e-03\n",
            "Loss: 7.367149e-03\n",
            "Loss: 7.366325e-03\n",
            "Loss: 7.365459e-03\n",
            "Loss: 7.365071e-03\n",
            "Loss: 7.363420e-03\n",
            "Loss: 7.361830e-03\n",
            "Loss: 7.359460e-03\n",
            "Loss: 7.356176e-03\n",
            "Loss: 7.353894e-03\n",
            "Loss: 7.352577e-03\n",
            "Loss: 7.350401e-03\n",
            "Loss: 7.349024e-03\n",
            "Loss: 7.348199e-03\n",
            "Loss: 7.347144e-03\n",
            "Loss: 7.345761e-03\n",
            "Loss: 7.343806e-03\n",
            "Loss: 7.341397e-03\n",
            "Loss: 7.339031e-03\n",
            "Loss: 7.339077e-03\n",
            "Loss: 7.337381e-03\n",
            "Loss: 7.335230e-03\n",
            "Loss: 7.332724e-03\n",
            "Loss: 7.330047e-03\n",
            "Loss: 7.326126e-03\n",
            "Loss: 7.321944e-03\n",
            "Loss: 7.318740e-03\n",
            "Loss: 7.315694e-03\n",
            "Loss: 7.313687e-03\n",
            "Loss: 7.307882e-03\n",
            "Loss: 7.317864e-03\n",
            "Loss: 7.305291e-03\n",
            "Loss: 7.301371e-03\n",
            "Loss: 7.297495e-03\n",
            "Loss: 7.299319e-03\n",
            "Loss: 7.296470e-03\n",
            "Loss: 7.294723e-03\n",
            "Loss: 7.291723e-03\n",
            "Loss: 7.289450e-03\n",
            "Loss: 7.285402e-03\n",
            "Loss: 7.291869e-03\n",
            "Loss: 7.284069e-03\n",
            "Loss: 7.281531e-03\n",
            "Loss: 7.278791e-03\n",
            "Loss: 7.276720e-03\n",
            "Loss: 7.276060e-03\n",
            "Loss: 7.274233e-03\n",
            "Loss: 7.273341e-03\n",
            "Loss: 7.272135e-03\n",
            "Loss: 7.270559e-03\n",
            "Loss: 7.269010e-03\n",
            "Loss: 7.267354e-03\n",
            "Loss: 7.265799e-03\n",
            "Loss: 7.263483e-03\n",
            "Loss: 7.262134e-03\n",
            "Loss: 7.260147e-03\n",
            "Loss: 7.258703e-03\n",
            "Loss: 7.257591e-03\n",
            "Loss: 7.254476e-03\n",
            "Loss: 7.251179e-03\n",
            "Loss: 7.245940e-03\n",
            "Loss: 7.247094e-03\n",
            "Loss: 7.244136e-03\n",
            "Loss: 7.242656e-03\n",
            "Loss: 7.241569e-03\n",
            "Loss: 7.240338e-03\n",
            "Loss: 7.238500e-03\n",
            "Loss: 7.236822e-03\n",
            "Loss: 7.235776e-03\n",
            "Loss: 7.234423e-03\n",
            "Loss: 7.233565e-03\n",
            "Loss: 7.231996e-03\n",
            "Loss: 7.231149e-03\n",
            "Loss: 7.229230e-03\n",
            "Loss: 7.226578e-03\n",
            "Loss: 7.226347e-03\n",
            "Loss: 7.225028e-03\n",
            "Loss: 7.223073e-03\n",
            "Loss: 7.220322e-03\n",
            "Loss: 7.218046e-03\n",
            "Loss: 7.215362e-03\n",
            "Loss: 7.212509e-03\n",
            "Loss: 7.208944e-03\n",
            "Loss: 7.206233e-03\n",
            "Loss: 7.203434e-03\n",
            "Loss: 7.200364e-03\n",
            "Loss: 7.197845e-03\n",
            "Loss: 7.194301e-03\n",
            "Loss: 7.192211e-03\n",
            "Loss: 7.189540e-03\n",
            "Loss: 7.187406e-03\n",
            "Loss: 7.184516e-03\n",
            "Loss: 7.180730e-03\n",
            "Loss: 7.176812e-03\n",
            "Loss: 7.173686e-03\n",
            "Loss: 7.172689e-03\n",
            "Loss: 7.171710e-03\n",
            "Loss: 7.169952e-03\n",
            "Loss: 7.168613e-03\n",
            "Loss: 7.165771e-03\n",
            "Loss: 7.164211e-03\n",
            "Loss: 7.160660e-03\n",
            "Loss: 7.158583e-03\n",
            "Loss: 7.156874e-03\n",
            "Loss: 7.153823e-03\n",
            "Loss: 7.163281e-03\n",
            "Loss: 7.153118e-03\n",
            "Loss: 7.151957e-03\n",
            "Loss: 7.150102e-03\n",
            "Loss: 7.148939e-03\n",
            "Loss: 7.147589e-03\n",
            "Loss: 7.146238e-03\n",
            "Loss: 7.145406e-03\n",
            "Loss: 7.144558e-03\n",
            "Loss: 7.143818e-03\n",
            "Loss: 7.142133e-03\n",
            "Loss: 7.147211e-03\n",
            "Loss: 7.141605e-03\n",
            "Loss: 7.140378e-03\n",
            "Loss: 7.139419e-03\n",
            "Loss: 7.138573e-03\n",
            "Loss: 7.137580e-03\n",
            "Loss: 7.143042e-03\n",
            "Loss: 7.136932e-03\n",
            "Loss: 7.135354e-03\n",
            "Loss: 7.133661e-03\n",
            "Loss: 7.132350e-03\n",
            "Loss: 7.130440e-03\n",
            "Loss: 7.143617e-03\n",
            "Loss: 7.129373e-03\n",
            "Loss: 7.126979e-03\n",
            "Loss: 7.123921e-03\n",
            "Loss: 7.121699e-03\n",
            "Loss: 7.117754e-03\n",
            "Loss: 7.126882e-03\n",
            "Loss: 7.115485e-03\n",
            "Loss: 7.110496e-03\n",
            "Loss: 7.107161e-03\n",
            "Loss: 7.103755e-03\n",
            "Loss: 7.100787e-03\n",
            "Loss: 7.095512e-03\n",
            "Loss: 7.092142e-03\n",
            "Loss: 7.088830e-03\n",
            "Loss: 7.085733e-03\n",
            "Loss: 7.083981e-03\n",
            "Loss: 7.081201e-03\n",
            "Loss: 7.079898e-03\n",
            "Loss: 7.078716e-03\n",
            "Loss: 7.076111e-03\n",
            "Loss: 7.075164e-03\n",
            "Loss: 7.073099e-03\n",
            "Loss: 7.070522e-03\n",
            "Loss: 7.067996e-03\n",
            "Loss: 7.066081e-03\n",
            "Loss: 7.064333e-03\n",
            "Loss: 7.061889e-03\n",
            "Loss: 7.057972e-03\n",
            "Loss: 7.056403e-03\n",
            "Loss: 7.055344e-03\n",
            "Loss: 7.054277e-03\n",
            "Loss: 7.053153e-03\n",
            "Loss: 7.051289e-03\n",
            "Loss: 7.054226e-03\n",
            "Loss: 7.050887e-03\n",
            "Loss: 7.049598e-03\n",
            "Loss: 7.048757e-03\n",
            "Loss: 7.047684e-03\n",
            "Loss: 7.046296e-03\n",
            "Loss: 7.055382e-03\n",
            "Loss: 7.045506e-03\n",
            "Loss: 7.043594e-03\n",
            "Loss: 7.040793e-03\n",
            "Loss: 7.038404e-03\n",
            "Loss: 7.042590e-03\n",
            "Loss: 7.037545e-03\n",
            "Loss: 7.035953e-03\n",
            "Loss: 7.034149e-03\n",
            "Loss: 7.032607e-03\n",
            "Loss: 7.031396e-03\n",
            "Loss: 7.031546e-03\n",
            "Loss: 7.030644e-03\n",
            "Loss: 7.029102e-03\n",
            "Loss: 7.027870e-03\n",
            "Loss: 7.025894e-03\n",
            "Loss: 7.023559e-03\n",
            "Loss: 7.020087e-03\n",
            "Loss: 7.017264e-03\n",
            "Loss: 7.015176e-03\n",
            "Loss: 7.013371e-03\n",
            "Loss: 7.011749e-03\n",
            "Loss: 7.008081e-03\n",
            "Loss: 7.011703e-03\n",
            "Loss: 7.006717e-03\n",
            "Loss: 7.004272e-03\n",
            "Loss: 7.002576e-03\n",
            "Loss: 7.000615e-03\n",
            "Loss: 6.997095e-03\n",
            "Loss: 7.001095e-03\n",
            "Loss: 6.994641e-03\n",
            "Loss: 6.990134e-03\n",
            "Loss: 6.987427e-03\n",
            "Loss: 6.985859e-03\n",
            "Loss: 6.984333e-03\n",
            "Loss: 6.983195e-03\n",
            "Loss: 6.981459e-03\n",
            "Loss: 6.980020e-03\n",
            "Loss: 6.978539e-03\n",
            "Loss: 6.977134e-03\n",
            "Loss: 6.975003e-03\n",
            "Loss: 6.972942e-03\n",
            "Loss: 6.971496e-03\n",
            "Loss: 6.968640e-03\n",
            "Loss: 6.973322e-03\n",
            "Loss: 6.967380e-03\n",
            "Loss: 6.966148e-03\n",
            "Loss: 6.964750e-03\n",
            "Loss: 6.963891e-03\n",
            "Loss: 6.960567e-03\n",
            "Loss: 6.959259e-03\n",
            "Loss: 6.957102e-03\n",
            "Loss: 6.954617e-03\n",
            "Loss: 6.957422e-03\n",
            "Loss: 6.953599e-03\n",
            "Loss: 6.952337e-03\n",
            "Loss: 6.952030e-03\n",
            "Loss: 6.950936e-03\n",
            "Loss: 6.949460e-03\n",
            "Loss: 6.948272e-03\n",
            "Loss: 6.947064e-03\n",
            "Loss: 6.945749e-03\n",
            "Loss: 6.944628e-03\n",
            "Loss: 6.942806e-03\n",
            "Loss: 6.941773e-03\n",
            "Loss: 6.939984e-03\n",
            "Loss: 6.938905e-03\n",
            "Loss: 6.937736e-03\n",
            "Loss: 6.935950e-03\n",
            "Loss: 6.957433e-03\n",
            "Loss: 6.935203e-03\n",
            "Loss: 6.933035e-03\n",
            "Loss: 6.931019e-03\n",
            "Loss: 6.929198e-03\n",
            "Loss: 6.927575e-03\n",
            "Loss: 6.930763e-03\n",
            "Loss: 6.926921e-03\n",
            "Loss: 6.925647e-03\n",
            "Loss: 6.924340e-03\n",
            "Loss: 6.923181e-03\n",
            "Loss: 6.921411e-03\n",
            "Loss: 6.918686e-03\n",
            "Loss: 6.917110e-03\n",
            "Loss: 6.914624e-03\n",
            "Loss: 6.912965e-03\n",
            "Loss: 6.910635e-03\n",
            "Loss: 6.908187e-03\n",
            "Loss: 6.906318e-03\n",
            "Loss: 6.904559e-03\n",
            "Loss: 6.903597e-03\n",
            "Loss: 6.901707e-03\n",
            "Loss: 6.899507e-03\n",
            "Loss: 6.895916e-03\n",
            "Loss: 6.893646e-03\n",
            "Loss: 6.891330e-03\n",
            "Loss: 6.888438e-03\n",
            "Loss: 6.884359e-03\n",
            "Loss: 6.880294e-03\n",
            "Loss: 6.876367e-03\n",
            "Loss: 6.873489e-03\n",
            "Loss: 6.875719e-03\n",
            "Loss: 6.871932e-03\n",
            "Loss: 6.869785e-03\n",
            "Loss: 6.867349e-03\n",
            "Loss: 6.866125e-03\n",
            "Loss: 6.865012e-03\n",
            "Loss: 6.862128e-03\n",
            "Loss: 6.860120e-03\n",
            "Loss: 6.857615e-03\n",
            "Loss: 6.855375e-03\n",
            "Loss: 6.854106e-03\n",
            "Loss: 6.847852e-03\n",
            "Loss: 6.845843e-03\n",
            "Loss: 6.842207e-03\n",
            "Loss: 6.839692e-03\n",
            "Loss: 6.836937e-03\n",
            "Loss: 6.834048e-03\n",
            "Loss: 6.831052e-03\n",
            "Loss: 6.828248e-03\n",
            "Loss: 6.828461e-03\n",
            "Loss: 6.825736e-03\n",
            "Loss: 6.823246e-03\n",
            "Loss: 6.820594e-03\n",
            "Loss: 6.818303e-03\n",
            "Loss: 6.815041e-03\n",
            "Loss: 6.811144e-03\n",
            "Loss: 6.809682e-03\n",
            "Loss: 6.807548e-03\n",
            "Loss: 6.803814e-03\n",
            "Loss: 6.802026e-03\n",
            "Loss: 6.800285e-03\n",
            "Loss: 6.798485e-03\n",
            "Loss: 6.796943e-03\n",
            "Loss: 6.793454e-03\n",
            "Loss: 6.791918e-03\n",
            "Loss: 6.789711e-03\n",
            "Loss: 6.787434e-03\n",
            "Loss: 6.783821e-03\n",
            "Loss: 6.779655e-03\n",
            "Loss: 6.776998e-03\n",
            "Loss: 6.772139e-03\n",
            "Loss: 6.765587e-03\n",
            "Loss: 6.761051e-03\n",
            "Loss: 6.758131e-03\n",
            "Loss: 6.755526e-03\n",
            "Loss: 6.753775e-03\n",
            "Loss: 6.750599e-03\n",
            "Loss: 6.747940e-03\n",
            "Loss: 6.748348e-03\n",
            "Loss: 6.746438e-03\n",
            "Loss: 6.744435e-03\n",
            "Loss: 6.743654e-03\n",
            "Loss: 6.742606e-03\n",
            "Loss: 6.741539e-03\n",
            "Loss: 6.739893e-03\n",
            "Loss: 6.737971e-03\n",
            "Loss: 6.736295e-03\n",
            "Loss: 6.734033e-03\n",
            "Loss: 6.731452e-03\n",
            "Loss: 6.728735e-03\n",
            "Loss: 6.767768e-03\n",
            "Loss: 6.727776e-03\n",
            "Loss: 6.726108e-03\n",
            "Loss: 6.724613e-03\n",
            "Loss: 6.723955e-03\n",
            "Loss: 6.723163e-03\n",
            "Loss: 6.763488e-03\n",
            "Loss: 6.722994e-03\n",
            "Loss: 6.722513e-03\n",
            "Loss: 6.721708e-03\n",
            "Loss: 6.720676e-03\n",
            "Loss: 6.719394e-03\n",
            "Loss: 6.717911e-03\n",
            "Loss: 6.717229e-03\n",
            "Loss: 6.714879e-03\n",
            "Loss: 6.716590e-03\n",
            "Loss: 6.713985e-03\n",
            "Loss: 6.712600e-03\n",
            "Loss: 6.710852e-03\n",
            "Loss: 6.708994e-03\n",
            "Loss: 6.712060e-03\n",
            "Loss: 6.708150e-03\n",
            "Loss: 6.706180e-03\n",
            "Loss: 6.704430e-03\n",
            "Loss: 6.702719e-03\n",
            "Loss: 6.700941e-03\n",
            "Loss: 6.698330e-03\n",
            "Loss: 6.702416e-03\n",
            "Loss: 6.697558e-03\n",
            "Loss: 6.695764e-03\n",
            "Loss: 6.694456e-03\n",
            "Loss: 6.692547e-03\n",
            "Loss: 6.690478e-03\n",
            "Loss: 6.687514e-03\n",
            "Loss: 6.684595e-03\n",
            "Loss: 6.682132e-03\n",
            "Loss: 6.680498e-03\n",
            "Loss: 6.679059e-03\n",
            "Loss: 6.678162e-03\n",
            "Loss: 6.676855e-03\n",
            "Loss: 6.676124e-03\n",
            "Loss: 6.674778e-03\n",
            "Loss: 6.673492e-03\n",
            "Loss: 6.672390e-03\n",
            "Loss: 6.670857e-03\n",
            "Loss: 6.669457e-03\n",
            "Loss: 6.671431e-03\n",
            "Loss: 6.668577e-03\n",
            "Loss: 6.666637e-03\n",
            "Loss: 6.665230e-03\n",
            "Loss: 6.663042e-03\n",
            "Loss: 6.661426e-03\n",
            "Loss: 6.663902e-03\n",
            "Loss: 6.660891e-03\n",
            "Loss: 6.659976e-03\n",
            "Loss: 6.659270e-03\n",
            "Loss: 6.657548e-03\n",
            "Loss: 6.656227e-03\n",
            "Loss: 6.654910e-03\n",
            "Loss: 6.652677e-03\n",
            "Loss: 6.651198e-03\n",
            "Loss: 6.649541e-03\n",
            "Loss: 6.647295e-03\n",
            "Loss: 6.646070e-03\n",
            "Loss: 6.679240e-03\n",
            "Loss: 6.645492e-03\n",
            "Loss: 6.643002e-03\n",
            "Loss: 6.640961e-03\n",
            "Loss: 6.640547e-03\n",
            "Loss: 6.637804e-03\n",
            "Loss: 6.637156e-03\n",
            "Loss: 6.636221e-03\n",
            "Loss: 6.635287e-03\n",
            "Loss: 6.634502e-03\n",
            "Loss: 6.633683e-03\n",
            "Loss: 6.632918e-03\n",
            "Loss: 6.631942e-03\n",
            "Loss: 6.631496e-03\n",
            "Loss: 6.629919e-03\n",
            "Loss: 6.627854e-03\n",
            "Loss: 6.627141e-03\n",
            "Loss: 6.625329e-03\n",
            "Loss: 6.624660e-03\n",
            "Loss: 6.623126e-03\n",
            "Loss: 6.621530e-03\n",
            "Loss: 6.618399e-03\n",
            "Loss: 6.615634e-03\n",
            "Loss: 6.613379e-03\n",
            "Loss: 6.611963e-03\n",
            "Loss: 6.614006e-03\n",
            "Loss: 6.610999e-03\n",
            "Loss: 6.609868e-03\n",
            "Loss: 6.608980e-03\n",
            "Loss: 6.607415e-03\n",
            "Loss: 6.605724e-03\n",
            "Loss: 6.607870e-03\n",
            "Loss: 6.604611e-03\n",
            "Loss: 6.603168e-03\n",
            "Loss: 6.601517e-03\n",
            "Loss: 6.600653e-03\n",
            "Loss: 6.599121e-03\n",
            "Loss: 6.597452e-03\n",
            "Loss: 6.595798e-03\n",
            "Loss: 6.593956e-03\n",
            "Loss: 6.592742e-03\n",
            "Loss: 6.592066e-03\n",
            "Loss: 6.590740e-03\n",
            "Loss: 6.590217e-03\n",
            "Loss: 6.589122e-03\n",
            "Loss: 6.587921e-03\n",
            "Loss: 6.586631e-03\n",
            "Loss: 6.585170e-03\n",
            "Loss: 6.581655e-03\n",
            "Loss: 6.579763e-03\n",
            "Loss: 6.581533e-03\n",
            "Loss: 6.578651e-03\n",
            "Loss: 6.577543e-03\n",
            "Loss: 6.576862e-03\n",
            "Loss: 6.575258e-03\n",
            "Loss: 6.572925e-03\n",
            "Loss: 6.578463e-03\n",
            "Loss: 6.571549e-03\n",
            "Loss: 6.569487e-03\n",
            "Loss: 6.568182e-03\n",
            "Loss: 6.567374e-03\n",
            "Loss: 6.565661e-03\n",
            "Loss: 6.569435e-03\n",
            "Loss: 6.564843e-03\n",
            "Loss: 6.563695e-03\n",
            "Loss: 6.562965e-03\n",
            "Loss: 6.562237e-03\n",
            "Loss: 6.560887e-03\n",
            "Loss: 6.558829e-03\n",
            "Loss: 6.561113e-03\n",
            "Loss: 6.557533e-03\n",
            "Loss: 6.555977e-03\n",
            "Loss: 6.554383e-03\n",
            "Loss: 6.552990e-03\n",
            "Loss: 6.552543e-03\n",
            "Loss: 6.550484e-03\n",
            "Loss: 6.549384e-03\n",
            "Loss: 6.548242e-03\n",
            "Loss: 6.547547e-03\n",
            "Loss: 6.546753e-03\n",
            "Loss: 6.545982e-03\n",
            "Loss: 6.544971e-03\n",
            "Loss: 6.543911e-03\n",
            "Loss: 6.542947e-03\n",
            "Loss: 6.542234e-03\n",
            "Loss: 6.541238e-03\n",
            "Loss: 6.540512e-03\n",
            "Loss: 6.539757e-03\n",
            "Loss: 6.538622e-03\n",
            "Loss: 6.537177e-03\n",
            "Loss: 6.538803e-03\n",
            "Loss: 6.536635e-03\n",
            "Loss: 6.535117e-03\n",
            "Loss: 6.533880e-03\n",
            "Loss: 6.532480e-03\n",
            "Loss: 6.531940e-03\n",
            "Loss: 6.530623e-03\n",
            "Loss: 6.529587e-03\n",
            "Loss: 6.528756e-03\n",
            "Loss: 6.528014e-03\n",
            "Loss: 6.527144e-03\n",
            "Loss: 6.526776e-03\n",
            "Loss: 6.526195e-03\n",
            "Loss: 6.526381e-03\n",
            "Loss: 6.525865e-03\n",
            "Loss: 6.525281e-03\n",
            "Loss: 6.524667e-03\n",
            "Loss: 6.524123e-03\n",
            "Loss: 6.526223e-03\n",
            "Loss: 6.523994e-03\n",
            "Loss: 6.523656e-03\n",
            "Loss: 6.523206e-03\n",
            "Loss: 6.521952e-03\n",
            "Loss: 6.521294e-03\n",
            "Loss: 6.525496e-03\n",
            "Loss: 6.521140e-03\n",
            "Loss: 6.520913e-03\n",
            "Loss: 6.520379e-03\n",
            "Loss: 6.519648e-03\n",
            "Loss: 6.518934e-03\n",
            "Loss: 6.518540e-03\n",
            "Loss: 6.516973e-03\n",
            "Loss: 6.516371e-03\n",
            "Loss: 6.515739e-03\n",
            "Loss: 6.515225e-03\n",
            "Loss: 6.514753e-03\n",
            "Loss: 6.514541e-03\n",
            "Loss: 6.513568e-03\n",
            "Loss: 6.513082e-03\n",
            "Loss: 6.512475e-03\n",
            "Loss: 6.511481e-03\n",
            "Loss: 6.510701e-03\n",
            "Loss: 6.510242e-03\n",
            "Loss: 6.509303e-03\n",
            "Loss: 6.508756e-03\n",
            "Loss: 6.508098e-03\n",
            "Loss: 6.507251e-03\n",
            "Loss: 6.506323e-03\n",
            "Loss: 6.505485e-03\n",
            "Loss: 6.504720e-03\n",
            "Loss: 6.503974e-03\n",
            "Loss: 6.503155e-03\n",
            "Loss: 6.502499e-03\n",
            "Loss: 6.501873e-03\n",
            "Loss: 6.501334e-03\n",
            "Loss: 6.500668e-03\n",
            "Loss: 6.500145e-03\n",
            "Loss: 6.499446e-03\n",
            "Loss: 6.498698e-03\n",
            "Loss: 6.496476e-03\n",
            "Loss: 6.494332e-03\n",
            "Loss: 6.492471e-03\n",
            "Loss: 6.490794e-03\n",
            "Loss: 6.489228e-03\n",
            "Loss: 6.487600e-03\n",
            "Loss: 6.484619e-03\n",
            "Loss: 6.503745e-03\n",
            "Loss: 6.484166e-03\n",
            "Loss: 6.482642e-03\n",
            "Loss: 6.481660e-03\n",
            "Loss: 6.480739e-03\n",
            "Loss: 6.479951e-03\n",
            "Loss: 6.479010e-03\n",
            "Loss: 6.478814e-03\n",
            "Loss: 6.478023e-03\n",
            "Loss: 6.477431e-03\n",
            "Loss: 6.476749e-03\n",
            "Loss: 6.476656e-03\n",
            "Loss: 6.475326e-03\n",
            "Loss: 6.474717e-03\n",
            "Loss: 6.474179e-03\n",
            "Loss: 6.473836e-03\n",
            "Loss: 6.473639e-03\n",
            "Loss: 6.472385e-03\n",
            "Loss: 6.472117e-03\n",
            "Loss: 6.471651e-03\n",
            "Loss: 6.472629e-03\n",
            "Loss: 6.471149e-03\n",
            "Loss: 6.470308e-03\n",
            "Loss: 6.469350e-03\n",
            "Loss: 6.468284e-03\n",
            "Loss: 6.467796e-03\n",
            "Loss: 6.467037e-03\n",
            "Loss: 6.466692e-03\n",
            "Loss: 6.466241e-03\n",
            "Loss: 6.465967e-03\n",
            "Loss: 6.465257e-03\n",
            "Loss: 6.464479e-03\n",
            "Loss: 6.467596e-03\n",
            "Loss: 6.464240e-03\n",
            "Loss: 6.463361e-03\n",
            "Loss: 6.462676e-03\n",
            "Loss: 6.462469e-03\n",
            "Loss: 6.462104e-03\n",
            "Loss: 6.461583e-03\n",
            "Loss: 6.460881e-03\n",
            "Loss: 6.460124e-03\n",
            "Loss: 6.459055e-03\n",
            "Loss: 6.458458e-03\n",
            "Loss: 6.457507e-03\n",
            "Loss: 6.456561e-03\n",
            "Loss: 6.455530e-03\n",
            "Loss: 6.454722e-03\n",
            "Loss: 6.454049e-03\n",
            "Loss: 6.453431e-03\n",
            "Loss: 6.453054e-03\n",
            "Loss: 6.452784e-03\n",
            "Loss: 6.452436e-03\n",
            "Loss: 6.451717e-03\n",
            "Loss: 6.453448e-03\n",
            "Loss: 6.451480e-03\n",
            "Loss: 6.450770e-03\n",
            "Loss: 6.450478e-03\n",
            "Loss: 6.449727e-03\n",
            "Loss: 6.449200e-03\n",
            "Loss: 6.448411e-03\n",
            "Loss: 6.447691e-03\n",
            "Loss: 6.447420e-03\n",
            "Loss: 6.446845e-03\n",
            "Loss: 6.446353e-03\n",
            "Loss: 6.445440e-03\n",
            "Loss: 6.444708e-03\n",
            "Loss: 6.445106e-03\n",
            "Loss: 6.444111e-03\n",
            "Loss: 6.443589e-03\n",
            "Loss: 6.443437e-03\n",
            "Loss: 6.443120e-03\n",
            "Loss: 6.442677e-03\n",
            "Loss: 6.441680e-03\n",
            "Loss: 6.440747e-03\n",
            "Loss: 6.439850e-03\n",
            "Loss: 6.439016e-03\n",
            "Loss: 6.437828e-03\n",
            "Loss: 6.437305e-03\n",
            "Loss: 6.436625e-03\n",
            "Loss: 6.435699e-03\n",
            "Loss: 6.434580e-03\n",
            "Loss: 6.435275e-03\n",
            "Loss: 6.433890e-03\n",
            "Loss: 6.433207e-03\n",
            "Loss: 6.432946e-03\n",
            "Loss: 6.432680e-03\n",
            "Loss: 6.431774e-03\n",
            "Loss: 6.431186e-03\n",
            "Loss: 6.429920e-03\n",
            "Loss: 6.428825e-03\n",
            "Loss: 6.427703e-03\n",
            "Loss: 6.426806e-03\n",
            "Loss: 6.425639e-03\n",
            "Loss: 6.425048e-03\n",
            "Loss: 6.423932e-03\n",
            "Loss: 6.423355e-03\n",
            "Loss: 6.422412e-03\n",
            "Loss: 6.421408e-03\n",
            "Loss: 6.420774e-03\n",
            "Loss: 6.419155e-03\n",
            "Loss: 6.418412e-03\n",
            "Loss: 6.417291e-03\n",
            "Loss: 6.415545e-03\n",
            "Loss: 6.412787e-03\n",
            "Loss: 6.425600e-03\n",
            "Loss: 6.411948e-03\n",
            "Loss: 6.410412e-03\n",
            "Loss: 6.409040e-03\n",
            "Loss: 6.407348e-03\n",
            "Loss: 6.406153e-03\n",
            "Loss: 6.403783e-03\n",
            "Loss: 6.402459e-03\n",
            "Loss: 6.400227e-03\n",
            "Loss: 6.398809e-03\n",
            "Loss: 6.397045e-03\n",
            "Loss: 6.394877e-03\n",
            "Loss: 6.392919e-03\n",
            "Loss: 6.391574e-03\n",
            "Loss: 6.389517e-03\n",
            "Loss: 6.387395e-03\n",
            "Loss: 6.385746e-03\n",
            "Loss: 6.385021e-03\n",
            "Loss: 6.384070e-03\n",
            "Loss: 6.383583e-03\n",
            "Loss: 6.382671e-03\n",
            "Loss: 6.383542e-03\n",
            "Loss: 6.382216e-03\n",
            "Loss: 6.381478e-03\n",
            "Loss: 6.380718e-03\n",
            "Loss: 6.380432e-03\n",
            "Loss: 6.379076e-03\n",
            "Loss: 6.378387e-03\n",
            "Loss: 6.377326e-03\n",
            "Loss: 6.376821e-03\n",
            "Loss: 6.375604e-03\n",
            "Loss: 6.374215e-03\n",
            "Loss: 6.372656e-03\n",
            "Loss: 6.371941e-03\n",
            "Loss: 6.370396e-03\n",
            "Loss: 6.370020e-03\n",
            "Loss: 6.369424e-03\n",
            "Loss: 6.368411e-03\n",
            "Loss: 6.372386e-03\n",
            "Loss: 6.368067e-03\n",
            "Loss: 6.366872e-03\n",
            "Loss: 6.366145e-03\n",
            "Loss: 6.365386e-03\n",
            "Loss: 6.364993e-03\n",
            "Loss: 6.364090e-03\n",
            "Loss: 6.363383e-03\n",
            "Loss: 6.363600e-03\n",
            "Loss: 6.363229e-03\n",
            "Loss: 6.362699e-03\n",
            "Loss: 6.362529e-03\n",
            "Loss: 6.362110e-03\n",
            "Loss: 6.361750e-03\n",
            "Loss: 6.361202e-03\n",
            "Loss: 6.360819e-03\n",
            "Loss: 6.360393e-03\n",
            "Loss: 6.360054e-03\n",
            "Loss: 6.359895e-03\n",
            "Loss: 6.359522e-03\n",
            "Loss: 6.359024e-03\n",
            "Loss: 6.358411e-03\n",
            "Loss: 6.357432e-03\n",
            "Loss: 6.356556e-03\n",
            "Loss: 6.355510e-03\n",
            "Loss: 6.354738e-03\n",
            "Loss: 6.353671e-03\n",
            "Loss: 6.352894e-03\n",
            "Loss: 6.351854e-03\n",
            "Loss: 6.350615e-03\n",
            "Loss: 6.350012e-03\n",
            "Loss: 6.349432e-03\n",
            "Loss: 6.348765e-03\n",
            "Loss: 6.347831e-03\n",
            "Loss: 6.350226e-03\n",
            "Loss: 6.347391e-03\n",
            "Loss: 6.346784e-03\n",
            "Loss: 6.346192e-03\n",
            "Loss: 6.345921e-03\n",
            "Loss: 6.345107e-03\n",
            "Loss: 6.344859e-03\n",
            "Loss: 6.343796e-03\n",
            "Loss: 6.343185e-03\n",
            "Loss: 6.342446e-03\n",
            "Loss: 6.341517e-03\n",
            "Loss: 6.340765e-03\n",
            "Loss: 6.339103e-03\n",
            "Loss: 6.338562e-03\n",
            "Loss: 6.338363e-03\n",
            "Loss: 6.337958e-03\n",
            "Loss: 6.337346e-03\n",
            "Loss: 6.336759e-03\n",
            "Loss: 6.336174e-03\n",
            "Loss: 6.335452e-03\n",
            "Loss: 6.334688e-03\n",
            "Loss: 6.334322e-03\n",
            "Loss: 6.333665e-03\n",
            "Loss: 6.333307e-03\n",
            "Loss: 6.332158e-03\n",
            "Loss: 6.331535e-03\n",
            "Loss: 6.330043e-03\n",
            "Loss: 6.329251e-03\n",
            "Loss: 6.329402e-03\n",
            "Loss: 6.328878e-03\n",
            "Loss: 6.328344e-03\n",
            "Loss: 6.327808e-03\n",
            "Loss: 6.326858e-03\n",
            "Loss: 6.325770e-03\n",
            "Loss: 6.324547e-03\n",
            "Loss: 6.323250e-03\n",
            "Loss: 6.322107e-03\n",
            "Loss: 6.319547e-03\n",
            "Loss: 6.317704e-03\n",
            "Loss: 6.315817e-03\n",
            "Loss: 6.314316e-03\n",
            "Loss: 6.313487e-03\n",
            "Loss: 6.312975e-03\n",
            "Loss: 6.312381e-03\n",
            "Loss: 6.310993e-03\n",
            "Loss: 6.309854e-03\n",
            "Loss: 6.308721e-03\n",
            "Loss: 6.307775e-03\n",
            "Loss: 6.307221e-03\n",
            "Loss: 6.305839e-03\n",
            "Loss: 6.304550e-03\n",
            "Loss: 6.303880e-03\n",
            "Loss: 6.302986e-03\n",
            "Loss: 6.302675e-03\n",
            "Loss: 6.301687e-03\n",
            "Loss: 6.300685e-03\n",
            "Loss: 6.299622e-03\n",
            "Loss: 6.298054e-03\n",
            "Loss: 6.297244e-03\n",
            "Loss: 6.296499e-03\n",
            "Loss: 6.295675e-03\n",
            "Loss: 6.294447e-03\n",
            "Loss: 6.293072e-03\n",
            "Loss: 6.291064e-03\n",
            "Loss: 6.291572e-03\n",
            "Loss: 6.290372e-03\n",
            "Loss: 6.289106e-03\n",
            "Loss: 6.288574e-03\n",
            "Loss: 6.287660e-03\n",
            "Loss: 6.286708e-03\n",
            "Loss: 6.285808e-03\n",
            "Loss: 6.283672e-03\n",
            "Loss: 6.282729e-03\n",
            "Loss: 6.281793e-03\n",
            "Loss: 6.282395e-03\n",
            "Loss: 6.281413e-03\n",
            "Loss: 6.280733e-03\n",
            "Loss: 6.279984e-03\n",
            "Loss: 6.279279e-03\n",
            "Loss: 6.278137e-03\n",
            "Loss: 6.278200e-03\n",
            "Loss: 6.277408e-03\n",
            "Loss: 6.276461e-03\n",
            "Loss: 6.275493e-03\n",
            "Loss: 6.274842e-03\n",
            "Loss: 6.274162e-03\n",
            "Loss: 6.273167e-03\n",
            "Loss: 6.271646e-03\n",
            "Loss: 6.270236e-03\n",
            "Loss: 6.269327e-03\n",
            "Loss: 6.268314e-03\n",
            "Loss: 6.274521e-03\n",
            "Loss: 6.268134e-03\n",
            "Loss: 6.267127e-03\n",
            "Loss: 6.266099e-03\n",
            "Loss: 6.264445e-03\n",
            "Loss: 6.264264e-03\n",
            "Loss: 6.263765e-03\n",
            "Loss: 6.262450e-03\n",
            "Loss: 6.261591e-03\n",
            "Loss: 6.261014e-03\n",
            "Loss: 6.260457e-03\n",
            "Loss: 6.259515e-03\n",
            "Loss: 6.265190e-03\n",
            "Loss: 6.259032e-03\n",
            "Loss: 6.257748e-03\n",
            "Loss: 6.256368e-03\n",
            "Loss: 6.255686e-03\n",
            "Loss: 6.255547e-03\n",
            "Loss: 6.254738e-03\n",
            "Loss: 6.254205e-03\n",
            "Loss: 6.253612e-03\n",
            "Loss: 6.253358e-03\n",
            "Loss: 6.255783e-03\n",
            "Loss: 6.253084e-03\n",
            "Loss: 6.252341e-03\n",
            "Loss: 6.251913e-03\n",
            "Loss: 6.251332e-03\n",
            "Loss: 6.252763e-03\n",
            "Loss: 6.251309e-03\n",
            "Loss: 6.251002e-03\n",
            "Loss: 6.250757e-03\n",
            "Loss: 6.249927e-03\n",
            "Loss: 6.248552e-03\n",
            "Loss: 6.251802e-03\n",
            "Loss: 6.248191e-03\n",
            "Loss: 6.246848e-03\n",
            "Loss: 6.246434e-03\n",
            "Loss: 6.245980e-03\n",
            "Loss: 6.245541e-03\n",
            "Loss: 6.244904e-03\n",
            "Loss: 6.246285e-03\n",
            "Loss: 6.244630e-03\n",
            "Loss: 6.243820e-03\n",
            "Loss: 6.243145e-03\n",
            "Loss: 6.243774e-03\n",
            "Loss: 6.242943e-03\n",
            "Loss: 6.242620e-03\n",
            "Loss: 6.242237e-03\n",
            "Loss: 6.241640e-03\n",
            "Loss: 6.241035e-03\n",
            "Loss: 6.239946e-03\n",
            "Loss: 6.239634e-03\n",
            "Loss: 6.238420e-03\n",
            "Loss: 6.237997e-03\n",
            "Loss: 6.237415e-03\n",
            "Loss: 6.236665e-03\n",
            "Loss: 6.236321e-03\n",
            "Loss: 6.235737e-03\n",
            "Loss: 6.235398e-03\n",
            "Loss: 6.235080e-03\n",
            "Loss: 6.234377e-03\n",
            "Loss: 6.233610e-03\n",
            "Loss: 6.233191e-03\n",
            "Loss: 6.234089e-03\n",
            "Loss: 6.232607e-03\n",
            "Loss: 6.232037e-03\n",
            "Loss: 6.231653e-03\n",
            "Loss: 6.231092e-03\n",
            "Loss: 6.230632e-03\n",
            "Loss: 6.230117e-03\n",
            "Loss: 6.229456e-03\n",
            "Loss: 6.228921e-03\n",
            "Loss: 6.228388e-03\n",
            "Loss: 6.228175e-03\n",
            "Loss: 6.227821e-03\n",
            "Loss: 6.227497e-03\n",
            "Loss: 6.227267e-03\n",
            "Loss: 6.226696e-03\n",
            "Loss: 6.226822e-03\n",
            "Loss: 6.226489e-03\n",
            "Loss: 6.226588e-03\n",
            "Loss: 6.226249e-03\n",
            "Loss: 6.225894e-03\n",
            "Loss: 6.225619e-03\n",
            "Loss: 6.225361e-03\n",
            "Loss: 6.224689e-03\n",
            "Loss: 6.224166e-03\n",
            "Loss: 6.223497e-03\n",
            "Loss: 6.223083e-03\n",
            "Loss: 6.222663e-03\n",
            "Loss: 6.222259e-03\n",
            "Loss: 6.221444e-03\n",
            "Loss: 6.221042e-03\n",
            "Loss: 6.220493e-03\n",
            "Loss: 6.220230e-03\n",
            "Loss: 6.219540e-03\n",
            "Loss: 6.220088e-03\n",
            "Loss: 6.219485e-03\n",
            "Loss: 6.218979e-03\n",
            "Loss: 6.218534e-03\n",
            "Loss: 6.218107e-03\n",
            "Loss: 6.217340e-03\n",
            "Loss: 6.216948e-03\n",
            "Loss: 6.215884e-03\n",
            "Loss: 6.215602e-03\n",
            "Loss: 6.215242e-03\n",
            "Loss: 6.214891e-03\n",
            "Loss: 6.214268e-03\n",
            "Loss: 6.213952e-03\n",
            "Loss: 6.213344e-03\n",
            "Loss: 6.213066e-03\n",
            "Loss: 6.212786e-03\n",
            "Loss: 6.212105e-03\n",
            "Loss: 6.211255e-03\n",
            "Loss: 6.209825e-03\n",
            "Loss: 6.210179e-03\n",
            "Loss: 6.209025e-03\n",
            "Loss: 6.208203e-03\n",
            "Loss: 6.207770e-03\n",
            "Loss: 6.207415e-03\n",
            "Loss: 6.206749e-03\n",
            "Loss: 6.208385e-03\n",
            "Loss: 6.206623e-03\n",
            "Loss: 6.206132e-03\n",
            "Loss: 6.205561e-03\n",
            "Loss: 6.204918e-03\n",
            "Loss: 6.204273e-03\n",
            "Loss: 6.204215e-03\n",
            "Loss: 6.203615e-03\n",
            "Loss: 6.203457e-03\n",
            "Loss: 6.202979e-03\n",
            "Loss: 6.202469e-03\n",
            "Loss: 6.202209e-03\n",
            "Loss: 6.201412e-03\n",
            "Loss: 6.200843e-03\n",
            "Loss: 6.200494e-03\n",
            "Loss: 6.199875e-03\n",
            "Loss: 6.201665e-03\n",
            "Loss: 6.199542e-03\n",
            "Loss: 6.198944e-03\n",
            "Loss: 6.198350e-03\n",
            "Loss: 6.198078e-03\n",
            "Loss: 6.197830e-03\n",
            "Loss: 6.197577e-03\n",
            "Loss: 6.197223e-03\n",
            "Loss: 6.197084e-03\n",
            "Loss: 6.196658e-03\n",
            "Loss: 6.196136e-03\n",
            "Loss: 6.195426e-03\n",
            "Loss: 6.198243e-03\n",
            "Loss: 6.194938e-03\n",
            "Loss: 6.194369e-03\n",
            "Loss: 6.193307e-03\n",
            "Loss: 6.192867e-03\n",
            "Loss: 6.193938e-03\n",
            "Loss: 6.192502e-03\n",
            "Loss: 6.191594e-03\n",
            "Loss: 6.191064e-03\n",
            "Loss: 6.190328e-03\n",
            "Loss: 6.189669e-03\n",
            "Loss: 6.191264e-03\n",
            "Loss: 6.189519e-03\n",
            "Loss: 6.189244e-03\n",
            "Loss: 6.188828e-03\n",
            "Loss: 6.188320e-03\n",
            "Loss: 6.187432e-03\n",
            "Loss: 6.186439e-03\n",
            "Loss: 6.185204e-03\n",
            "Loss: 6.184831e-03\n",
            "Loss: 6.184369e-03\n",
            "Loss: 6.183538e-03\n",
            "Loss: 6.182536e-03\n",
            "Loss: 6.183926e-03\n",
            "Loss: 6.182086e-03\n",
            "Loss: 6.181164e-03\n",
            "Loss: 6.180685e-03\n",
            "Loss: 6.180027e-03\n",
            "Loss: 6.179341e-03\n",
            "Loss: 6.178736e-03\n",
            "Loss: 6.178382e-03\n",
            "Loss: 6.177629e-03\n",
            "Loss: 6.176540e-03\n",
            "Loss: 6.175833e-03\n",
            "Loss: 6.175249e-03\n",
            "Loss: 6.174401e-03\n",
            "Loss: 6.173912e-03\n",
            "Loss: 6.172784e-03\n",
            "Loss: 6.172969e-03\n",
            "Loss: 6.172036e-03\n",
            "Loss: 6.171072e-03\n",
            "Loss: 6.169599e-03\n",
            "Loss: 6.169451e-03\n",
            "Loss: 6.169074e-03\n",
            "Loss: 6.168277e-03\n",
            "Loss: 6.167319e-03\n",
            "Loss: 6.166310e-03\n",
            "Loss: 6.165453e-03\n",
            "Loss: 6.164937e-03\n",
            "Loss: 6.164589e-03\n",
            "Loss: 6.164152e-03\n",
            "Loss: 6.162875e-03\n",
            "Loss: 6.161412e-03\n",
            "Loss: 6.167931e-03\n",
            "Loss: 6.160631e-03\n",
            "Loss: 6.158495e-03\n",
            "Loss: 6.156760e-03\n",
            "Loss: 6.155347e-03\n",
            "Loss: 6.154309e-03\n",
            "Loss: 6.152993e-03\n",
            "Loss: 6.152259e-03\n",
            "Loss: 6.151814e-03\n",
            "Loss: 6.150758e-03\n",
            "Loss: 6.149231e-03\n",
            "Loss: 6.148336e-03\n",
            "Loss: 6.147760e-03\n",
            "Loss: 6.147177e-03\n",
            "Loss: 6.146565e-03\n",
            "Loss: 6.147870e-03\n",
            "Loss: 6.146098e-03\n",
            "Loss: 6.145581e-03\n",
            "Loss: 6.144122e-03\n",
            "Loss: 6.143002e-03\n",
            "Loss: 6.142058e-03\n",
            "Loss: 6.143734e-03\n",
            "Loss: 6.141769e-03\n",
            "Loss: 6.141415e-03\n",
            "Loss: 6.140795e-03\n",
            "Loss: 6.140219e-03\n",
            "Loss: 6.139154e-03\n",
            "Loss: 6.137209e-03\n",
            "Loss: 6.135391e-03\n",
            "Loss: 6.133948e-03\n",
            "Loss: 6.132630e-03\n",
            "Loss: 6.131479e-03\n",
            "Loss: 6.130707e-03\n",
            "Loss: 6.130150e-03\n",
            "Loss: 6.129005e-03\n",
            "Loss: 6.128334e-03\n",
            "Loss: 6.126721e-03\n",
            "Loss: 6.130642e-03\n",
            "Loss: 6.126247e-03\n",
            "Loss: 6.125104e-03\n",
            "Loss: 6.124486e-03\n",
            "Loss: 6.123517e-03\n",
            "Loss: 6.122409e-03\n",
            "Loss: 6.120489e-03\n",
            "Loss: 6.117665e-03\n",
            "Loss: 6.116146e-03\n",
            "Loss: 6.114925e-03\n",
            "Loss: 6.114235e-03\n",
            "Loss: 6.113105e-03\n",
            "Loss: 6.111706e-03\n",
            "Loss: 6.110389e-03\n",
            "Loss: 6.108637e-03\n",
            "Loss: 6.106455e-03\n",
            "Loss: 6.104423e-03\n",
            "Loss: 6.101743e-03\n",
            "Loss: 6.098650e-03\n",
            "Loss: 6.098794e-03\n",
            "Loss: 6.097247e-03\n",
            "Loss: 6.095772e-03\n",
            "Loss: 6.094550e-03\n",
            "Loss: 6.093152e-03\n",
            "Loss: 6.093275e-03\n",
            "Loss: 6.091656e-03\n",
            "Loss: 6.089344e-03\n",
            "Loss: 6.087977e-03\n",
            "Loss: 6.086522e-03\n",
            "Loss: 6.085489e-03\n",
            "Loss: 6.087641e-03\n",
            "Loss: 6.084894e-03\n",
            "Loss: 6.084298e-03\n",
            "Loss: 6.083773e-03\n",
            "Loss: 6.083079e-03\n",
            "Loss: 6.082675e-03\n",
            "Loss: 6.081501e-03\n",
            "Loss: 6.081106e-03\n",
            "Loss: 6.080574e-03\n",
            "Loss: 6.079592e-03\n",
            "Loss: 6.077900e-03\n",
            "Loss: 6.077779e-03\n",
            "Loss: 6.076944e-03\n",
            "Loss: 6.075228e-03\n",
            "Loss: 6.074355e-03\n",
            "Loss: 6.073609e-03\n",
            "Loss: 6.072434e-03\n",
            "Loss: 6.071058e-03\n",
            "Loss: 6.069596e-03\n",
            "Loss: 6.067573e-03\n",
            "Loss: 6.066472e-03\n",
            "Loss: 6.065339e-03\n",
            "Loss: 6.064514e-03\n",
            "Loss: 6.063908e-03\n",
            "Loss: 6.063276e-03\n",
            "Loss: 6.062434e-03\n",
            "Loss: 6.061883e-03\n",
            "Loss: 6.061505e-03\n",
            "Loss: 6.061243e-03\n",
            "Loss: 6.060864e-03\n",
            "Loss: 6.060786e-03\n",
            "Loss: 6.060188e-03\n",
            "Loss: 6.059819e-03\n",
            "Loss: 6.059120e-03\n",
            "Loss: 6.058424e-03\n",
            "Loss: 6.060609e-03\n",
            "Loss: 6.057924e-03\n",
            "Loss: 6.057230e-03\n",
            "Loss: 6.056554e-03\n",
            "Loss: 6.056018e-03\n",
            "Loss: 6.055291e-03\n",
            "Loss: 6.054309e-03\n",
            "Loss: 6.053780e-03\n",
            "Loss: 6.052945e-03\n",
            "Loss: 6.052140e-03\n",
            "Loss: 6.051209e-03\n",
            "Loss: 6.050657e-03\n",
            "Loss: 6.050134e-03\n",
            "Loss: 6.049741e-03\n",
            "Loss: 6.049190e-03\n",
            "Loss: 6.048871e-03\n",
            "Loss: 6.048373e-03\n",
            "Loss: 6.048073e-03\n",
            "Loss: 6.047371e-03\n",
            "Loss: 6.046638e-03\n",
            "Loss: 6.048624e-03\n",
            "Loss: 6.046381e-03\n",
            "Loss: 6.046096e-03\n",
            "Loss: 6.045567e-03\n",
            "Loss: 6.044925e-03\n",
            "Loss: 6.044293e-03\n",
            "Loss: 6.044302e-03\n",
            "Loss: 6.044142e-03\n",
            "Loss: 6.043948e-03\n",
            "Loss: 6.043982e-03\n",
            "Loss: 6.044014e-03\n",
            "Loss: 6.043974e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.044001e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043956e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043958e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043936e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043955e-03\n",
            "Loss: 6.043991e-03\n",
            "Loss: 6.043931e-03\n",
            "Loss: 6.043955e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "Loss: 6.043927e-03\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
            "  Objective function value: 0.006044\n",
            "  Number of iterations: 6275\n",
            "  Number of functions evaluations: 6692\n",
            "Error u: 6.631518e-03\n",
            "Error u (idn): 5.329245e-03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mn3fPX8kthV"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rud4V8pEkthW"
      },
      "source": [
        "## PLOTTING: \n",
        "import matplotlib as mpl\n",
        "mpl.use('pgf')\n",
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
        "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
        "    \"text.usetex\": True,                # use LaTeX to write all text\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
        "    \"font.sans-serif\": [],\n",
        "    \"font.monospace\": [],\n",
        "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
        "    \"font.size\": 10,\n",
        "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
        "    \"pgf.preamble\": [\n",
        "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
        "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
        "        ]\n",
        "    }\n",
        "mpl.rcParams.update(pgf_with_latex)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots))\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def savefig(filename, crop = True):\n",
        "    if crop == True:\n",
        "        plt.savefig('{}.png'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "     #   plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "        plt.savefig('{}.png'.format(filename))\n",
        "        plt.savefig('{}.pdf'.format(filename))\n",
        "     #   plt.savefig('{}.eps'.format(filename))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81x5SFAikthW",
        "outputId": "3f9133e2-211c-4be8-825f-0e823e047948"
      },
      "source": [
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "rc('text', usetex=True)\n",
        "mpl.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n",
        "!apt-get update\n",
        "!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.227.220.27)] [Wa\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rGet:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.227.220.27)] [3 \r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [3 In\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [3 InRelease 14.2 kB/15.9 k\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [3 InRelease 14.2 kB/15.9 k\r                                                                               \r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Connecting to ppa.launchpad.net (91.189.95.85)] \r                                                                               \rHit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r                                                                               \r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers]\r                                                   \rHit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers]\r                                                   \rHit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [895 kB]\n",
            "Fetched 911 kB in 2s (555 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cm-super is already the newest version (0.3.4-11).\n",
            "dvipng is already the newest version (1.15-1).\n",
            "texlive-fonts-extra is already the newest version (2017.20180305-2).\n",
            "texlive-fonts-recommended is already the newest version (2017.20180305-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "O_MB4UOvkthW",
        "outputId": "73c63676-83b8-4905-8713-e56abe2aac83"
      },
      "source": [
        "  ######################################################################\n",
        "    ############################# Plotting ###############################\n",
        "    ######################################################################    \n",
        "    \n",
        "fig, ax = newfig(1.5, 1)\n",
        "ax.axis('off')\n",
        "    \n",
        "    ######## Row 2: Pressure #######################\n",
        "    ########      Predicted p(t,x,y)     ########### \n",
        "gs = gridspec.GridSpec(1, 2)\n",
        "gs.update(top=0.8, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n",
        "ax = plt.subplot(gs[:, 0])\n",
        "h = ax.imshow(Exact_sol, interpolation='nearest', cmap='jet', \n",
        "                  extent=[lb_sol[0], ub_sol[0], lb_sol[1], ub_sol[1]],\n",
        "                  origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "ax.set_title('Exact Dynamics', fontsize = 10)\n",
        "line = np.linspace(lb_sol[1], ub_sol[1], 2)[:,None]\n",
        "ax.plot(t_idn[index]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ########     Exact p(t,x,y)     ########### \n",
        "ax = plt.subplot(gs[:, 1])\n",
        "h = ax.imshow(U_pred, interpolation='nearest', cmap='jet', \n",
        "                 extent=[lb_sol[0], ub_sol[0], lb_sol[1], ub_sol[1]],origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$x$')\n",
        "ax.set_title('Learned Dynamics', fontsize = 10)\n",
        "line = np.linspace(lb_sol[1], ub_sol[1], 2)[:,None]\n",
        "ax.plot(t_idn[index]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "#savefig('/content/figures/kdV_sine')\n",
        "savefig('/content/figures/kdV_cosine')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 582.814x360.199 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}